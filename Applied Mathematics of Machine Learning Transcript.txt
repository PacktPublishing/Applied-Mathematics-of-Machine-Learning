Transcript
This transcript was computer generated and may contain errors.

Session: Applied Mathematics of Machine Learning

00:00:00: Abhishek Kaushik
Hello, everyone, and welcome to this workshop on applying applied mathematics and machine learning. I'm Abhishek Kashit, and I'll be your host for today's workshop. It's a pleasure to have you all with us.

00:00:10: Abhishek Kaushik
Before we begin, a quick note on participation.

00:00:13: Abhishek Kaushik
If you have any questions at any such point, please feel free to drop them into the q and a tab onto the right side of your image screen.

00:00:21: Abhishek Kaushik
The more you engage, ask questions, and contribute, the richer the experience will be not just for you, but for the entire community.

00:00:28: Abhishek Kaushik
And there is a fun bonus to keep in mind.

00:00:30: Abhishek Kaushik
Keep an active live keep an active eye on the live leaderboard throughout the session. The top three active participants will be walking away with some exciting prizes. So jump in, stay involved, and make your presence count and know. Now coming to today's session,

00:00:44: Abhishek Kaushik
we have the privilege of learning from an exceptional mind,

00:00:47: Abhishek Kaushik
in the field. He has had more than 30,000 engineers built clarity and confidence in machine learning and mathematics. He also holds a PhD in mathematics. From the University of Zegedad and brings over ten years of experience teaching mathematics and machine learning to learners across the globe. Known for his ability to build and simplify mathematical ideas,

00:01:07: Abhishek Kaushik
he consistently bridges theory with real world machine learning applications in a And

00:01:13: Abhishek Kaushik
He's also the founder of the Palyn Room newsletter, a trusted resource followed by thousands of engineers who rely on his insights to better understand the mathematical foundations of the modern AI systems.

00:01:25: Abhishek Kaushik
I, I request you all to please join me

00:01:29: Abhishek Kaushik
giving a warm welcome and a round of applause

00:01:31: Abhishek Kaushik
our host and speaker for tonight for today's workshop, mister Tiwada Tankar.

00:01:39: Abhishek Kaushik
Hi, Tiwala. Over to you, Tiwala. Over to you. Thank you.

00:01:41: Tivadar Danka
Hi.

00:01:41: Tivadar Danka
Thank you.

00:01:44: Tivadar Danka
Thank you. Thank you for your nice introduction. I'm I'm not not an excellent mind, as you said. I'm

00:01:51: Tivadar Danka
I'm an average mind.

00:01:53: Tivadar Danka
I just

00:01:54: Tivadar Danka
spent a lot of time

00:01:56: Tivadar Danka
doing mathematics, so I I learned quite a lot about it.

00:01:60: Tivadar Danka
This is my my first time giving such a such a

00:02:04: Tivadar Danka
long workshop.

00:02:06: Tivadar Danka
In in the product ecosystem.

00:02:09: Tivadar Danka
I'm I'm hoping that it will be like, a great experience for for you all. I love teaching, and I also love teaching online. So

00:02:17: Tivadar Danka
I will I will enjoy it for for sure.

00:02:20: Tivadar Danka
If you have any questions, just drop drop the questions into the into the

00:02:26: Tivadar Danka
q and a

00:02:27: Tivadar Danka
tab.

00:02:28: Tivadar Danka
Is is the audio

00:02:31: Tivadar Danka
good for you guys? Because we had a comment in the chat saying that the audio is not not optimal.

00:02:37: Tivadar Danka
If you have if you have any questions about my audio, or any concerns, just feel free to drop a message in the chat.

00:02:43: Tivadar Danka
I will periodically

00:02:46: Tivadar Danka
check-in on the chat, checking on the questions.

00:02:48: Tivadar Danka
I prepared a couple of introductory slides

00:02:53: Tivadar Danka
for you

00:02:57: Tivadar Danka
So

00:02:58: Tivadar Danka
as you know, I'm the author of the mathematics for for machine learning book.

00:03:03: Tivadar Danka
Which is

00:03:04: Tivadar Danka
probably my my most ambitious

00:03:07: Tivadar Danka
project to date.

00:03:08: Tivadar Danka
I started my career by doing a PhD in in mathematics. I did some some pure mathematical research, but I but I left for for,

00:03:17: Tivadar Danka
left left to to work in a computational biology group.

00:03:21: Tivadar Danka
Where I where I did machine learning.

00:03:23: Tivadar Danka
It was about ten years ago.

00:03:26: Tivadar Danka
The first time I came into contact with machine learning. And

00:03:29: Tivadar Danka
I've been in the field

00:03:31: Tivadar Danka
ever since.

00:03:32: Tivadar Danka
First couple of years, I was working as a machine learning engineer.

00:03:36: Tivadar Danka
I had a shorter job. So sorry. First couple of years, I was a learning researcher.

00:03:42: Tivadar Danka
Then I had had a had a

00:03:44: Tivadar Danka
a

00:03:44: Tivadar Danka
small period of time where I was working as a machine learning engineer.

00:03:48: Tivadar Danka
Then I found my own company, birthed my life savings on my first company as a

00:03:53: Tivadar Danka
it is usual.

00:03:55: Tivadar Danka
And then, basically, after my company failed, I start started to post about mathematics online.

00:04:02: Tivadar Danka
First time, I posted on Twitter. And then it it got big. And and I realized that this is what I enjoy doing the most.

00:04:10: Tivadar Danka
Creating educational content about mathematics. And the fruit of this whole whole online education educating

00:04:17: Tivadar Danka
project was was the mathematics of machine learning book, which

00:04:21: Tivadar Danka
is what this workshop is about.

00:04:23: Tivadar Danka
Alright. So

00:04:25: Tivadar Danka
they are going to talk about

00:04:27: Tivadar Danka
mainly four things in this workshop, vectors, matrices,

00:04:32: Tivadar Danka
that is linear algebra,

00:04:34: Tivadar Danka
calculus, multivariable calculus, and if we have

00:04:37: Tivadar Danka
time,

00:04:39: Tivadar Danka
VLC, a bit of probability theory.

00:04:42: Tivadar Danka
This this session will be a mix of of a live coding and and and me explaining mathematical concepts to you. On my iPad. So I have my

00:04:52: Tivadar Danka
iPad.

00:04:53: Tivadar Danka
Right in front of me and my trust that I can see.

00:04:56: Tivadar Danka
They are going to take things slow

00:04:59: Tivadar Danka
instead of instead of, you know, like like a

00:05:03: Tivadar Danka
creating, like, a, like, a fancy, but hard to understand slideshow for you.

00:05:07: Tivadar Danka
I prefer the the blackboard style

00:05:10: Tivadar Danka
talks, blackboard style lectures, but I I I write formulas by my hand

00:05:16: Tivadar Danka
you know, in a slower pace, but

00:05:19: Tivadar Danka
I I feel that this is the way

00:05:20: Tivadar Danka
to to obtain, like, a deep understanding of of the topic.

00:05:25: Tivadar Danka
Alright.

00:05:27: Tivadar Danka
Let me let me switch to my to my live coding

00:05:31: Tivadar Danka
environment here.

00:05:34: Tivadar Danka
I'm using OBS, and currently, I'm I'm juggling between

00:05:39: Tivadar Danka
seven or or or nine active

00:05:41: Tivadar Danka
applications and and Windows here. So please be patient. First, we'll we'll we'll start

00:05:50: Tivadar Danka
by

00:05:51: Tivadar Danka
looking at what this workshop will be about.

00:05:54: Tivadar Danka
So if if you if you read

00:05:56: Tivadar Danka
if you read the the the the description of this workshop, I basically mentioned to you that

00:06:03: Tivadar Danka
in this workshop, we are going to build a linear regression model.

00:06:06: Tivadar Danka
Linear regression model from scratch, which is not exactly state of the art. The Neo regression is very basic.

00:06:13: Tivadar Danka
You can you can train a linear regression model in two or three lines. Even even scikit learn contains the the linear regression model. It's very simple. Not fancy. However, if you dissect linear regression into all of its components, and and actually implement linear regression like, a very general form of linear regression by yourself from scratch. You you you you learn more about machine learning than you could ever learn

00:06:46: Tivadar Danka
just by doing

00:06:48: Tivadar Danka
basically,

00:06:49: Tivadar Danka
machine learning projects. Alright? So we are going to focus on the fundamentals right now.

00:06:58: Tivadar Danka
Very simple thing, linear regression. We are going to start in in one dimensions. So so

00:07:07: Tivadar Danka
so

00:07:08: Tivadar Danka
I will I will generate a toy dataset

00:07:11: Tivadar Danka
in in NumPy

00:07:16: Tivadar Danka
Most most of my code

00:07:18: Tivadar Danka
I will I will write it live.

00:07:20: Tivadar Danka
However, there will be occasions where I'm copying and pasting calls from from my other

00:07:25: Tivadar Danka
Versus code instance in my second monitor.

00:07:28: Tivadar Danka
These are the instances where the code itself is is not that important. Like, right now, I'm just going to I I just want to want to

00:07:35: Tivadar Danka
demonstrate to you the problem that that we are we are working on.

00:07:41: Tivadar Danka
So here, I'm just going to generate, like, a toy toy dataset

00:07:45: Tivadar Danka
This this here.

00:07:48: Tivadar Danka
Which is going to be

00:07:50: Tivadar Danka
our our initial example. So here, what you what you see is the x axis and the y axis.

00:07:58: Tivadar Danka
X axis contains the the the input of your machine learning model the y axis is the output, which is

00:08:03: Tivadar Danka
something which you you you want to predict. So the the simplest possible way

00:08:11: Tivadar Danka
maybe I can write a lot of code live as well. Yeah. Sure. Let's do live lot of coding.

00:08:18: Tivadar Danka
So the simplest possible way

00:08:20: Tivadar Danka
for for for us to train the model

00:08:23: Tivadar Danka
for this regression problem is the linear regression.

00:08:27: Tivadar Danka
Alright. So so if if you are having trouble

00:08:31: Tivadar Danka
seeing the the the

00:08:33: Tivadar Danka
letters on my screen, just let me know in the chat. So

00:08:35: Tivadar Danka
I can increase the font size for you if if you want.

00:08:41: Tivadar Danka
I'm just

00:08:43: Tivadar Danka
quickly scrolling to the chat here.

00:08:45: Tivadar Danka
Checking if if there are any issues. No issues, so we can we can continue. So this this what you have here is something called, like, a a linear function. In Python, this can be implemented very easily. So

00:09:02: Tivadar Danka
x here is going to be our our input variable.

00:09:07: Tivadar Danka
Or or feature, so to speak, and a and b will be our our model parameters. Like, we have two model parameters.

00:09:15: Tivadar Danka
And this is a very simple function implemented in Python.

00:09:19: Tivadar Danka
A times x plus b. This

00:09:22: Tivadar Danka
already works perfectly.

00:09:24: Tivadar Danka
I think we are better off if we set some default values

00:09:29: Tivadar Danka
for for the a and b arguments.

00:09:32: Tivadar Danka
Oh, yeah. Yeah. This this workshop is not about clean coding. Alright? So I will I will do some sketchy stuff here.

00:09:39: Tivadar Danka
I just want to show you mathematical ideas

00:09:42: Tivadar Danka
Like,

00:09:43: Tivadar Danka
not

00:09:43: Tivadar Danka
not building

00:09:45: Tivadar Danka
robust and and production ready frameworks.

00:09:48: Tivadar Danka
So apologies if I if I trigger you with with one of my Python hacks. You don't understand something, just feel free to to

00:09:57: Tivadar Danka
shoot a question in in in the q and a tab of the of the Ermit Audio could be louder. Alright.

00:10:08: Tivadar Danka
I can move my mic closer to myself.

00:10:13: Tivadar Danka
Or I can increase the volume of my mic in my operating system.

00:10:17: Tivadar Danka
Alright. A little bigger font, please.

00:10:19: Tivadar Danka
Alright.

00:10:24: Tivadar Danka
Bigger font.

00:10:25: Tivadar Danka
I think I think this is

00:10:27: Tivadar Danka
this is

00:10:28: Tivadar Danka
this this should be good by now.

00:10:35: Tivadar Danka
Font size.

00:10:36: Tivadar Danka
Seems

00:10:37: Tivadar Danka
good now. Alright. Let's continue with the

00:10:42: Tivadar Danka
linear regression. So here, I just I just plotted the the training date and the linear regression model.

00:10:48: Tivadar Danka
Which is like, the the graph of this very simple function is is just just a line. This is

00:10:55: Tivadar Danka
linear regression. And as I mentioned, we are going to

00:10:59: Tivadar Danka
take these to the extremes. We are going to build, like, a very general and vectorized linear regression model in the next four hours.

00:11:07: Tivadar Danka
I hope I will be able to finish in time.

00:11:09: Tivadar Danka
What's the issue?

00:11:11: Tivadar Danka
With with what what we are seeing here with this

00:11:13: Tivadar Danka
toy model and this toy dataset?

00:11:16: Tivadar Danka
Real machine learning data is is never one dimensional.

00:11:20: Tivadar Danka
If a dataset is one dimensional, it's

00:11:23: Tivadar Danka
like,

00:11:24: Tivadar Danka
I wouldn't say it's not that interesting because there are some time series problems where they are fundamentally one dimensional data, which is very complex to predict. But

00:11:36: Tivadar Danka
most cases, data is not one dimensional. So let let me just check. Well, let let us just

00:11:42: Tivadar Danka
check how real dataset looks like.

00:11:44: Tivadar Danka
So we are going to import the datasets from Scikit Learn.

00:11:49: Tivadar Danka
You probably know scikit learn. This is one of the most

00:11:53: Tivadar Danka
basic

00:11:55: Tivadar Danka
yet

00:11:57: Tivadar Danka
most useful and most beautiful my opinion, machine learning packages out there.

00:12:04: Tivadar Danka
Actually, it contains

00:12:06: Tivadar Danka
bunch of

00:12:07: Tivadar Danka
bunch of datasets, like,

00:12:09: Tivadar Danka
training datasets for for you to test your problems.

00:12:12: Tivadar Danka
As well. For this demonstration, we are going to use the so called diabetes dataset Again, this is not something which you should really, really understand how how it works. I'm just going to call the load date load function, which returns you, like, a a a

00:12:32: Tivadar Danka
an object. And this object has a couple of attributes. One of them is the data section. One one is the is the target variable to predict. Traditionally, the the dataset is denoted by x, The target variable is denoted by y. So this is why I'm going to name them.

00:12:48: Tivadar Danka
This way, the the data attribute of the dataset object

00:12:54: Tivadar Danka
returned by the low diabetes function is going to contain the data.

00:12:59: Tivadar Danka
The target attribute of this this low liabilities

00:13:04: Tivadar Danka
stuff is going to be the target variable.

00:13:08: Tivadar Danka
Let's let's check x or at least the first 10 rows of x.

00:13:12: Tivadar Danka
What what what you see here

00:13:14: Tivadar Danka
is is a big

00:13:16: Tivadar Danka
table of numbers. It looks a bit messy.

00:13:19: Tivadar Danka
Point is this is what we call a matrix. This is instead of of just

00:13:26: Tivadar Danka
list of of scalars, and instead of just being a single scalar, this is a table of numbers.

00:13:32: Tivadar Danka
And

00:13:33: Tivadar Danka
this journey, at least, the first part of this journey will be will be about how

00:13:38: Tivadar Danka
generalizing your or or simple linear regression model

00:13:42: Tivadar Danka
to to a multidimensional, fully vectorized

00:13:46: Tivadar Danka
linear regression model.

00:13:48: Tivadar Danka
By the way, this is NumPy here.

00:13:50: Tivadar Danka
We they are seeing here x is a x is a is a is an instance of an array. And the array short for

00:13:60: Tivadar Danka
n dimensional array. And, actually, we can we can check the the the dimensions of of this array by by taking a look at the shape attribute.

00:14:09: Tivadar Danka
This is a two dimensional array.

00:14:12: Tivadar Danka
A table,

00:14:13: Tivadar Danka
table datasets. If you

00:14:15: Tivadar Danka
like that. That's a terminology more. Has 442 rows.

00:14:20: Tivadar Danka
And ten ten columns

00:14:22: Tivadar Danka
Each row is a data sample

00:14:24: Tivadar Danka
a data measurement, a concrete measurement, and each column is a feature. I mean, here,

00:14:32: Tivadar Danka
I think we have had the features built into the datasets,

00:14:36: Tivadar Danka
object itself.

00:14:39: Tivadar Danka
You can see that the first first column is age, second column is sex, third is body mass index, Fourth is, I guess, is blood pressure, and the other ones, I I I don't know

00:14:49: Tivadar Danka
what what are these.

00:14:52: Tivadar Danka
But most importantly, I

00:14:56: Tivadar Danka
the I mean, for instance, this this, what you see here, is the first row.

00:15:02: Tivadar Danka
Of the x matrix. This is like a what we call a vector. This number here, I hope you can see my highlights. You can see my highlights. Alright. This is this is the the the age attribute of the first measurement or the first person's age attribute.

00:15:23: Tivadar Danka
Second is the sex and so on. Of course, these are

00:15:28: Tivadar Danka
normalized

00:15:31: Tivadar Danka
instances.

00:15:35: Tivadar Danka
The point is that this is basically like a like a list of numbers.

00:15:39: Tivadar Danka
So

00:15:40: Tivadar Danka
mathematically speaking,

00:15:44: Tivadar Danka
we are talking about

00:15:45: Tivadar Danka
vectors and and matrices here.

00:15:49: Tivadar Danka
And I I

00:15:52: Tivadar Danka
probably

00:15:53: Tivadar Danka
won't spoil much to you if if I if I go a little bit ahead.

00:15:60: Tivadar Danka
In in the end,

00:16:02: Tivadar Danka
like,

00:16:03: Tivadar Danka
we have we have this model function. Right?

00:16:07: Tivadar Danka
Which which works for for for for for a scaler instance.

00:16:12: Tivadar Danka
Right now,

00:16:13: Tivadar Danka
they can plug in like a

00:16:16: Tivadar Danka
a a a a matrix here as well, but it wouldn't wouldn't work properly. Right? So our goal is to to make our model work for for these these

00:16:27: Tivadar Danka
datasets features.

00:16:28: Tivadar Danka
Measurements, whatever.

00:16:33: Tivadar Danka
First, let's talk about vectors, and this is

00:16:37: Tivadar Danka
but I'm going to switch to my iPad here.

00:16:40: Tivadar Danka
Let me check the chat for questions.

00:16:47: Tivadar Danka
Couple of questions in the q and a. Are we supposed to be coding along

00:16:51: Tivadar Danka
God no. Just

00:16:52: Tivadar Danka
pay attention. I will share your I will share my my notes with you. After the the lesson.

00:16:59: Tivadar Danka
Even even

00:17:00: Tivadar Danka
aside the the notebook

00:17:03: Tivadar Danka
I am actually writing live, I I I wrote

00:17:07: Tivadar Danka
like,

00:17:07: Tivadar Danka
lecture notes for you.

00:17:10: Tivadar Danka
Which read, like, like, a book or basically book chapters you can use that to review the the material.

00:17:19: Tivadar Danka
I think it's it's best for you if you just pay pay attention. Is there any GitHub link

00:17:24: Tivadar Danka
that we can follow?

00:17:25: Tivadar Danka
Not right now. But as I mentioned, I have prepared lecture notes for you. Which are always in the form of Jupyter Notebooks. Jupyter Notebooks is my favorite. Tool for for creating

00:17:38: Tivadar Danka
like, basically written

00:17:39: Tivadar Danka
content. The whole mathematics for machine learning book was written in in Jupyter Notebook which

00:17:45: Tivadar Danka
caused a lot of headache for the for the for the editorial team, but I'm really grateful for them. That they they made it work.

00:17:53: Tivadar Danka
You don't see the the GitHub link and the resources session.

00:17:56: Tivadar Danka
I will I will

00:17:58: Tivadar Danka
fix this later after the session. Like,

00:18:01: Tivadar Danka
right now, you don't need to open any any other tab. You just just

00:18:06: Tivadar Danka
open open the the the air meat tab on your browser. Take a look at at your screen and just follow me. And I will do all the coding and all the explanation. And then you can you can review everything later.

00:18:17: Tivadar Danka
Alright. Now I'm going to going to probably finish my last coffee for the day.

00:18:24: Tivadar Danka
It it's 3PM here.

00:18:26: Tivadar Danka
By the way.

00:18:28: Tivadar Danka
Usually, I don't drink coffee at such a late.

00:18:32: Tivadar Danka
Time, but I want to keep my mind fresh. So sorry for the interruption.

00:18:39: Tivadar Danka
Alright. Cafe finished. We are ready to to move towards more mathematical

00:18:48: Tivadar Danka
more mathematical fields, Alright.

00:18:53: Tivadar Danka
Probably half of my half of my work shift will consist of me writing

00:18:58: Tivadar Danka
scribbling formulas into my into my iPad. So we are talking about vectors here. Right? So

00:19:06: Tivadar Danka
mathematically speaking,

00:19:08: Tivadar Danka
when we are we are talking about vectors, we are we are

00:19:12: Tivadar Danka
talking about top loads of numbers, for instance.

00:19:15: Tivadar Danka
V is a vector. I usually denote vectors by by putting a small arrow on top of the symbols.

00:19:24: Tivadar Danka
This is this is supposed to be an arrow.

00:19:27: Tivadar Danka
On on on on on Jupiter notebook, I denote vectors by using both face.

00:19:32: Tivadar Danka
Both face variables, but I cannot really write both face with my Apple pencil. So I'm going to stick with these these arrow notation here. So if you see

00:19:41: Tivadar Danka
an arrow on top of that letter, this means a vector. So

00:19:45: Tivadar Danka
so v one, v two, and so on. V n, this is a vector. So

00:19:56: Tivadar Danka
it has n n coordinates. So they say that this is an n dimension. Vector.

00:20:01: Tivadar Danka
And

00:20:02: Tivadar Danka
these are the coordinates.

00:20:08: Tivadar Danka
My handwriting can be a little bit ugly, but it has has a has a has a has a

00:20:15: Tivadar Danka
so to say, an advantage. It forces you to really pay attention to to to what we are doing and use use your your your brain instead of just relying on my on my handwriting to to be able to read all the information. So think of it like I'm training your your internal neural network model by by leaving intentional gaps, by by

00:20:35: Tivadar Danka
having really

00:20:37: Tivadar Danka
ugly handwriting

00:20:44: Tivadar Danka
Alright.

00:20:48: Tivadar Danka
I just quickly checked the the chat for any any other questions.

00:20:52: Tivadar Danka
What makes a vector vector?

00:20:56: Tivadar Danka
You would think a vector is vector is because it's it's it's a list of of of

00:21:01: Tivadar Danka
coordinates, but you are wrong. Mathematically speaking,

00:21:05: Tivadar Danka
a vector is defined by its its set set of operations.

00:21:12: Tivadar Danka
So in in

00:21:14: Tivadar Danka
two dimensions, they can illustrate

00:21:17: Tivadar Danka
vectors by draw drawing

00:21:20: Tivadar Danka
arrows

00:21:22: Tivadar Danka
Like, this is this is the x axis. This is the y axis. This is the the Euclidean plane or Cartesian plane.

00:21:29: Tivadar Danka
If you prefer that. That terminology.

00:21:33: Tivadar Danka
This here,

00:21:36: Tivadar Danka
is the vector x.

00:21:38: Tivadar Danka
X one x two. It has two coordinates.

00:21:43: Tivadar Danka
This is

00:21:44: Tivadar Danka
or or

00:21:45: Tivadar Danka
rather, I used x and y as as access names. So instead of x one and x two,

00:21:53: Tivadar Danka
and we'll use x zero and y zero

00:22:04: Tivadar Danka
So this is x zero.

00:22:08: Tivadar Danka
And this is y zero. Similarly, higher dimensional vector can also be thought of as arrows.

00:22:15: Tivadar Danka
From the origin

00:22:16: Tivadar Danka
to the to the point described by the coordinates itself.

00:22:20: Tivadar Danka
Alright.

00:22:22: Tivadar Danka
A vector is a vector because you can you can add them together and and stretch them out.

00:22:29: Tivadar Danka
If you have two vectors x and y,

00:22:40: Tivadar Danka
where

00:22:41: Tivadar Danka
sometimes I I have to fight my iPad.

00:22:50: Tivadar Danka
So apologies for that. So x is x one. And so on. X m

00:22:53: Tivadar Danka
y is

00:22:55: Tivadar Danka
y one, and so on.

00:22:57: Tivadar Danka
Y n.

00:22:59: Tivadar Danka
The the

00:22:59: Tivadar Danka
sum of these vectors are simply defined

00:23:04: Tivadar Danka
coordinate wise to x one plus

00:23:09: Tivadar Danka
y one and so on.

00:23:12: Tivadar Danka
X m plus y m. Alright.

00:23:16: Tivadar Danka
In two dimensions, we can visualize this addition

00:23:20: Tivadar Danka
simply

00:23:21: Tivadar Danka
in

00:23:28: Tivadar Danka
So we have the the

00:23:30: Tivadar Danka
x factor here.

00:23:34: Tivadar Danka
We have the y vector here. And we add these together by the so called parallel

00:23:39: Tivadar Danka
parallelogram rule. It's a differ it's difficult word to say, so my my tongue will stumble sometimes.

00:23:46: Tivadar Danka
But, hopefully, this is the last time I'm going to say parallelogram. In this workshop.

00:23:53: Tivadar Danka
So what what we what we do here is we we translate the x arrow

00:23:59: Tivadar Danka
to y.

00:24:03: Tivadar Danka
This this is basically, you can think about this moving here.

00:24:08: Tivadar Danka
And

00:24:08: Tivadar Danka
the sum will be defined

00:24:12: Tivadar Danka
by this this one here.

00:24:16: Tivadar Danka
This is x plus.

00:24:19: Tivadar Danka
Y. It's called the parallelogram rule because

00:24:23: Tivadar Danka
because these structure forms a paral parallelogram.

00:24:28: Tivadar Danka
Alright. I said the word at least two more times, but I promise you this is going to be the last time. Alright.

00:24:35: Tivadar Danka
It works very similarly in in higher dimensions. You might ask, why is this useful?

00:24:42: Tivadar Danka
I have

00:24:44: Tivadar Danka
generally, I have I have have two answers for that question. One is that that that

00:24:49: Tivadar Danka
more more representations

00:24:51: Tivadar Danka
are always useful.

00:24:52: Tivadar Danka
Sometimes, if you are solving a problem in practice,

00:24:56: Tivadar Danka
the key is to find the right representation. And and second, it's a geometric representation so you can actually

00:25:03: Tivadar Danka
see or visualize what happens

00:25:05: Tivadar Danka
in your head.

00:25:06: Tivadar Danka
It makes it much easier to work with with with vectors or matrices.

00:25:11: Tivadar Danka
We are not going to dig that deep into geometry. But just keep in mind that these are are

00:25:17: Tivadar Danka
basically arrows. Alright. Second vector operation. If c is a real number and x

00:25:28: Tivadar Danka
an n dimensional vector.

00:25:30: Tivadar Danka
Haven't explained this notation here.

00:25:32: Tivadar Danka
So so first of all,

00:25:35: Tivadar Danka
this double sided r letter means the set of real numbers. It's basically the real line. I won't talk about this

00:25:44: Tivadar Danka
here because

00:25:44: Tivadar Danka
discussing the real line can take

00:25:48: Tivadar Danka
semesters. So we just

00:25:49: Tivadar Danka
take this fermented.

00:25:51: Tivadar Danka
Real

00:25:52: Tivadar Danka
numbers and

00:25:55: Tivadar Danka
I write and here, which signifies that this is this is a random vector.

00:25:60: Tivadar Danka
So x is, again, x one and so on.

00:26:04: Tivadar Danka
XM. So c times x is c times x one and so on. C times

00:26:13: Tivadar Danka
x m. And this has a very, very simple interpretation.

00:26:19: Tivadar Danka
So if this is x,

00:26:20: Tivadar Danka
it's supposed that if c is larger than one,

00:26:24: Tivadar Danka
then

00:26:27: Tivadar Danka
this is c times x.

00:26:28: Tivadar Danka
This is this is a stretching.

00:26:31: Tivadar Danka
If c is somewhere between zero and one,

00:26:34: Tivadar Danka
then

00:26:39: Tivadar Danka
And we have our x here.

00:26:43: Tivadar Danka
This c times axe is like squeezing the vector itself. And if c

00:26:51: Tivadar Danka
is less than zero,

00:26:52: Tivadar Danka
then

00:26:54: Tivadar Danka
that is c is negative,

00:26:58: Tivadar Danka
the multiplying vector with c is is a stretching

00:27:03: Tivadar Danka
or squeezing and the reflection

00:27:06: Tivadar Danka
Right? So this changes orientations. This is c times x.

00:27:12: Tivadar Danka
Alright.

00:27:13: Tivadar Danka
That's it about about

00:27:15: Tivadar Danka
mathematical annotations.

00:27:17: Tivadar Danka
One one more thing.

00:27:19: Tivadar Danka
Want to mention to you here.

00:27:22: Tivadar Danka
Again, you might ask why do we have to

00:27:25: Tivadar Danka
compact numbers into into seemingly more complicated datasets instead of, you know, just working with the coordinates.

00:27:35: Tivadar Danka
I will I will give you, like, a very quick example. Hopefully, we are going to see this by the end of the workshop. So

00:27:42: Tivadar Danka
let's say that we have a dataset.

00:27:44: Tivadar Danka
You want to calculate the the average feature

00:27:48: Tivadar Danka
of of the dataset,

00:27:51: Tivadar Danka
then what you do is

00:27:52: Tivadar Danka
basically take

00:27:58: Tivadar Danka
I mean,

00:28:01: Tivadar Danka
this this should be

00:28:02: Tivadar Danka
m instead of m.

00:28:09: Tivadar Danka
So you you take the column wise sums or column wise averages of the of the dataset itself So this is this is, like, a coordinate wise operation.

00:28:33: Tivadar Danka
These these are the feature averages

00:28:36: Tivadar Danka
of of a tabular dataset.

00:28:37: Tivadar Danka
Bunch of sums, bunch of coordinates, whatever. However, if you use vectors, all of this stuff

00:28:45: Tivadar Danka
this here,

00:28:46: Tivadar Danka
everything,

00:28:48: Tivadar Danka
can be written

00:28:50: Tivadar Danka
with a with a single expression. So if you vectorize

00:28:53: Tivadar Danka
your notations, if you vectorize your code, this is

00:28:57: Tivadar Danka
simply

00:28:60: Tivadar Danka
single

00:29:01: Tivadar Danka
sum

00:29:03: Tivadar Danka
so to speak,

00:29:05: Tivadar Danka
Like, the

00:29:07: Tivadar Danka
this

00:29:08: Tivadar Danka
is this here.

00:29:11: Tivadar Danka
This is called vectorization, and

00:29:15: Tivadar Danka
it's a very important concept in machine learning.

00:29:18: Tivadar Danka
Basically, it allows you to write

00:29:20: Tivadar Danka
faster and and and more

00:29:23: Tivadar Danka
compact code.

00:29:24: Tivadar Danka
It is faster because because hardware is optimized for vectorized code.

00:29:28: Tivadar Danka
Or matrix sized code. Sorry.

00:29:32: Tivadar Danka
Yeah. Sorry for the off topic, but you have a great t shirt. Yeah. This is my favorite

00:29:36: Tivadar Danka
Polish black metal band. So I'm I'm a big black metal fan. This is something which I

00:29:42: Tivadar Danka
I don't usually like, people don't usually know about me.

00:29:47: Tivadar Danka
Yeah. This is this is a great band. Indeed, I hopefully, they really release an album.

00:29:52: Tivadar Danka
Ever in the future. One can hope.

00:29:56: Tivadar Danka
Let me just check q and a.

00:29:59: Tivadar Danka
There are there are no questions so far. Alright.

00:30:03: Tivadar Danka
We can we can proceed. Now we are going to get our hands dirty. And implement vectors and matrices in in in Python. Of course, we have NumPy, which is which is basically the the number one vector or matrix or or, in general, Tensor library.

00:30:22: Tivadar Danka
But we want to understand how vectors work. Right? So instead of relying on a third party

00:30:28: Tivadar Danka
tool like NumPy, we are going to make our own.

00:30:32: Tivadar Danka
And this is going to be the the

00:30:34: Tivadar Danka
the the most crucial part of this entire workshop.

00:30:38: Tivadar Danka
Let me switch to my to my code editor here.

00:30:43: Tivadar Danka
Alright. So

00:30:44: Tivadar Danka
second section. Vectors in Python.

00:30:52: Tivadar Danka
How would you implement vectors in Python? The first idea is to use

00:30:58: Tivadar Danka
toplos. Right? Toplos are are built in

00:31:01: Tivadar Danka
data type in in Python. It is defined by by

00:31:06: Tivadar Danka
putting your your objects in between parentheses and and commas.

00:31:11: Tivadar Danka
This is in theory, it represents the vector. It three that three-dimensional vector with coordinates one, two, and three.

00:31:17: Tivadar Danka
Alright. This is called a topple. Can change the topple of sorry. Not the topple, but the type of axe, which is a topple.

00:31:25: Tivadar Danka
Alright. It has

00:31:28: Tivadar Danka
a couple of issues.

00:31:29: Tivadar Danka
Each one versus the other. So issue number one you cannot really add these together. So if you write x plus y, what happens? It concatenates. The the

00:31:44: Tivadar Danka
the

00:31:46: Tivadar Danka
purpose itself.

00:31:48: Tivadar Danka
You can you can even write three times x. Which repeats x three times.

00:31:55: Tivadar Danka
This feature

00:31:56: Tivadar Danka
although it's very useful for general purpose programming,

00:31:60: Tivadar Danka
it's not useful at all for linear algebra. So we have to look for another

00:32:06: Tivadar Danka
data structure, so to speak.

00:32:08: Tivadar Danka
Second idea, we can use lists.

00:32:13: Tivadar Danka
Alright? So lists are they are called the work courses of Python. They are defined by putting putting your your objects in between square brackets instead of parentheses. The issue

00:32:26: Tivadar Danka
is the same.

00:32:28: Tivadar Danka
Addition, multiplication,

00:32:31: Tivadar Danka
This this works like concatenation.

00:32:35: Tivadar Danka
So

00:32:36: Tivadar Danka
instead of of

00:32:38: Tivadar Danka
hacking our way

00:32:40: Tivadar Danka
with with with and and

00:32:41: Tivadar Danka
and

00:32:43: Tivadar Danka
this we are going to

00:32:44: Tivadar Danka
basically implement our own

00:32:48: Tivadar Danka
vector class.

00:32:51: Tivadar Danka
We are going to use object oriented programming in Python.

00:32:54: Tivadar Danka
It's not a problem if you are not familiar with object oriented Python. Even though this is not not an OOP tutorial,

00:33:03: Tivadar Danka
I I hope that it will be simple for you to follow if you don't know the if you don't know the object oriented

00:33:11: Tivadar Danka
aspect of Python,

00:33:12: Tivadar Danka
just let me know in the questions. Alright? And I will periodically check the the chat and and q and a.

00:33:20: Tivadar Danka
Classes in Python are defined by the the keyword

00:33:24: Tivadar Danka
class. So here, we defined

00:33:27: Tivadar Danka
vector class, which so far does really nothing. It's just just a empty class in itself.

00:33:35: Tivadar Danka
So

00:33:36: Tivadar Danka
what we want

00:33:37: Tivadar Danka
to do here what I want to to to do is to write something like vector one two three.

00:33:44: Tivadar Danka
I want this

00:33:47: Tivadar Danka
line of code to represent a vector, like a three-dimensional vector with coordinate s one.

00:33:52: Tivadar Danka
Two, and and and three.

00:33:56: Tivadar Danka
Usually, whenever I'm creating my own classes, I start with the interfaces. I start by by figuring out how do I want to use

00:34:04: Tivadar Danka
the the class itself.

00:34:06: Tivadar Danka
And then

00:34:08: Tivadar Danka
create the implementation accordingly.

00:34:11: Tivadar Danka
So, basically, what you do here is pass, like, a a a couple of numbers or coordinates to the to the

00:34:16: Tivadar Danka
constructor of the vector itself.

00:34:19: Tivadar Danka
This is implemented by by the the the method in it.

00:34:24: Tivadar Danka
Surrounded by by double underscores. This this this is called the magic method. Every every method surrounded by double underscore are called magic methods.

00:34:36: Tivadar Danka
These are the

00:34:36: Tivadar Danka
meats and bones of of of a

00:34:40: Tivadar Danka
of a Python

00:34:41: Tivadar Danka
class.

00:34:45: Tivadar Danka
The first argument of every Python

00:34:50: Tivadar Danka
class method or not class method, every method of a class

00:34:54: Tivadar Danka
is itself, which

00:34:56: Tivadar Danka
refers

00:34:58: Tivadar Danka
to the the the object itself from which

00:35:00: Tivadar Danka
the the the method is called. And the the second I mean, after after that,

00:35:06: Tivadar Danka
the the arguments of the methods follow.

00:35:08: Tivadar Danka
Here, we just simply pass a list of coordinates. Right?

00:35:20: Tivadar Danka
So here, I will just take the the chords argument and store it in in the chords attribute. And I guess this is not exactly what I wrote here because then I will have to surround it by either in in a parenthesis or or or square

00:35:36: Tivadar Danka
brackets, the square parenthesis, whatever. So this here,

00:35:41: Tivadar Danka
is is a vector. We can access the the coordinates of this vector by taking a look at the chords

00:35:48: Tivadar Danka
attribute. Alright.

00:35:53: Tivadar Danka
Second thing I always do whenever I'm implementing my custom class is to give it a nice string representation line. Right now, I am working in a Jupyter notebook environment, which

00:36:04: Tivadar Danka
might not be the case for you whenever you work on future projects.

00:36:08: Tivadar Danka
However, if if I print

00:36:10: Tivadar Danka
if I write x to a cell and execute the cell,

00:36:13: Tivadar Danka
it's the same as as actually printing out the variable itself.

00:36:20: Tivadar Danka
The issue, this here,

00:36:22: Tivadar Danka
is not informative at all. It it reveals nothing about the the properties of of the of the x object. That are important for us.

00:36:32: Tivadar Danka
It tells us the the class

00:36:36: Tivadar Danka
and it tells us the memory address of of the object itself. But

00:36:39: Tivadar Danka
I mean, we don't want to do linear algebra right now, so we don't really care about that.

00:36:44: Tivadar Danka
This is solved by adding a string representation

00:36:47: Tivadar Danka
I mean,

00:36:48: Tivadar Danka
what we are talking about here is not strictly

00:36:52: Tivadar Danka
machine learning. It's just

00:36:54: Tivadar Danka
good Python practice, but I want to include it for for

00:36:58: Tivadar Danka
for sake of completion, if you like it. So here, instead of

00:37:05: Tivadar Danka
copying and pasting,

00:37:08: Tivadar Danka
so now I'm going to enhance this sector class. And instead of copying and pasting, the code I wrote here, I will just subclass it.

00:37:16: Tivadar Danka
And and you can you can think about

00:37:19: Tivadar Danka
this.

00:37:20: Tivadar Danka
As

00:37:21: Tivadar Danka
me copying and pasting the calls from here to here.

00:37:24: Tivadar Danka
Alright?

00:37:27: Tivadar Danka
So let's continue with the string representation.

00:37:30: Tivadar Danka
It is it is given by the it is implemented by the magic method wrapper.

00:37:36: Tivadar Danka
And, basically, it returns a string.

00:37:39: Tivadar Danka
So here, we can return, like, a format string.

00:37:48: Tivadar Danka
If if you don't understand, let let me show you an example.

00:37:55: Tivadar Danka
Now

00:37:56: Tivadar Danka
if we print out x, we we get this nice little vector representation.

00:38:01: Tivadar Danka
Alright. So it it reflects the the the state of of of x.

00:38:11: Tivadar Danka
Alright.

00:38:16: Tivadar Danka
Questions? Let me check for questions.

00:38:19: Tivadar Danka
So the main reason for vectorization is to run a

00:38:22: Tivadar Danka
more compact and faster code.

00:38:24: Tivadar Danka
Essentially. Yes. It's also more beautiful. As a as a mathematician, I care about

00:38:32: Tivadar Danka
beautiful code and beautiful formulas, but you shouldn't I think it's not not

00:38:36: Tivadar Danka
not an indicative of of of a good professional. I'm just, like, I'm a mathematician, so I always care about mathematical beauty.

00:38:44: Tivadar Danka
In this case, can the memory address change

00:38:47: Tivadar Danka
mean, the memory memory address of the object itself

00:38:50: Tivadar Danka
I guess, it it it won't change throughout its lifetime. But I'm not a Python expert. Don't take my word for it.

00:38:60: Tivadar Danka
If you if you re reinstalliate the object,

00:39:03: Tivadar Danka
even with the same same attributes, the memory address will change.

00:39:08: Tivadar Danka
For sure.

00:39:09: Tivadar Danka
Or or with a very, very, very high probability.

00:39:14: Tivadar Danka
Alright. Let's continue.

00:39:18: Tivadar Danka
Alright. What is missing?

00:39:22: Tivadar Danka
I mean, if we have a Python list, say, one, two, three, they can access the element of this list by indexing.

00:39:29: Tivadar Danka
Right? This is

00:39:32: Tivadar Danka
not something we can do.

00:39:34: Tivadar Danka
Right now. Why? Because this is not

00:39:37: Tivadar Danka
so called a subscriptable

00:39:40: Tivadar Danka
object.

00:39:41: Tivadar Danka
What we also want to to do

00:39:45: Tivadar Danka
we also also want to basically assign

00:39:48: Tivadar Danka
certain

00:39:51: Tivadar Danka
certain elements of the list of stuff. So if I have a list l,

00:39:55: Tivadar Danka
containing one, two, and three, And if I write l zero equals, I'll start to need to set the the zeros

00:40:04: Tivadar Danka
element, which is one to 13. This doesn't work for our vectors either. So

00:40:10: Tivadar Danka
I

00:40:11: Tivadar Danka
we will implement to get item and set item,

00:40:16: Tivadar Danka
class.

00:40:16: Tivadar Danka
They're not class, magic method.

00:40:20: Tivadar Danka
Because

00:40:22: Tivadar Danka
item so called getting and setting are implemented by by them.

00:40:26: Tivadar Danka
Let me just do the work, and you will see. Again, I'm subclassing the vector. Here is the

00:40:37: Tivadar Danka
getitem

00:40:39: Tivadar Danka
method. I mean, as I mentioned, the first argument is always self.

00:40:43: Tivadar Danka
And the the real argument of the getitem method

00:40:46: Tivadar Danka
which is called whenever you use

00:40:48: Tivadar Danka
indexing. The real argument is the index, which I named with IDX, but you can name this whatever Let's just

00:40:60: Tivadar Danka
keep it at index. And the second method I want to implement is a set item. Is responsible for you actually able to set the the

00:41:12: Tivadar Danka
attributes with b d with

00:41:14: Tivadar Danka
with

00:41:15: Tivadar Danka
indexes. Sorry. It it also contains an index and the value.

00:41:21: Tivadar Danka
To which you want to set the the

00:41:24: Tivadar Danka
coordinates

00:41:25: Tivadar Danka
that specific coordinate. It's actually very simple. We just we just access the chords attribute and return. And you use indexing on the chords attribute itself.

00:41:40: Tivadar Danka
Because here,

00:41:42: Tivadar Danka
to the vector itself, we passed the list

00:41:46: Tivadar Danka
I mean, we can we can pass a couple

00:41:48: Tivadar Danka
also. I wouldn't recommend it because tappls are are immutable.

00:41:55: Tivadar Danka
A bit of tension here. So so if you have have a top of one two, and three, I mean, they can they can use indexing for sure, but we cannot change the value itself.

00:42:11: Tivadar Danka
Because it doesn't support item assignment.

00:42:14: Tivadar Danka
It's called immutable.

00:42:16: Tivadar Danka
Alright. So

00:42:18: Tivadar Danka
again, method.

00:42:22: Tivadar Danka
Actually, I don't need to vector anything. I just need to to use basically the the set item method of the of the list

00:42:29: Tivadar Danka
itself.

00:42:34: Tivadar Danka
Of course, I don't need to click on the set item. I can just use

00:42:39: Tivadar Danka
good old assignment operator

00:42:44: Tivadar Danka
This this is the this is the the assignment operator.

00:42:48: Tivadar Danka
Okay. Now, again,

00:42:51: Tivadar Danka
every every time I update the vector class, I need to reinstantiate our old

00:42:56: Tivadar Danka
vector

00:42:57: Tivadar Danka
instances because

00:42:60: Tivadar Danka
they have, like, they have different classes. Okay. So we can we can write write x out. X zero is one, so it works. So let's

00:43:11: Tivadar Danka
modify the value of x zero.

00:43:16: Tivadar Danka
It works as well.

00:43:19: Tivadar Danka
Alright.

00:43:21: Tivadar Danka
So

00:43:24: Tivadar Danka
What is the most important thing? Before that, let me just share for for questions and comments.

00:43:32: Tivadar Danka
Alright. Nothing nothing new. Let me just drink

00:43:36: Tivadar Danka
sip of water.

00:43:43: Tivadar Danka
And then we are going to to take a short break. Not not right now. Let me just implement back to the addition and back to multiplication. After that, we are going to take, like, a very short

00:43:55: Tivadar Danka
ten minutes break. I will let you know.

00:43:57: Tivadar Danka
Let's keep working meanwhile. So what I want is to write expressions like this, where x and x and y are vectors.

00:44:12: Tivadar Danka
This doesn't work yet, but we build on this. These are done by the the add

00:44:19: Tivadar Danka
magic method.

00:44:25: Tivadar Danka
Again, I'm or odd.

00:44:28: Tivadar Danka
Vector class. And I'm going to going to implement a magic method called add. Self is the instance itself.

00:44:37: Tivadar Danka
For which the the the

00:44:41: Tivadar Danka
to discard.

00:44:43: Tivadar Danka
A quick comment before I do that.

00:44:46: Tivadar Danka
So x plus y.

00:44:48: Tivadar Danka
Is essentially equivalent to x dot

00:44:53: Tivadar Danka
add y

00:44:56: Tivadar Danka
This is called I think this is called the the the infix notation.

00:45:00: Tivadar Danka
For for the additional stuff.

00:45:03: Tivadar Danka
Or maybe I'm wrong.

00:45:04: Tivadar Danka
Whatever. The point is add is the the the method that implements addition to this.

00:45:12: Tivadar Danka
Doesn't work.

00:45:13: Tivadar Danka
At the moment. So the second argument of add is other. Which is going to be another

00:45:20: Tivadar Danka
vector instance.

00:45:23: Tivadar Danka
What I do here

00:45:24: Tivadar Danka
is that

00:45:25: Tivadar Danka
I create a new vector instance

00:45:28: Tivadar Danka
whose coordinates are are the the the

00:45:31: Tivadar Danka
the the coordinate wise

00:45:32: Tivadar Danka
sums of of self and other.

00:45:38: Tivadar Danka
I'm going to do that by by by using a list comprehension here. So I hope you understand this expression.

00:45:54: Tivadar Danka
Is a list of coordinates. I'm going to going to take the the the

00:45:59: Tivadar Danka
basically, the ZIP of of these these two methods. Let me just demonstrate this for you quickly.

00:46:06: Tivadar Danka
Let's say we have

00:46:08: Tivadar Danka
I don't know, l l one.

00:46:09: Tivadar Danka
Is one, two, and three. L l two is

00:46:16: Tivadar Danka
I don't know, a.

00:46:18: Tivadar Danka
B, and a c.

00:46:21: Tivadar Danka
And

00:46:24: Tivadar Danka
the zip of l one and l two first of all, this not a list is something called

00:46:30: Tivadar Danka
iterator or a generator.

00:46:32: Tivadar Danka
I always

00:46:33: Tivadar Danka
conflate these two terms. But we can we can convert it to a list explicitly. Right? So what what it does

00:46:40: Tivadar Danka
is it takes the corresponding coordinates and converts them into a tuple Right? So so here, we are just

00:46:46: Tivadar Danka
iterating through the the ZIP of of the coordinates of self and the coordinates of of other

00:46:54: Tivadar Danka
Alright.

00:47:00: Tivadar Danka
Let's try it out.

00:47:10: Tivadar Danka
And it works because because one plus four is five,

00:47:15: Tivadar Danka
two plus five is seven,

00:47:18: Tivadar Danka
Three plus six is

00:47:20: Tivadar Danka
is nine.

00:47:22: Tivadar Danka
Great.

00:47:24: Tivadar Danka
I mean,

00:47:26: Tivadar Danka
in principle, we can write things like x plus five, but it doesn't rework because because this five here is not

00:47:34: Tivadar Danka
not not a vector instance itself.

00:47:37: Tivadar Danka
Actually, here, other

00:47:40: Tivadar Danka
does not have to be a vector instance either. It just has to have a cord attribute.

00:47:46: Tivadar Danka
His.

00:47:48: Tivadar Danka
Again, this this code, not what I wrote here, it can fail

00:47:52: Tivadar Danka
in so many ways. For instance, if if self and other have different dimensions, it fails. Or it it might not fail. It might might produce the correct might might produce a result which is incorrect.

00:48:05: Tivadar Danka
Again, our our goal is, like,

00:48:08: Tivadar Danka
insight

00:48:08: Tivadar Danka
We are not building frameworks

00:48:11: Tivadar Danka
just yet.

00:48:13: Tivadar Danka
Again,

00:48:16: Tivadar Danka
what I also want to do is I want to write expressions like two times x.

00:48:20: Tivadar Danka
Which should should be should be vector two, four, and six.

00:48:32: Tivadar Danka
Right? Because this this is this is what what

00:48:34: Tivadar Danka
scalar multiplication is.

00:48:36: Tivadar Danka
Doesn't work just yet, but it is implemented by the method of the rule. Let's implement

00:48:44: Tivadar Danka
the metal door. Is very similar to to add in its signature It takes self

00:48:58: Tivadar Danka
and other

00:49:01: Tivadar Danka
which is going to be

00:49:04: Tivadar Danka
a number type.

00:49:05: Tivadar Danka
This moment. So here, what I do is I return a new vector instance Again, I use this comprehension.

00:49:14: Tivadar Danka
X

00:49:15: Tivadar Danka
times

00:49:16: Tivadar Danka
other for x in

00:49:20: Tivadar Danka
south dot cords. I don't even need to use the zip.

00:49:24: Tivadar Danka
Here because other is just a simple number type.

00:49:27: Tivadar Danka
Let's try it out. X is

00:49:31: Tivadar Danka
vector one, two, and three.

00:49:33: Tivadar Danka
X times two point zero.

00:49:37: Tivadar Danka
Which it returns the correct result.

00:49:41: Tivadar Danka
Mean, if if I multiply it to the flow, the results are are float float numbers, if I multiply it within the result,

00:49:48: Tivadar Danka
contains integers.

00:49:48: Tivadar Danka
Not important for us right now.

00:49:52: Tivadar Danka
It works.

00:49:52: Tivadar Danka
There's an issue. Two times x.

00:49:57: Tivadar Danka
It doesn't work.

00:49:59: Tivadar Danka
And the reason is

00:50:00: Tivadar Danka
is two times x I mean, not two times, but more most

00:50:04: Tivadar Danka
if I want to be precise, this is two star x. Two star x is two more

00:50:14: Tivadar Danka
x.

00:50:15: Tivadar Danka
And

00:50:16: Tivadar Danka
the the rule method of true

00:50:18: Tivadar Danka
does not work for for vector instances. So what we do here is to implement something called the right multiplication.

00:50:29: Tivadar Danka
Which is called

00:50:31: Tivadar Danka
whenever this expression fails. So this expression caused to that mole x.

00:50:37: Tivadar Danka
It fails,

00:50:42: Tivadar Danka
the part of interpreter calls

00:50:44: Tivadar Danka
x dot m will

00:50:49: Tivadar Danka
two. But this x dot armul is not implemented

00:50:55: Tivadar Danka
just yet, but we can do it.

00:51:03: Tivadar Danka
Signature is very similar to to add a new Basically, the implementation

00:51:15: Tivadar Danka
is

00:51:16: Tivadar Danka
literally the same. I can just copy and paste.

00:51:23: Tivadar Danka
Alright. And now now it will work. X equals vector one, two, and three.

00:51:30: Tivadar Danka
And

00:51:32: Tivadar Danka
two times x.

00:51:34: Tivadar Danka
Works perfectly. X times two also works perfectly.

00:51:41: Tivadar Danka
Alright.

00:51:44: Tivadar Danka
Let's take a quick

00:51:45: Tivadar Danka
ten minute break.

00:51:48: Tivadar Danka
Before that, let me answer the questions. Let me check for for questions in the q and a.

00:51:56: Tivadar Danka
So why is working with vectors faster because the data is already loaded in memory? I mean, Divya talk about it after the break because then I want to throw everything away that you built, and I want to show you NumPy. And

00:52:08: Tivadar Danka
during that time, I will explain why it is faster.

00:52:13: Tivadar Danka
To just stay tuned.

00:52:17: Tivadar Danka
Questions in the chat.

00:52:30: Tivadar Danka
Can someone remind me what the purpose was for vector again?

00:52:34: Tivadar Danka
Yeah.

00:52:35: Tivadar Danka
I will I will tell you. Okay. So so here,

00:52:40: Tivadar Danka
just take take a look at this

00:52:42: Tivadar Danka
here, the highlighted

00:52:44: Tivadar Danka
highlighted piece of code. It only contains one method.

00:52:48: Tivadar Danka
Right?

00:52:48: Tivadar Danka
So

00:52:49: Tivadar Danka
if if I if I don't write if I don't subclass it,

00:52:54: Tivadar Danka
I will have to

00:52:57: Tivadar Danka
copy and paste every previous method.

00:53:03: Tivadar Danka
For it to work.

00:53:07: Tivadar Danka
But I don't want to do that because I'm lazy. And because I also want to save you some time.

00:53:14: Tivadar Danka
So here,

00:53:15: Tivadar Danka
instead, I subclass

00:53:17: Tivadar Danka
to have these methods available because these are already implemented. They work perfectly. I don't want to

00:53:24: Tivadar Danka
throw them away.

00:53:26: Tivadar Danka
Even even if I did want to throw them away,

00:53:28: Tivadar Danka
I can just

00:53:30: Tivadar Danka
override them.

00:53:32: Tivadar Danka
Order to avoid all these copy and paste, I just subclass it.

00:53:39: Tivadar Danka
It's a Jupyter Notebook hack.

00:53:41: Tivadar Danka
Doesn't work in production environments. Doesn't work.

00:53:44: Tivadar Danka
Then you are not

00:53:46: Tivadar Danka
really in in Jupyter Notebooks. This is just for for for demonstration.

00:53:52: Tivadar Danka
Is there an easy way to see all the methods a given class has? I mean, I I wouldn't call this easy, but that's that's the way Let me let me show it to you.

00:54:08: Tivadar Danka
So

00:54:09: Tivadar Danka
x is a is a vector instance. Python has a function called called dear. For for directory. And if you if you call it it lists all all all the methods, all the possible methods, attributes. However,

00:54:24: Tivadar Danka
this is going to be, like, a long, long list of metals because

00:54:29: Tivadar Danka
because default Python

00:54:32: Tivadar Danka
methods already have a bunch of bunch of

00:54:34: Tivadar Danka
methods implemented.

00:54:36: Tivadar Danka
And bigger will will contain them all.

00:54:45: Tivadar Danka
Alright.

00:54:46: Tivadar Danka
I think there are there are no more questions.

00:54:48: Tivadar Danka
Right now.

00:54:54: Tivadar Danka
So zip zip is vector additions.

00:54:56: Tivadar Danka
Zip is not vector addition. Zip is

00:54:60: Tivadar Danka
more like composing tuples, like, coordinate wise tuples.

00:55:04: Tivadar Danka
Zip is just just a just a a Python trick.

00:55:08: Tivadar Danka
Which is very useful.

00:55:09: Tivadar Danka
For general purpose programming also.

00:55:14: Tivadar Danka
I hope I answered what

00:55:16: Tivadar Danka
your questions. For adding two vectors, why are we doing scaler additions? I mean, we are we are doing a coordinate wise addition Right? And we are doing this

00:55:28: Tivadar Danka
because we want to gain insight into how vectors really work. I could just use NumPy

00:55:36: Tivadar Danka
I choose not to.

00:55:36: Tivadar Danka
Because I'd want to show you

00:55:38: Tivadar Danka
how how

00:55:40: Tivadar Danka
vectors really work, what makes them a vector, and so on. And I think, like,

00:55:46: Tivadar Danka
it's it's my my

00:55:47: Tivadar Danka
number one guiding principle in in computer science that if you want to understand something, you have to implement it from scratch.

00:55:55: Tivadar Danka
I only understand neural networks because I took the time and built a neural network

00:56:00: Tivadar Danka
framework by myself. It took me

00:56:04: Tivadar Danka
a couple of months, but I did it. And and now I think I understand the raw networks.

00:56:08: Tivadar Danka
Same day with SQL. Like, I I I built built a mini relational database in in Python back in the day.

00:56:16: Tivadar Danka
Because for me, the only way to understand something is to implement is from scratch.

00:56:20: Tivadar Danka
And this this impacts the the way I I teach things.

00:56:24: Tivadar Danka
I like to focus on the fundamentals because, overall, I think

00:56:29: Tivadar Danka
if if you if you focus on on the tooling of of machine learning, you can get behind pretty fast because tooling changes

00:56:37: Tivadar Danka
by the hour.

00:56:39: Tivadar Danka
But if you have have a very strong set of fundamentals,

00:56:43: Tivadar Danka
you are set for life.

00:56:45: Tivadar Danka
Because if you understand the basics,

00:56:48: Tivadar Danka
you you can understand everything in such a short time.

00:56:51: Tivadar Danka
It's

00:56:52: Tivadar Danka
not even funny.

00:56:53: Tivadar Danka
Right? So so fundamentals, I think they are a superpower for you. And

00:56:58: Tivadar Danka
this workshop is all about the fundamentals. Alright? So let's just take a ten minute break for for you guys to to

00:57:05: Tivadar Danka
to

00:57:06: Tivadar Danka
take take a breath of fresh air. I also

00:57:08: Tivadar Danka
will also probably take a breath of fresh air and and see you

00:57:12: Tivadar Danka
you know,

00:57:12: Tivadar Danka
couple of minutes.

01:08:09: Abhishek Kaushik
Hi,

01:08:10: Abhishek Kaushik
You're on mute.

01:08:11: Abhishek Kaushik
Tivada, you're on mute.

01:08:12: Tivadar Danka
Hey. Can you hear me now?

01:08:16: Tivadar Danka
Can you hear me now?

01:08:19: Tivadar Danka
Alright. You can you can hear me now. Sorry. I was on mute. So let me restart the this whole little monologue, which I I told you about. I have have a mute button on my microphone.

01:08:32: Tivadar Danka
Here, and I forgot to press it again.

01:08:36: Tivadar Danka
Hopefully, I won't make this mistake after the second break. Alright.

01:08:41: Tivadar Danka
We did our custom vector implementation.

01:08:44: Tivadar Danka
However, you should not use this in practice because it is still very slow. It builds upon lists.

01:08:52: Tivadar Danka
Which is

01:08:52: Tivadar Danka
although very good for general purpose programming, it is still

01:08:57: Tivadar Danka
extremely slow.

01:08:59: Tivadar Danka
For for numerical computing.

01:09:01: Tivadar Danka
So what we use is Python.

01:09:05: Tivadar Danka
So sorry. What we use is NumPy. We also use Python, but but NumPy is the package we want to use. Short for numerical Python. I think it's it's one of the one of the killer features of Python, so to speak.

01:09:22: Tivadar Danka
Most often, we import this whole packages empty.

01:09:26: Tivadar Danka
And the the core structure of of NumPy

01:09:30: Tivadar Danka
is something called the n dimensional array. Which you can construct by the empty empty array. Function. So if I write

01:09:40: Tivadar Danka
this here, x equals m p dot array

01:09:43: Tivadar Danka
one two three.

01:09:45: Tivadar Danka
This is going to be

01:09:48: Tivadar Danka
an array, which will effectively serve as a vector. Let me instantiate another one.

01:10:04: Tivadar Danka
To see if the addition really works. Yeah. You can see that vector wise addition the net vector wise, coordinate wise addition.

01:10:13: Tivadar Danka
Otherwise, no. No. No. No. No. No. No. No. No. No. No. No

01:10:16: Tivadar Danka
We can also multiply or vector with a scalar, so everything is is is fine. We can we can even multiply them, take the coordinate wise, for vector factors.

01:10:28: Tivadar Danka
Because it it it even works.

01:10:32: Tivadar Danka
Okay. Why do we use NumPy? Yeah.

01:10:37: Tivadar Danka
One one one,

01:10:40: Tivadar Danka
short

01:10:40: Tivadar Danka
command. For instance, we have

01:10:44: Tivadar Danka
again, why why is is is vectorization better? So we have the exponential function.

01:10:50: Tivadar Danka
Which you are probably all very familiar of.

01:10:53: Tivadar Danka
If not, never mind. It's it's basically a function that maps a real number to a real number. So I don't know. This is a function that takes

01:11:05: Tivadar Danka
a number as an argument. However,

01:11:08: Tivadar Danka
because of of of, non py magic, they can also put

01:11:14: Tivadar Danka
arrays here.

01:11:16: Tivadar Danka
I can even put higher dimensional arrays here. This makes it very good because because the very same code

01:11:22: Tivadar Danka
vlan

01:11:24: Tivadar Danka
for for for vectors and matrices,

01:11:27: Tivadar Danka
making vectorization

01:11:28: Tivadar Danka
really easy.

01:11:31: Tivadar Danka
We had a question before the break. Why is is NumPy

01:11:36: Tivadar Danka
faster than Python?

01:11:39: Tivadar Danka
Basically, Python's lists like, let me just explain this.

01:11:45: Tivadar Danka
Python's, arrays are very sorry. Python's lists are very flexible. So

01:11:51: Tivadar Danka
you can store

01:11:52: Tivadar Danka
anything in a in a Python if

01:11:56: Tivadar Danka
You can you can store list itself. You can even even store

01:12:01: Tivadar Danka
the vector class itself

01:12:04: Tivadar Danka
in the Python list. You see?

01:12:08: Tivadar Danka
These are very flexible.

01:12:10: Tivadar Danka
But they are

01:12:12: Tivadar Danka
not fast.

01:12:15: Tivadar Danka
I think I can demonstrate you the issue here.

01:12:19: Tivadar Danka
So the ID function in Python

01:12:23: Tivadar Danka
returns the memory address of this given object.

01:12:28: Tivadar Danka
So the the the first object, which is the the

01:12:31: Tivadar Danka
the

01:12:32: Tivadar Danka
number one, stored at this address.

01:12:38: Tivadar Danka
The second object is stored at this address. And you you can already see that these memory addresses

01:12:44: Tivadar Danka
are quite far from each other.

01:12:51: Tivadar Danka
And so on.

01:12:52: Tivadar Danka
You see that this is

01:12:55: Tivadar Danka
far, far, far away. NumPy implements everything in c. And if you have experience with c, you know that that

01:13:05: Tivadar Danka
a c array

01:13:07: Tivadar Danka
stores the numbers subsequently

01:13:10: Tivadar Danka
in memory.

01:13:12: Tivadar Danka
So so the the next number in your array is actually at the at the neighbor neighbor,

01:13:19: Tivadar Danka
memory address.

01:13:20: Tivadar Danka
Which makes it

01:13:22: Tivadar Danka
very fast. Like, this is one reason why it is faster.

01:13:26: Tivadar Danka
Second thing is that that

01:13:30: Tivadar Danka
addition like, basically, vector operations are also implemented in in in c.

01:13:36: Tivadar Danka
Because because non pair arrays are a bit more restrictive. For instance, like, you can you you saw here that here, you can

01:13:44: Tivadar Danka
basically store any type of object in a single array. You cannot do this in NumPy.

01:13:49: Tivadar Danka
I mean,

01:13:56: Tivadar Danka
here, basically, it converted this this here to to to a string.

01:14:00: Tivadar Danka
But you

01:14:02: Tivadar Danka
probably cannot do this.

01:14:04: Tivadar Danka
Yeah. I mean, you you you you can't you can't even do this. Sorry. I was wrong. But

01:14:09: Tivadar Danka
the data type here of of this array is a field the the general, so it won't be that efficient.

01:14:18: Tivadar Danka
Hope that that, your your your question.

01:14:23: Tivadar Danka
Alright.

01:14:27: Tivadar Danka
One more thing I want to talk about regarding

01:14:31: Tivadar Danka
vectors.

01:14:32: Tivadar Danka
Which is something called the dot product. A very important vector operation. Let me turn the lights on.

01:14:42: Tivadar Danka
In my small and not really properly lit room.

01:14:47: Tivadar Danka
If

01:14:48: Tivadar Danka
x and y are are two vectors,

01:14:53: Tivadar Danka
we can take the dot product

01:14:56: Tivadar Danka
and

01:14:57: Tivadar Danka
which is denoted by this small bot between the the two vectors.

01:15:05: Tivadar Danka
And this is

01:15:07: Tivadar Danka
defined by

01:15:11: Tivadar Danka
the sum

01:15:12: Tivadar Danka
of

01:15:13: Tivadar Danka
of the coordinates

01:15:14: Tivadar Danka
products.

01:15:16: Tivadar Danka
I'm going to give you a quick example. So one I mean, let's just give a more general example.

01:15:24: Tivadar Danka
For instance, in two dimensions,

01:15:26: Tivadar Danka
So x is x one, x two,

01:15:30: Tivadar Danka
y is y one,

01:15:33: Tivadar Danka
y two.

01:15:35: Tivadar Danka
So x and y is x one times

01:15:40: Tivadar Danka
y one plus x two times y two.

01:15:45: Tivadar Danka
This is something which you you you

01:15:47: Tivadar Danka
probably learned in in

01:15:49: Tivadar Danka
high school.

01:15:51: Tivadar Danka
More likely in your high school geometry class. Dot product

01:15:56: Tivadar Danka
is a very, very, very useful

01:15:59: Tivadar Danka
operation for for vectors.

01:16:03: Tivadar Danka
Why it's useful.

01:16:05: Tivadar Danka
For one, we can we can write

01:16:08: Tivadar Danka
the linear regression model

01:16:12: Tivadar Danka
as a dot product. I mean,

01:16:14: Tivadar Danka
back to linear regression.

01:16:17: Tivadar Danka
Just a reminder. So linear regression

01:16:23: Tivadar Danka
a is a function a times x.

01:16:25: Tivadar Danka
Plus b.

01:16:26: Tivadar Danka
Where x is your feature.

01:16:33: Tivadar Danka
So x is the feature. But what if you have more features?

01:16:40: Tivadar Danka
More than one?

01:16:44: Tivadar Danka
If you are diving deep into multivariable territory, for instance, you have n features, like in the diabetes datasets, we we saw we have 10 features.

01:16:56: Tivadar Danka
Here,

01:16:56: Tivadar Danka
what you can do is to

01:17:00: Tivadar Danka
take I mean,

01:17:03: Tivadar Danka
a number

01:17:04: Tivadar Danka
a real number, w one, multiply it by x one and so on. Take this w w n times x n.

01:17:16: Tivadar Danka
This here is a direct analog of this this method.

01:17:20: Tivadar Danka
Only in higher dimensions.

01:17:23: Tivadar Danka
What you see here is is the multivariate linear regression. Coincidentally,

01:17:34: Tivadar Danka
this is also the dot sorry.

01:17:37: Tivadar Danka
The dot product of

01:17:39: Tivadar Danka
x and this w vector where w is called the weight vector, and it is composed of w one and so on.

01:17:47: Tivadar Danka
W m.

01:17:50: Tivadar Danka
One quick note. Dot product is usually I mean, if you are reading a mathematics textbook,

01:17:56: Tivadar Danka
the dot product of two vectors

01:17:60: Tivadar Danka
are usually denoted in these in these, bracket quotation. These are equivalent. This is just notation. I will I will use this one here.

01:18:14: Tivadar Danka
Sometimes I I I forget about myself, and I use this notation. This is just to

01:18:20: Tivadar Danka
in mind.

01:18:22: Tivadar Danka
So

01:18:23: Tivadar Danka
here, basically, we have a, like, a very general

01:18:28: Tivadar Danka
linear regression model.

01:18:34: Tivadar Danka
And now you can already see the the the advantage of factorization. Take a look at this here.

01:18:44: Tivadar Danka
It's much simpler.

01:18:47: Tivadar Danka
Than this one.

01:18:49: Tivadar Danka
And this is not just just just just like a like a an aesthetical difference. This makes your job much easier as programmer.

01:19:01: Tivadar Danka
Developer, machine learning engineer.

01:19:05: Tivadar Danka
Doppler is also strongly related to to cosine similarity. We are not going to talk about it

01:19:12: Tivadar Danka
right now.

01:19:14: Tivadar Danka
Let's switch back to to Python and see

01:19:20: Tivadar Danka
how the dot product is implemented.

01:19:24: Tivadar Danka
Will take a look at the chat.

01:19:27: Tivadar Danka
NumPy uses force and libraries.

01:19:31: Tivadar Danka
Probably. Again, I'm not an expert on that question. But

01:19:35: Tivadar Danka
NumPy itself is implemented in c.

01:19:38: Tivadar Danka
Which in turn might use fortune libraries.

01:19:40: Tivadar Danka
That that I don't know.

01:19:43: Tivadar Danka
But but I know for sure that that's

01:19:45: Tivadar Danka
I think.

01:19:46: Tivadar Danka
NumPy arrays follow the implementation of c arrays.

01:19:52: Tivadar Danka
But, please, like, if if I mean, you can correct me in the chat.

01:20:04: Tivadar Danka
You last on the second reason to use vectors. Can you can you reexplain I mean,

01:20:10: Tivadar Danka
I'm not sure.

01:20:12: Tivadar Danka
What what you you you meant here.

01:20:19: Tivadar Danka
I I to to to be honest, I don't don't,

01:20:22: Tivadar Danka
don't know what this question refers to.

01:20:24: Tivadar Danka
But it is this is probably because of me.

01:20:28: Tivadar Danka
My mind is full of adrenaline right now,

01:20:30: Tivadar Danka
because because I'm teaching this class live to you guys, and sometimes, I I I have these these, like,

01:20:38: Tivadar Danka
blank memory errors.

01:20:40: Tivadar Danka
This is probably one of those instances

01:20:43: Tivadar Danka
Hopefully hopefully, I will be able to recall

01:20:46: Tivadar Danka
my my one of my previous thoughts.

01:20:48: Tivadar Danka
If not, I'm sorry in advance.

01:20:54: Tivadar Danka
Alright.

01:20:58: Tivadar Danka
We can we can go back to code a bit more. Again, like, we have we have vectors here.

01:21:10: Tivadar Danka
I can I can implement the the version of dot products?

01:21:15: Tivadar Danka
Without NumPy. This is just just for for fun.

01:21:20: Tivadar Danka
Again,

01:21:22: Tivadar Danka
will implement this with the with the list comprehension.

01:21:37: Tivadar Danka
Mean, here, x and y are just simply assumed to be lists. This is just like

01:21:46: Tivadar Danka
just to have this this thing in in front of us. So that one, two, and three.

01:21:52: Tivadar Danka
Four, five, and six.

01:21:57: Tivadar Danka
Is a dot product.

01:22:00: Tivadar Danka
This implementation doesn't use our custom vector class writing. It's just

01:22:06: Tivadar Danka
basically the the the mere translation of the mathematical definition. However,

01:22:12: Tivadar Danka
we want to use NumPy.

01:22:14: Tivadar Danka
For that.

01:22:16: Tivadar Danka
Want to use NumPy for everything. So NumPy has a dot collecting currently in the dots.

01:22:22: Tivadar Danka
Function itself.

01:22:25: Tivadar Danka
Probably my x and y vectors are not properly shaped.

01:22:38: Tivadar Danka
And now now we see that that this dot

01:22:41: Tivadar Danka
product implementation burst just like or or or from scratch, homemade implementation. It's It's it's just much faster

01:22:54: Tivadar Danka
This is going to be very important for us Alright.

01:23:05: Tivadar Danka
I think we can implement

01:23:08: Tivadar Danka
I mean,

01:23:08: Tivadar Danka
basically, not implement, but generalize the linear regression model. The recall that

01:23:15: Tivadar Danka
or

01:23:16: Tivadar Danka
first version of our for linear regression model.

01:23:20: Tivadar Danka
Was was the simple a times x plus b expression one dimensional linear regression.

01:23:28: Tivadar Danka
Using using NumPy.

01:23:33: Tivadar Danka
Mean, basically, we can we can assume that these x and w are

01:23:38: Tivadar Danka
NumPy arrays.

01:23:42: Tivadar Danka
They can just simply use the dot tracked

01:23:44: Tivadar Danka
One more caveat I like to to mention. Let me quickly go back to the iPad.

01:23:55: Tivadar Danka
You see this expression here?

01:23:59: Tivadar Danka
Is something missing.

01:23:60: Tivadar Danka
Right?

01:24:04: Tivadar Danka
This term here.

01:24:08: Tivadar Danka
It seems to be missing.

01:24:11: Tivadar Danka
But

01:24:13: Tivadar Danka
if you if you always assume

01:24:16: Tivadar Danka
XM

01:24:17: Tivadar Danka
to be one,

01:24:19: Tivadar Danka
then this expression also contains this so called bias weight, bias, attribute.

01:24:28: Tivadar Danka
You can you can

01:24:31: Tivadar Danka
basically achieve this in practice by adding adding,

01:24:34: Tivadar Danka
a constant one as a feature.

01:24:36: Tivadar Danka
This is called bias feature.

01:24:40: Tivadar Danka
So for for simplicity,

01:24:42: Tivadar Danka
I will not include this this, bias term. Because we can we can do this by adding

01:24:48: Tivadar Danka
a column of of ones

01:24:50: Tivadar Danka
to to our our our dataset.

01:24:55: Tivadar Danka
Alright.

01:24:56: Tivadar Danka
Let's go back to matrices.

01:24:60: Tivadar Danka
Is,

01:25:01: Tivadar Danka
a tough one.

01:25:02: Tivadar Danka
Matrix multiplication.

01:25:04: Tivadar Danka
Matrix. So the matrix addition.

01:25:11: Tivadar Danka
Sorry for that. These are not the not the easiest things to understand.

01:25:20: Tivadar Danka
Let me talk about the the math first. So as I mentioned,

01:25:36: Tivadar Danka
matrices are

01:25:38: Tivadar Danka
tables of numbers.

01:25:43: Tivadar Danka
Usually, I I denote them by capital

01:25:46: Tivadar Danka
letters,

01:25:48: Tivadar Danka
For instance, a capital x,

01:25:51: Tivadar Danka
build a note

01:25:52: Tivadar Danka
a matrix,

01:25:54: Tivadar Danka
We write r in so sorry. We write x in r

01:25:59: Tivadar Danka
and times

01:25:60: Tivadar Danka
m. This means that x is a matrix of

01:26:05: Tivadar Danka
m rows

01:26:07: Tivadar Danka
and m columns.

01:26:16: Tivadar Danka
So instead of of

01:26:18: Tivadar Danka
one index, the element of elements of a matrix have two indices.

01:26:23: Tivadar Danka
The first row is x one one,

01:26:26: Tivadar Danka
x one two,

01:26:28: Tivadar Danka
x one two,

01:26:32: Tivadar Danka
and so on.

01:26:34: Tivadar Danka
X

01:26:35: Tivadar Danka
one m

01:26:39: Tivadar Danka
Actually, I don't think there's a column in the matrix notation.

01:26:44: Tivadar Danka
The second second row of of the matrix is x two one,

01:26:49: Tivadar Danka
x two two,

01:26:52: Tivadar Danka
Again, no comma here.

01:26:54: Tivadar Danka
X two

01:26:56: Tivadar Danka
m

01:26:58: Tivadar Danka
and so on, x n one, x n two,

01:27:05: Tivadar Danka
x m m

01:27:09: Tivadar Danka
This is general,

01:27:12: Tivadar Danka
mathematical notation for for the matrix.

01:27:18: Tivadar Danka
This is this is the row. This is the last row.

01:27:22: Tivadar Danka
This is a column here.

01:27:25: Tivadar Danka
So

01:27:26: Tivadar Danka
And as as I as I said, if if we are talking about data,

01:27:31: Tivadar Danka
this is a data sample or measurement.

01:27:37: Tivadar Danka
And these these the the columns store features.

01:27:52: Tivadar Danka
Alright.

01:27:57: Tivadar Danka
Let's revisit our our linear regression model.

01:28:01: Tivadar Danka
Or generalized linear regression model.

01:28:07: Tivadar Danka
Which is defined by x, f of x.

01:28:10: Tivadar Danka
Is x dot product of of the

01:28:16: Tivadar Danka
Here, x is a single

01:28:20: Tivadar Danka
data sample.

01:28:22: Tivadar Danka
Alright?

01:28:24: Tivadar Danka
Can we can we

01:28:26: Tivadar Danka
calculate the output of the model

01:28:30: Tivadar Danka
for all of the data samples. So if if we pass a matrix

01:28:34: Tivadar Danka
instead of vector,

01:28:37: Tivadar Danka
single vector,

01:28:38: Tivadar Danka
and putting the the

01:28:40: Tivadar Danka
putting the outputs manually into

01:28:43: Tivadar Danka
into a list.

01:28:45: Tivadar Danka
A vector.

01:28:45: Tivadar Danka
We can we can do that.

01:28:50: Tivadar Danka
For instance, if we have

01:28:52: Tivadar Danka
the first data point

01:28:55: Tivadar Danka
I

01:28:56: Tivadar Danka
x one one x one two, and so on.

01:29:01: Tivadar Danka
Extra

01:29:03: Tivadar Danka
one m.

01:29:09: Tivadar Danka
And we have

01:29:10: Tivadar Danka
the the the rates itself of the model.

01:29:19: Tivadar Danka
Scratch the explanation.

01:29:24: Tivadar Danka
So, again, suppose that we we store

01:29:28: Tivadar Danka
the data in the matrix, x one one,

01:29:32: Tivadar Danka
and

01:29:34: Tivadar Danka
x one two, and so on. X one m, x two one, x two two,

01:29:41: Tivadar Danka
x two m,

01:29:52: Tivadar Danka
This is the data matrix.

01:29:55: Tivadar Danka
As we have or

01:29:58: Tivadar Danka
or or body parameters, w one, w two, and so on, w m. We have as as much

01:30:05: Tivadar Danka
parameters, as much features.

01:30:11: Tivadar Danka
Which we can store

01:30:13: Tivadar Danka
in in in a vector, but this vector could also be written in a column form.

01:30:19: Tivadar Danka
V one w one, w two, and so on.

01:30:24: Tivadar Danka
W m,

01:30:25: Tivadar Danka
And the output of the model should be Basically,

01:30:36: Tivadar Danka
let us denote the the first row

01:30:42: Tivadar Danka
of this this,

01:30:44: Tivadar Danka
data metrics.

01:30:45: Tivadar Danka
X

01:30:46: Tivadar Danka
one the second row,

01:30:49: Tivadar Danka
is x two and so on. The end row is x m.

01:30:56: Tivadar Danka
Then

01:30:57: Tivadar Danka
the

01:30:58: Tivadar Danka
result of of of the of the model

01:31:01: Tivadar Danka
could be written as x one dot product w

01:31:09: Tivadar Danka
x two dot product w and so on.

01:31:12: Tivadar Danka
X m dot product w.

01:31:18: Tivadar Danka
Alright. So so here,

01:31:19: Tivadar Danka
we took a matrix, and we took a vector, and we applied some sort of operation. And what we what we have here

01:31:28: Tivadar Danka
underlined by this yellow color

01:31:32: Tivadar Danka
is the prediction

01:31:33: Tivadar Danka
of

01:31:34: Tivadar Danka
of our model

01:31:36: Tivadar Danka
for all of the data samples

01:31:38: Tivadar Danka
This is

01:31:40: Tivadar Danka
this is the the the matrix

01:31:42: Tivadar Danka
vector product.

01:31:44: Tivadar Danka
What we see here.

01:31:48: Tivadar Danka
I hope the explanation was clear enough. Let me check for questions.

01:31:56: Tivadar Danka
Question x one two and so on. X n two are marked as features.

01:31:60: Tivadar Danka
RND's value is actually same

01:32:02: Tivadar Danka
feature across all data points. I mean, the the

01:32:05: Tivadar Danka
the

01:32:06: Tivadar Danka
feature values are different, but the feature itself is consistent. Say,

01:32:11: Tivadar Danka
mean, this is this is the age

01:32:13: Tivadar Danka
a person.

01:32:14: Tivadar Danka
Or the blood sugar of a person.

01:32:17: Tivadar Danka
So the the the meaning of of of the the value will

01:32:23: Tivadar Danka
stay constant, which is, for instance, blood sugar, body mass index, whatever. But the values change

01:32:29: Tivadar Danka
across data points.

01:32:32: Tivadar Danka
I hope it answered your your your question.

01:32:38: Tivadar Danka
Alright. They can even take this idea further.

01:32:45: Tivadar Danka
So what happens if we we have

01:32:47: Tivadar Danka
not one target variable?

01:32:49: Tivadar Danka
But multiple target variables.

01:32:52: Tivadar Danka
If we have multiple target variables, then then we can also train multiple.

01:32:57: Tivadar Danka
Linear regression

01:32:59: Tivadar Danka
models.

01:33:03: Tivadar Danka
And we can compress all of these into into the

01:33:07: Tivadar Danka
matrix matrix product.

01:33:17: Tivadar Danka
Hopefully, I will have enough room on my iPad here

01:33:25: Tivadar Danka
So if x one one and so on, x

01:33:28: Tivadar Danka
one

01:33:29: Tivadar Danka
m

01:33:35: Tivadar Danka
denote our our data matrix.

01:33:39: Tivadar Danka
And this w one one w

01:33:44: Tivadar Danka
one w one k

01:33:49: Tivadar Danka
And so on w,

01:33:53: Tivadar Danka
m one w

01:33:56: Tivadar Danka
m k

01:33:57: Tivadar Danka
denote our parameter matrix where each

01:33:60: Tivadar Danka
each row

01:34:01: Tivadar Danka
Each row

01:34:09: Tivadar Danka
is is a a linear regression model.

01:34:12: Tivadar Danka
Or or a set of parameters for for a linear regression that we saw. Then this whole

01:34:21: Tivadar Danka
whole output of all of our model all of our model predictions and all of our data.

01:34:28: Tivadar Danka
Could be written

01:34:29: Tivadar Danka
as

01:34:30: Tivadar Danka
bear with me.

01:34:33: Tivadar Danka
Here, I will denote

01:34:37: Tivadar Danka
this, actually, let me write this in blue.

01:34:43: Tivadar Danka
They can

01:34:44: Tivadar Danka
they can write this as w

01:34:46: Tivadar Danka
one

01:34:48: Tivadar Danka
they can write the last the first column of w as w one.

01:34:53: Tivadar Danka
Vector w one.

01:34:55: Tivadar Danka
And so we can write the last column of w as

01:35:02: Tivadar Danka
w k, I guess. So we have k.

01:35:05: Tivadar Danka
Vectors here.

01:35:09: Tivadar Danka
So then the the the output of all of our vectors and all of our models So the all of our samples into all of our models.

01:35:16: Tivadar Danka
Will be something described by the so called matrix matrix product.

01:35:23: Tivadar Danka
Which is

01:35:26: Tivadar Danka
it will be the dot product of

01:35:29: Tivadar Danka
x one and w

01:35:33: Tivadar Danka
one

01:35:36: Tivadar Danka
and so on.

01:35:38: Tivadar Danka
So

01:35:39: Tivadar Danka
X

01:35:43: Tivadar Danka
x one and w k.

01:35:45: Tivadar Danka
And so on.

01:35:53: Tivadar Danka
X

01:35:56: Tivadar Danka
m

01:35:57: Tivadar Danka
w

01:36:01: Tivadar Danka
one

01:36:08: Tivadar Danka
This is here.

01:36:10: Tivadar Danka
Is the is the matrix matrix product.

01:36:14: Tivadar Danka
Let me give

01:36:16: Tivadar Danka
like, a more explicit definition

01:36:20: Tivadar Danka
the matrix matrix reducts. So this is this is how we

01:36:22: Tivadar Danka
multiply matrices together.

01:36:25: Tivadar Danka
What we saw here. And

01:36:27: Tivadar Danka
again,

01:36:28: Tivadar Danka
if you want to want to glimpse

01:36:30: Tivadar Danka
the reason

01:36:31: Tivadar Danka
why matrix multiplication is defined that way,

01:36:36: Tivadar Danka
is because because of this.

01:36:38: Tivadar Danka
Think of matrix matrix for act

01:36:41: Tivadar Danka
as applying

01:36:42: Tivadar Danka
several linear regression models to several

01:36:45: Tivadar Danka
data samples simultaneously.

01:36:48: Tivadar Danka
So if we have two vectors, sorry, two matrices a and b,

01:36:58: Tivadar Danka
They can usually write

01:37:01: Tivadar Danka
a

01:37:04: Tivadar Danka
matrices in this notation.

01:37:10: Tivadar Danka
This is this is short for for a for an n times m

01:37:14: Tivadar Danka
denoted by a

01:37:17: Tivadar Danka
So if this is our a,

01:37:20: Tivadar Danka
and this is our b,

01:37:21: Tivadar Danka
then

01:37:28: Tivadar Danka
So a is a is a matrix of

01:37:31: Tivadar Danka
m rows and m columns, and b is a matrix of m rows and k columns. Then

01:37:38: Tivadar Danka
I

01:37:39: Tivadar Danka
a times b,

01:37:41: Tivadar Danka
again, it's it's a matrix,

01:37:45: Tivadar Danka
And the general

01:37:46: Tivadar Danka
element

01:37:47: Tivadar Danka
for for this a times b

01:37:49: Tivadar Danka
will be defined

01:37:51: Tivadar Danka
by some

01:37:57: Tivadar Danka
a.

01:37:58: Tivadar Danka
Y

01:37:60: Tivadar Danka
I guess,

01:38:02: Tivadar Danka
l times

01:38:04: Tivadar Danka
b l

01:38:05: Tivadar Danka
and j, where l will go from one to

01:38:09: Tivadar Danka
m

01:38:11: Tivadar Danka
and I j will go from one to m and k. So this is the definition of the matrix product. It's again, it's quite complicated. It seems

01:38:22: Tivadar Danka
magical.

01:38:24: Tivadar Danka
It's a sum with a bunch of numbers thrown into it.

01:38:28: Tivadar Danka
If you if you want to want to understand why matrix multiplication was defined this way, think of of multiple linear regression models

01:38:36: Tivadar Danka
applied to

01:38:37: Tivadar Danka
multiple pieces of data.

01:38:41: Tivadar Danka
Let me show you an example of matrix multiplication. Alright? I just want to want to make sure that

01:38:45: Tivadar Danka
we see an actual example before we we move on.

01:38:52: Tivadar Danka
So let's say we have have the matrix

01:38:57: Tivadar Danka
I mean, should we use a general example or a more concrete one? Let's use a more concrete one. One, two,

01:39:05: Tivadar Danka
three, four. That will be our first matrix.

01:39:09: Tivadar Danka
Second matrix, five, six,

01:39:12: Tivadar Danka
seven, eight,

01:39:13: Tivadar Danka
I like these simple round consecutive numbers to keep things easy. So

01:39:21: Tivadar Danka
here,

01:39:22: Tivadar Danka
will be

01:39:24: Tivadar Danka
the first element of the first row.

01:39:26: Tivadar Danka
Of the matrix product.

01:39:29: Tivadar Danka
You obtain this

01:39:31: Tivadar Danka
and

01:39:35: Tivadar Danka
by taking the first row of a.

01:39:37: Tivadar Danka
I mean,

01:39:40: Tivadar Danka
suppose that this is a.

01:39:42: Tivadar Danka
Suppose that this is b.

01:39:45: Tivadar Danka
What what goes here?

01:39:47: Tivadar Danka
Is a dot product of the first row of a

01:39:51: Tivadar Danka
and the first column of b.

01:39:57: Tivadar Danka
Alright. So it's it's

01:39:59: Tivadar Danka
one times

01:40:00: Tivadar Danka
five plus two times seven.

01:40:05: Tivadar Danka
Alright.

01:40:06: Tivadar Danka
Next element.

01:40:08: Tivadar Danka
Here.

01:40:09: Tivadar Danka
The

01:40:09: Tivadar Danka
second element of the first row of the product matrix.

01:40:14: Tivadar Danka
Is obtained by the

01:40:16: Tivadar Danka
s dot product of the first row of a

01:40:21: Tivadar Danka
and the second column of b.

01:40:23: Tivadar Danka
So

01:40:27: Tivadar Danka
Again, this is just

01:40:29: Tivadar Danka
applying the definition

01:40:31: Tivadar Danka
So it's

01:40:32: Tivadar Danka
one times six plus

01:40:36: Tivadar Danka
two times eight.

01:40:40: Tivadar Danka
Let's go to the second row of the correct matrix. As before,

01:40:45: Tivadar Danka
not as before, but similarly as before.

01:40:48: Tivadar Danka
It is a dot product of the second row of a

01:40:53: Tivadar Danka
and the first column of b.

01:41:01: Tivadar Danka
So it's

01:41:01: Tivadar Danka
three times five plus four times seven.

01:41:06: Tivadar Danka
And

01:41:08: Tivadar Danka
finally,

01:41:09: Tivadar Danka
second element,

01:41:10: Tivadar Danka
of the second row of the of the product

01:41:17: Tivadar Danka
Dot product of the of the

01:41:19: Tivadar Danka
second row of a.

01:41:21: Tivadar Danka
The second column of b.

01:41:25: Tivadar Danka
Which is

01:41:27: Tivadar Danka
three times six plus

01:41:30: Tivadar Danka
four times eight. Alright. We just

01:41:33: Tivadar Danka
apply the definition here.

01:41:35: Tivadar Danka
This is this is a matrix multiplication.

01:41:39: Tivadar Danka
We are going to implement it.

01:41:41: Tivadar Danka
In a while.

01:41:42: Tivadar Danka
Let me just check for for

01:41:45: Tivadar Danka
questions.

01:41:49: Tivadar Danka
So question

01:41:50: Tivadar Danka
x one to x

01:41:52: Tivadar Danka
Yeah. Okay. I think I answered this question.

01:41:54: Tivadar Danka
Other question.

01:41:56: Tivadar Danka
Backorders are are always one dimensional collection of numbers, and matrix can be thought of as wood. Yeah. I mean, vectors are not one dimensional.

01:42:05: Tivadar Danka
It has one dimension.

01:42:09: Tivadar Danka
If you look at it as a as a tensor.

01:42:12: Tivadar Danka
But

01:42:13: Tivadar Danka
in terms of vectors, we say that if a vector has an coordinates system, it's it's an n dimensional vector.

01:42:20: Tivadar Danka
Yeah. And matrix

01:42:21: Tivadar Danka
is a bunch of vectors stacked together. Right? I mean, either you stack

01:42:26: Tivadar Danka
raw vectors

01:42:29: Tivadar Danka
vertically or you stack column vectors horizontally.

01:42:34: Tivadar Danka
Both representations are useful to keep in mind because depending on your situation,

01:42:39: Tivadar Danka
one might prove more useful than than the other.

01:42:42: Tivadar Danka
So vectorization is basically a way to represent the data. In this case, I in a more

01:42:48: Tivadar Danka
complex representation

01:42:50: Tivadar Danka
If so, when creating the vector data, does the LLM delete

01:42:54: Tivadar Danka
x data?

01:42:55: Tivadar Danka
I'm not an expert on large language models, to be honest. I try to to

01:43:01: Tivadar Danka
focus more more on the fundamental aspects of of of mathematics and machine learning.

01:43:07: Tivadar Danka
Know

01:43:07: Tivadar Danka
I

01:43:09: Tivadar Danka
basically, I know

01:43:10: Tivadar Danka
large language models on on a user level.

01:43:13: Tivadar Danka
So I wouldn't dare to answer this question.

01:43:17: Tivadar Danka
But, essentially,

01:43:20: Tivadar Danka
I think I think a large English model takes, like, a

01:43:24: Tivadar Danka
a huge vector

01:43:26: Tivadar Danka
is basically the token representation of a text, and turns it into another vector which is more useful because the the the

01:43:34: Tivadar Danka
the vector

01:43:37: Tivadar Danka
store

01:43:37: Tivadar Danka
the vector representation inside large English model

01:43:41: Tivadar Danka
is learned from data.

01:43:44: Tivadar Danka
So it can highlight the the

01:43:46: Tivadar Danka
crucial features of the dataset.

01:43:49: Tivadar Danka
But, again,

01:43:50: Tivadar Danka
not an expert in large.

01:43:56: Tivadar Danka
Alright.

01:43:57: Tivadar Danka
Adding to one dimensional vectors involves performing a bunch of additions on two arrays of numbers instead of performing

01:44:06: Tivadar Danka
separate additions, vectorization performs the additional entire vectors I mean, yeah.

01:44:13: Tivadar Danka
I mean,

01:44:14: Tivadar Danka
again, I'm not an expert on the hardware level either.

01:44:17: Tivadar Danka
But I think

01:44:18: Tivadar Danka
vectorization is baked into the hardware.

01:44:22: Tivadar Danka
So it is a very, very like, supported on a very low level.

01:44:26: Tivadar Danka
And if your code is properly vectorized, it can be much faster.

01:44:30: Tivadar Danka
Again, one reason because

01:44:32: Tivadar Danka
it is it is supported by by the hardware level.

01:44:36: Tivadar Danka
So you don't have to spend as much

01:44:39: Tivadar Danka
optimization

01:44:42: Tivadar Danka
Alright.

01:44:49: Tivadar Danka
Let's implement matrices in in Python

01:44:53: Tivadar Danka
quickly.

01:44:56: Tivadar Danka
But before that, let me take a sip of of water.

01:45:05: Tivadar Danka
Time just goes by really fast. I have to

01:45:09: Tivadar Danka
have to pick up the pace a bit.

01:45:21: Tivadar Danka
Alright. Make this as in Python.

01:45:26: Tivadar Danka
So maybe we will implement this similarly as we did for vectors. So

01:45:37: Tivadar Danka
for for instance, like, what what I want to write let me just

01:45:43: Tivadar Danka
copy and paste the code here.

01:45:46: Tivadar Danka
I want to write something such as this. So I want to pass a list of lists

01:45:53: Tivadar Danka
and each

01:45:54: Tivadar Danka
list will represent a row.

01:45:58: Tivadar Danka
So the the in it method of matrix will take

01:46:06: Tivadar Danka
basically a list which I will

01:46:16: Tivadar Danka
So this what what you see here, this list of lists, is going to be passed in the rows

01:46:25: Tivadar Danka
argument.

01:46:30: Tivadar Danka
I also want to to

01:46:33: Tivadar Danka
store the shape of the matrix itself for for

01:46:37: Tivadar Danka
for

01:46:38: Tivadar Danka
other purposes. It will make some some methods easier to account.

01:46:43: Tivadar Danka
So the shape is the top row composed of the of the number of of rows.

01:46:53: Tivadar Danka
And the number of columns

01:46:54: Tivadar Danka
and

01:46:55: Tivadar Danka
again, they are assumed to be to be

01:46:58: Tivadar Danka
to be nice. We don't want to to intentionally

01:47:03: Tivadar Danka
break our code.

01:47:05: Tivadar Danka
We will only pass rules

01:47:06: Tivadar Danka
that have the same amount of of

01:47:08: Tivadar Danka
of, of coordinates.

01:47:11: Tivadar Danka
So the

01:47:13: Tivadar Danka
second attribute will be the length

01:47:16: Tivadar Danka
of the of the first row itself.

01:47:21: Tivadar Danka
So this this works as well.

01:47:25: Tivadar Danka
I also want to add a nice string representation whose details are completely unimportant for us. So I will just copy and paste it from my other

01:47:33: Tivadar Danka
Versus code instance. And I also admit that I I wrote this code with the help of a larger than Visual L.

01:47:43: Tivadar Danka
But it will work. So x is the matrix.

01:47:47: Tivadar Danka
And if we print it out,

01:47:49: Tivadar Danka
we can see the the structure nicely.

01:47:54: Tivadar Danka
Again,

01:47:56: Tivadar Danka
couple of things are missing.

01:47:59: Tivadar Danka
Indexing, most importantly. So we will implement a getitem and setitem. Again, using the subclassing trick.

01:48:13: Tivadar Danka
Because I I don't want to copy and paste

01:48:15: Tivadar Danka
all of this code every time I want to add a new method.

01:48:32: Tivadar Danka
These methods are already familiar for us to get item, a set item, So the the argument IDX

01:48:42: Tivadar Danka
will come

01:48:43: Tivadar Danka
in form of a tuple. So x zero one And then whenever you write this,

01:48:49: Tivadar Danka
what's here?

01:48:52: Tivadar Danka
Is going to be inserted into, like, a a topple.

01:48:57: Tivadar Danka
And this topple will be passed To that,

01:49:05: Tivadar Danka
getitem method here.

01:49:09: Tivadar Danka
For simplicity, I go to stack these vectors I think I I might have to

01:49:25: Tivadar Danka
to access the elements of this ID. Hopefully, this will work. And we will just simply return

01:49:32: Tivadar Danka
the

01:49:35: Tivadar Danka
jth element of the

01:49:38: Tivadar Danka
Alright. And and

01:49:41: Tivadar Danka
set item will work similarly but instead of of returning the value, we will set the value.

01:49:57: Tivadar Danka
Three instantiate x. Because we modified the class.

01:50:05: Tivadar Danka
Indexing works.

01:50:06: Tivadar Danka
That's

01:50:09: Tivadar Danka
that's

01:50:10: Tivadar Danka
let's change the value to test that item.

01:50:13: Tivadar Danka
And it works as well.

01:50:14: Tivadar Danka
So

01:50:16: Tivadar Danka
great.

01:50:19: Tivadar Danka
Addition and scalar multiplication also works for me. Just says I mean, let's let's implement them.

01:50:26: Tivadar Danka
For for sake of completeness and practice I mean, addition is implemented by the add,

01:50:39: Tivadar Danka
magic method. Again,

01:50:41: Tivadar Danka
first argument is always self.

01:50:43: Tivadar Danka
Second argument is rather representing

01:50:46: Tivadar Danka
the other matrix instance

01:50:48: Tivadar Danka
They want to add to self.

01:50:51: Tivadar Danka
And

01:50:52: Tivadar Danka
it's a very simple list comprehension at this point.

01:51:01: Tivadar Danka
It it takes basically, I think it's double double is comprehension. Because

01:51:09: Tivadar Danka
first,

01:51:12: Tivadar Danka
the iterate

01:51:13: Tivadar Danka
through the the

01:51:14: Tivadar Danka
the rows of of the of the

01:51:17: Tivadar Danka
of both matrices. So let's just let's just write placeholder here.

01:51:24: Tivadar Danka
This will

01:51:25: Tivadar Danka
run through

01:51:31: Tivadar Danka
in the zip of of, cell that rose.

01:51:37: Tivadar Danka
And the other that rose.

01:51:41: Tivadar Danka
So we we basically in the first step is we take the first row of both matrices, create the the the coordinates by additions,

01:51:49: Tivadar Danka
We move on to the second row of both matrices and so on. Here, again, it's just like a simple

01:51:57: Tivadar Danka
vector

01:51:59: Tivadar Danka
addition. It's x plus y four x y in, again, in ZIP.

01:52:08: Tivadar Danka
X is y is

01:52:12: Tivadar Danka
Multiplication

01:52:13: Tivadar Danka
almost the same. I can just copy and paste the entire code.

01:52:19: Tivadar Danka
Rename the method,

01:52:21: Tivadar Danka
and

01:52:22: Tivadar Danka
I mean, here I mean, by by more,

01:52:25: Tivadar Danka
the mean

01:52:28: Tivadar Danka
scale or multiplication. So I guess it's it's not the same after all. Because other other is is is a

01:52:35: Tivadar Danka
is a scalar value.

01:52:38: Tivadar Danka
Right? Like, two or three or 42. So

01:52:45: Tivadar Danka
there will be no no zips here.

01:52:50: Tivadar Danka
So we'll just run through the the

01:52:53: Tivadar Danka
axis I mean, rows of of self dot rows.

01:53:11: Tivadar Danka
Let me check it one more time. I guess this will work. And

01:53:15: Tivadar Danka
we know that we have to implement, right, multiplication as well. Which is going to be

01:53:25: Tivadar Danka
simply

01:53:27: Tivadar Danka
self

01:53:29: Tivadar Danka
star

01:53:29: Tivadar Danka
other.

01:53:33: Tivadar Danka
Because this here

01:53:35: Tivadar Danka
what you see here in the highlighted part, we'll call this method.

01:53:41: Tivadar Danka
So it will solve our problem.

01:53:45: Tivadar Danka
Okay. Let's instantiate two matrices. The first one will be one, two, three,

01:53:55: Tivadar Danka
four, five, six. And the second one will

01:53:60: Tivadar Danka
take similar form.

01:54:05: Tivadar Danka
Seven eight nine ten, 11,

01:54:11: Tivadar Danka
valve.

01:54:13: Tivadar Danka
X plus y.

01:54:14: Tivadar Danka
Vercs

01:54:17: Tivadar Danka
properly to star x, which means scalar multiplication by two.

01:54:23: Tivadar Danka
Works perfectly.

01:54:27: Tivadar Danka
And what's left?

01:54:30: Tivadar Danka
Is the matrix multiplication itself.

01:54:33: Tivadar Danka
Which is

01:54:35: Tivadar Danka
not that easy.

01:54:39: Tivadar Danka
It it requires

01:54:40: Tivadar Danka
a bit of thinking. So, like, first, let me check on the on the

01:54:47: Tivadar Danka
questions in the q and a.

01:54:53: Tivadar Danka
Since machine learning is based on notes, that makes sense since you are constantly running the learning notes against the data for each pass.

01:55:01: Tivadar Danka
I mean, what learning notes you refer to here?

01:55:06: Tivadar Danka
Keep in mind,

01:55:08: Tivadar Danka
I know almost nothing about machine learning infrastructure.

01:55:13: Tivadar Danka
Never never did any work in in in these

01:55:16: Tivadar Danka
in these areas. Basically, I did machine learning models for for research purposes.

01:55:23: Tivadar Danka
And whenever I I I trained the model, I did not focus on deploying it at all. Consider me like a, more of a machine learning hack than a machine learning engineer. On the other hand, I understand the the the theoretical and mathematical background

01:55:39: Tivadar Danka
quite well.

01:55:45: Tivadar Danka
So I I am unable to transfer this,

01:55:49: Tivadar Danka
question.

01:55:51: Tivadar Danka
But we have new questions.

01:55:53: Tivadar Danka
Always the vector I mean, I'm I'm not sure what you what you refer to, but I think

01:55:59: Tivadar Danka
might refer to computational graphs. Right? So

01:56:03: Tivadar Danka
computational graphs can be vectorized as well.

01:56:06: Tivadar Danka
Yeah.

01:56:09: Tivadar Danka
Alright. So let's just take a deep breath and deep plant the the vector multiplication.

01:56:16: Tivadar Danka
Sorry. The matrix multiplication.

01:56:19: Tivadar Danka
In Python, matrix multiplication is

01:56:23: Tivadar Danka
implemented by this at operator

01:56:26: Tivadar Danka
or or if you if you use NumPy.

01:56:30: Tivadar Danka
The the at operator is used for for matrix multiplication.

01:56:35: Tivadar Danka
This is implemented by the the the magic method. So we have a matrix here.

01:56:47: Tivadar Danka
A new new matrix class, which is a subclass of our old class.

01:56:53: Tivadar Danka
I will implement two helper methods.

01:56:56: Tivadar Danka
To make a job.

01:56:57: Tivadar Danka
Bit easier.

01:56:60: Tivadar Danka
It will be called row which will return the. It will just access the the list of rows

01:57:11: Tivadar Danka
and take the I for instance. I will also also in the column, which will return the column. Here, we have to be

01:57:24: Tivadar Danka
bit more tricky since we have a list of rows. We have to use, like, a list comprehension to access the the jth member of every row.

01:57:34: Tivadar Danka
We iterate through rows and and and access the element. I do just demonstrate these these methods for you.

01:57:50: Tivadar Danka
I will con continue the implementation. Here. Later.

01:58:01: Tivadar Danka
I want to demonstrate the the row and call methods.

01:58:04: Tivadar Danka
So row one returns

01:58:07: Tivadar Danka
I mean, it it returns the second row because we index from from one.

01:58:13: Tivadar Danka
Call zero returns the first column

01:58:16: Tivadar Danka
So one, four, seven, and 10. And so on.

01:58:21: Tivadar Danka
Second column. So it's it's

01:58:25: Tivadar Danka
it's an easy easy easy method to to understand. But it will be useful for us

01:58:31: Tivadar Danka
through implementing

01:58:33: Tivadar Danka
the matrix multiplication.

01:58:37: Tivadar Danka
So I will I will sketch out the the method

01:58:41: Tivadar Danka
first. So

01:58:43: Tivadar Danka
I

01:58:45: Tivadar Danka
I will I will assemble

01:58:47: Tivadar Danka
the matrix product

01:58:49: Tivadar Danka
quite manually.

01:58:52: Tivadar Danka
It will it will be in the form list of lists.

01:58:56: Tivadar Danka
What is needed to to instantiate our matrix.

01:58:59: Tivadar Danka
Class. I will call the

01:59:01: Tivadar Danka
rows of the product prod underscore rows.

01:59:06: Tivadar Danka
This will be a list.

01:59:09: Tivadar Danka
We will complete this implementation later.

01:59:13: Tivadar Danka
And what will return

01:59:14: Tivadar Danka
is a matrix instance

01:59:17: Tivadar Danka
defined by this list of lists. Alright?

01:59:25: Tivadar Danka
So

01:59:27: Tivadar Danka
let's suppose this is an end by end matrix.

01:59:31: Tivadar Danka
I will store these

01:59:33: Tivadar Danka
constants

01:59:36: Tivadar Danka
into variables.

01:59:40: Tivadar Danka
Because I don't want to access the the

01:59:42: Tivadar Danka
first and second element of self shape all the time.

01:59:45: Tivadar Danka
So

01:59:47: Tivadar Danka
product pros.

01:59:50: Tivadar Danka
Here goes

01:59:52: Tivadar Danka
the first row

01:59:53: Tivadar Danka
of the product.

01:59:55: Tivadar Danka
Itself. The wheel the wheel feel

01:59:58: Tivadar Danka
this out later.

02:00:01: Tivadar Danka
But what we know,

02:00:04: Tivadar Danka
is that self dot shape If self dot shape is m and m,

02:00:12: Tivadar Danka
then the other

02:00:14: Tivadar Danka
dot shape must be m and and k.

02:00:18: Tivadar Danka
For some k. We we don't know that, but

02:00:21: Tivadar Danka
the the the product shape will be

02:00:27: Tivadar Danka
n and and and k. Okay. So it will have n rows. And k columns because this is how matrix multiplication is is defined.

02:00:40: Tivadar Danka
Alright.

02:01:01: Tivadar Danka
Let me think on it a bit.

02:01:05: Tivadar Danka
Whenever I I I implement

02:01:07: Tivadar Danka
linear algebra related stuff, I have to think

02:01:10: Tivadar Danka
really hard, not to mess up the dimensions.

02:01:13: Tivadar Danka
Indices,

02:01:14: Tivadar Danka
whatever. So

02:01:17: Tivadar Danka
I will have to figure out figure it out. What what this this,

02:01:21: Tivadar Danka
I mean, the first row of of the product will be.

02:01:25: Tivadar Danka
Mean, it will be

02:01:27: Tivadar Danka
composed of of

02:01:29: Tivadar Danka
sums. Alright?

02:01:35: Tivadar Danka
So it will have

02:01:37: Tivadar Danka
I think,

02:01:38: Tivadar Danka
m rules. So

02:01:54: Tivadar Danka
Yes. I think it's gonna have m

02:01:57: Tivadar Danka
rows.

02:01:58: Tivadar Danka
For sure. And

02:02:01: Tivadar Danka
what's here?

02:02:03: Tivadar Danka
It will be like a like a mathematic collection will will go here.

02:02:08: Tivadar Danka
But

02:02:09: Tivadar Danka
this will have

02:02:14: Tivadar Danka
the k cons.

02:02:16: Tivadar Danka
Let me think on it for a second.

02:02:37: Tivadar Danka
We'll be we'll write a sum here.

02:02:40: Tivadar Danka
Alright?

02:02:43: Tivadar Danka
And the sum will will will

02:02:45: Tivadar Danka
take the form x

02:02:48: Tivadar Danka
y k times sorry, x y k times y.

02:02:53: Tivadar Danka
K j.

02:02:56: Tivadar Danka
Alright. And and

02:02:60: Tivadar Danka
these are actually

02:03:02: Tivadar Danka
taken from from

02:03:08: Tivadar Danka
like, this this will will are I mean, k is our running index.

02:03:12: Tivadar Danka
So

02:03:14: Tivadar Danka
I have to figure out

02:03:16: Tivadar Danka
the correct form of this list comprehension. So so

02:03:19: Tivadar Danka
I think x y k y k j bill come from the ZIP of of something.

02:03:32: Tivadar Danka
But but what? I mean, x is the

02:03:37: Tivadar Danka
x I mean, this is x

02:03:40: Tivadar Danka
y k.

02:03:41: Tivadar Danka
Will be the from the of of

02:03:45: Tivadar Danka
self,

02:03:48: Tivadar Danka
and these will be

02:03:52: Tivadar Danka
the jade column of other.

02:03:54: Tivadar Danka
Yes. I think that's correct.

02:04:03: Tivadar Danka
I hear

02:04:08: Tivadar Danka
will go from from one to

02:04:13: Tivadar Danka
n because the first matrix have has n

02:04:17: Tivadar Danka
rows, and j

02:04:19: Tivadar Danka
will go from

02:04:21: Tivadar Danka
So the the other with matrix's shape is is, I think, will be m and k, but I don't want to overwrite why we'll write them on the score here.

02:04:56: Tivadar Danka
South

02:04:57: Tivadar Danka
Sorry. Not south. Oh, they're in bad shape.

02:05:01: Tivadar Danka
Alright.

02:05:02: Tivadar Danka
Yes.

02:05:09: Tivadar Danka
Let's try this.

02:05:12: Tivadar Danka
Let's try it with a simple example.

02:05:17: Tivadar Danka
1234 or or maybe it should be 1001 and y should be

02:05:28: Tivadar Danka
123, and 4.

02:05:33: Tivadar Danka
Alright. Here goes nothing.

02:05:39: Tivadar Danka
It doesn't work properly.

02:05:46: Tivadar Danka
It doesn't fit properly here either.

02:05:49: Tivadar Danka
So XAtY should be

02:05:54: Tivadar Danka
Y.

02:05:58: Tivadar Danka
So what I have in my notes on my other screen

02:06:02: Tivadar Danka
the only difference is that here,

02:06:05: Tivadar Danka
these y

02:06:06: Tivadar Danka
both go from from

02:06:08: Tivadar Danka
y and j both go from from zero to m.

02:06:17: Tivadar Danka
Doesn't work either.

02:06:19: Tivadar Danka
So let's let's spot

02:06:21: Tivadar Danka
the error here.

02:06:41: Tivadar Danka
As you can see, linear algebra is quite hard, especially if you want to implement this from from scratch.

02:06:49: Tivadar Danka
Let me try to spot the error.

02:06:51: Tivadar Danka
In in my in my implementation.

02:07:28: Tivadar Danka
The one one issue here is that here I wrote self.

02:07:33: Tivadar Danka
Instead of other.

02:07:35: Tivadar Danka
And we have

02:07:37: Tivadar Danka
k rows

02:07:39: Tivadar Danka
for sort of k columns for the other.

02:07:43: Tivadar Danka
And

02:07:44: Tivadar Danka
I rows

02:07:45: Tivadar Danka
for self.

02:07:48: Tivadar Danka
So the other rolls for south.

02:07:49: Tivadar Danka
Yeah. It works. I fixed the error. I broke broke a sweat here.

02:07:57: Tivadar Danka
And this is the magic of of a live calling. I think this implement is correct. Let's test it out in a couple of of examples.

02:08:12: Tivadar Danka
Matrix multiplication is usually not commutating, so you can just

02:08:15: Tivadar Danka
switch the orders around.

02:08:17: Tivadar Danka
But it will it will work

02:08:20: Tivadar Danka
I mean,

02:08:21: Tivadar Danka
syntacticality, sometimes it will it will work, but the results won't be the same.

02:08:32: Tivadar Danka
Let me show you

02:08:33: Tivadar Danka
a fun little example of matrix multiplication. So let f be the matrix. Defined by the following.

02:08:43: Tivadar Danka
List of list one one

02:08:47: Tivadar Danka
one and and zero, I think. Alright. F at f

02:08:56: Tivadar Danka
at f. Let's let's just keep keep trying. Keep multiplying itself. I mean, we are calculating the powers of f here.

02:09:03: Tivadar Danka
Mathematically speaking, f to the power of m. Do you see my starting form here? And you'll just keep keep on multiplying f with itself.

02:09:23: Tivadar Danka
The name f

02:09:24: Tivadar Danka
should be a hint for you. Which stands for Fibonacci.

02:09:30: Tivadar Danka
Because these are these are the Fibonacci numbers here.

02:09:34: Tivadar Danka
Actually, as it turns out, we can compute the Fibonacci numbers by matrix and multiplication. And

02:09:41: Tivadar Danka
it's the fastest way to compute them. If you if you

02:09:45: Tivadar Danka
compute numbers by recursion,

02:09:49: Tivadar Danka
you are in for a for a bad time because

02:09:51: Tivadar Danka
there are bunch of recursive function calls. Probably, I mean,

02:09:55: Tivadar Danka
computing the the

02:09:57: Tivadar Danka
20 number will take

02:09:60: Tivadar Danka
lots of time, but you you can't even complete the the can't even compute the thousand Fibonacci number. But with matrix multiplication, you can do just that.

02:10:11: Tivadar Danka
Because it is it is fast as hell. Right? These are the Fibonacci numbers.

02:10:17: Tivadar Danka
Alright.

02:10:21: Tivadar Danka
Yeah. I mean, we have a a question in the q and a. Pointing out that I'm not using the other in the sum.

02:10:27: Tivadar Danka
Again, that was the issue. So thanks for for for pointing it out. It took me quite a while to to realize, but we we got there eventually.

02:10:38: Tivadar Danka
In the chat Did everyone lose audio suddenly?

02:10:42: Tivadar Danka
Okay. We have a confirmation. That someone still has sound. If you have

02:10:48: Tivadar Danka
sound issues, just let me know. My microphone is not muted, so

02:10:55: Tivadar Danka
it's probably the the the

02:10:58: Tivadar Danka
issue is is not in in my setup, hopefully.

02:11:03: Tivadar Danka
Alright. I think it's time time to take another break. I have to wipe off the blood, sweat, and tears.

02:11:09: Tivadar Danka
That that I I got from implementing the matrix multiplication for you, but

02:11:13: Tivadar Danka
they are there finally.

02:11:15: Tivadar Danka
They will they will continue.

02:11:17: Tivadar Danka
Or or workshop with matrices in

02:11:21: Tivadar Danka
NumPy. So

02:11:22: Tivadar Danka
see you in the next

02:21:36: Tivadar Danka
Hey. We are back after the shorts. Time is break.

02:21:41: Tivadar Danka
I did not forgot to unmute my microphone this time.

02:21:45: Tivadar Danka
So you should be hearing me. Right?

02:21:50: Tivadar Danka
Let's continue with matrices.

02:21:53: Tivadar Danka
To be more precise, matrices in NumPy Because as I mentioned,

02:21:58: Tivadar Danka
even though we implemented our custom matrices and vectors from scratch,

02:22:02: Tivadar Danka
these are not good to use in practice. But what I want to show you

02:22:08: Tivadar Danka
is is how matrices are done by by NumPy.

02:22:13: Tivadar Danka
So you you saw this this,

02:22:15: Tivadar Danka
piece of code I I written here.

02:22:25: Tivadar Danka
Instantiating a NumPy

02:22:27: Tivadar Danka
matrix

02:22:29: Tivadar Danka
is going to be done.

02:22:31: Tivadar Danka
In the very same way

02:22:33: Tivadar Danka
but instead of of of

02:22:34: Tivadar Danka
a matrix object, we call the MP dot array.

02:22:39: Tivadar Danka
Function. So here, f is is the same.

02:22:45: Tivadar Danka
Matrix that's here.

02:22:49: Tivadar Danka
But it's in it's in NumPy.

02:22:53: Tivadar Danka
Alright. I will just copy and paste.

02:22:56: Tivadar Danka
An example for you to to show that matrix addition matrix multiplication, the works as as intended.

02:23:08: Tivadar Danka
Even with our example.

02:23:11: Tivadar Danka
And

02:23:12: Tivadar Danka
the most most important things for for us to understand in in NumPy that

02:23:18: Tivadar Danka
matrix

02:23:19: Tivadar Danka
can be reshaped

02:23:21: Tivadar Danka
to to other,

02:23:22: Tivadar Danka
dimensions as well. So for that, let's take a look at the more more general not a square matrix. So let x be the MP array

02:23:36: Tivadar Danka
constructed by

02:23:38: Tivadar Danka
the list one two.

02:23:42: Tivadar Danka
Three, and four, five, six. Sorry.

02:23:46: Tivadar Danka
So it's a it's a two by three matrix. And every every has this method called reshape. Instead of trying to to

02:23:57: Tivadar Danka
describe what happens

02:23:60: Tivadar Danka
with words, I'm going to show you.

02:24:03: Tivadar Danka
So

02:24:05: Tivadar Danka
so here,

02:24:06: Tivadar Danka
we have a a two by three matrix, which we can reshape into a six by one matrix.

02:24:13: Tivadar Danka
Which is

02:24:15: Tivadar Danka
essentially

02:24:17: Tivadar Danka
six rows, one column.

02:24:19: Tivadar Danka
And this this reshape

02:24:22: Tivadar Danka
function works

02:24:25: Tivadar Danka
quite well. You can reshape the x into several different forms.

02:24:31: Tivadar Danka
You can't

02:24:32: Tivadar Danka
reshape into all possible forms. I mean, the product of these two numbers

02:24:37: Tivadar Danka
must be six because we have

02:24:41: Tivadar Danka
six elements in in total. So that covers a six by one matrix, a two by three matrix, a three by two matrix,

02:24:51: Tivadar Danka
a

02:24:53: Tivadar Danka
the one by six matrix.

02:24:57: Tivadar Danka
We can do this because

02:24:60: Tivadar Danka
even though

02:25:01: Tivadar Danka
this seems like a multidimensional array,

02:25:05: Tivadar Danka
NumPy still stores it

02:25:07: Tivadar Danka
as a linear array.

02:25:09: Tivadar Danka
Consecutive sequence of numbers.

02:25:11: Tivadar Danka
And the only thing that's changed is the so called view of the matrix, which is basically controlled by internally.

02:25:20: Tivadar Danka
Again, this is this, reshaping function is is a

02:25:24: Tivadar Danka
very useful

02:25:26: Tivadar Danka
One more thing I want to mention to you about about.

02:25:30: Tivadar Danka
Not exactly related to mathematics of machine learning, but very important. If you are ever doing

02:25:37: Tivadar Danka
you know, algebra in practice.

02:25:39: Tivadar Danka
Which is broadcasting. So

02:25:42: Tivadar Danka
let us define

02:25:44: Tivadar Danka
a simple

02:25:46: Tivadar Danka
vector

02:25:49: Tivadar Danka
for instance, a vector of one and two, this will be

02:25:53: Tivadar Danka
let us define a matrix denoted by y. Defined by, I don't know, one, two, three, four,

02:26:05: Tivadar Danka
five six. Alright. And and in principle,

02:26:14: Tivadar Danka
x plus y shouldn't work.

02:26:17: Tivadar Danka
But it does in NumPy. You can

02:26:21: Tivadar Danka
you can

02:26:21: Tivadar Danka
see that what happens here is it takes this vector

02:26:25: Tivadar Danka
and adds it

02:26:27: Tivadar Danka
into each row of y.

02:26:29: Tivadar Danka
Because it's it's compatible. So so what happens here

02:26:33: Tivadar Danka
behind the scenes is that

02:26:36: Tivadar Danka
x is converted

02:26:40: Tivadar Danka
into into matrix that is obtained by stacking x

02:26:45: Tivadar Danka
three times

02:26:45: Tivadar Danka
vertically. So just to be more precise, axis is

02:26:54: Tivadar Danka
so called broadcasted into into this

02:26:57: Tivadar Danka
x matrix here. And now it defines element wise

02:27:02: Tivadar Danka
matrix addition. The library the the two years ago when I implemented my own, neural network framework,

02:27:10: Tivadar Danka
this this, broadcasting and reshaping caused me quite a bit of headache.

02:27:16: Tivadar Danka
Which you will see in my next book, the the

02:27:20: Tivadar Danka
sequel to mathematics of machine learning. Hopefully,

02:27:23: Tivadar Danka
be released

02:27:26: Tivadar Danka
soon. I don't know to share any details about it, This will be about the neural network framework I I brought

02:27:35: Tivadar Danka
So

02:27:37: Tivadar Danka
again, we have

02:27:40: Tivadar Danka
make this in compiled. It's quite easy to use. Actually, we we we we we built the vector and matrix custom classes

02:27:51: Tivadar Danka
to to have the same interface as as NumPy.

02:27:54: Tivadar Danka
So I don't need

02:27:57: Tivadar Danka
any additional explanation.

02:27:59: Tivadar Danka
Numbers work very similarly. So lastly, turn to to linear regression.

02:28:05: Tivadar Danka
One more time. Hopefully, by the end of this version, we will we will be able to train, like, a general

02:28:13: Tivadar Danka
linear regression model by gradient descent. What will be the model? So first, we'll have

02:28:20: Tivadar Danka
a vector of of weights. I don't know. I will just put some some random numbers here. For instance, we have a a

02:28:28: Tivadar Danka
four dimensional weight vector here.

02:28:33: Tivadar Danka
One minus 1.2, 4.5, and 2.5.

02:28:37: Tivadar Danka
And let's generate like, a a data matrix

02:28:42: Tivadar Danka
I view the copy and paste.

02:28:45: Tivadar Danka
From my other Versus code instance. Let's suppose the data comes this way. Like I say here, this is the this is the biased

02:28:55: Tivadar Danka
feature I talked to you about. So this is a

02:28:59: Tivadar Danka
five times four matrix, meaning five data samples and four features, and we also have four weights for model parameters.

02:29:09: Tivadar Danka
So

02:29:10: Tivadar Danka
the definition of our model is is is,

02:29:13: Tivadar Danka
quite easy.

02:29:19: Tivadar Danka
So x at

02:29:23: Tivadar Danka
w

02:29:24: Tivadar Danka
but there's a caveat. We have to reshape w because if I just write x at

02:29:33: Tivadar Danka
w, it won't

02:29:36: Tivadar Danka
strange

02:29:38: Tivadar Danka
I did not expect this to to work.

02:29:41: Tivadar Danka
But it it works.

02:29:43: Tivadar Danka
Probably probably it's because none of my other sense what I want to do. However,

02:29:48: Tivadar Danka
I can explicitly reshape Actually, let's let's check if the if the results are same. So I can I can reshape

02:30:08: Tivadar Danka
w into a column vector?

02:30:10: Tivadar Danka
So the result will have different shape but the same numbers. Right?

02:30:16: Tivadar Danka
Mathematically speaking, this expression shouldn't work. But, again, as I said, NumPy always surprises me.

02:30:25: Tivadar Danka
And this is this is one one instance. But I want to be automatically precise, so I'll be, like, explicitly

02:30:31: Tivadar Danka
reshape

02:30:32: Tivadar Danka
w in in my in my model. I mean, you you you can you can write a negative term, like,

02:30:39: Tivadar Danka
minus one in the reshape function, which means that

02:30:43: Tivadar Danka
this this, value will be determined automatically.

02:30:47: Tivadar Danka
Based on what's here.

02:30:50: Tivadar Danka
Because that can be done. So this is our our linear regression model.

02:30:57: Tivadar Danka
Or general linear regression model. And, hopefully, we will learn how to to train it. In practice. But right now,

02:31:04: Tivadar Danka
we will move on to to calculus.

02:31:06: Tivadar Danka
Single variable calculus, most importantly.

02:31:09: Tivadar Danka
I will do a bunch of mathematical explanation before we will see our very first line of code. Let's check the the

02:31:24: Tivadar Danka
questions.

02:31:35: Tivadar Danka
This session will be

02:31:37: Tivadar Danka
will be recorded.

02:31:40: Tivadar Danka
Yes. This will be recorded. Alright.

02:31:46: Tivadar Danka
I'm happy to get a refresher and learn some new things, about Python coding. I thought there would be more emphasis on the mathematical framework of machine learning too. We are just getting there. I was a bit slow. Because I wanted to implement vectors and matrixes from scratch, but now comes the matrix.

02:32:01: Tivadar Danka
I I promise. Now comes calculus to be to be more precise. So so

02:32:06: Tivadar Danka
stick with me for for for an hour and a half. And I will I will

02:32:11: Tivadar Danka
I will

02:32:12: Tivadar Danka
talk bit more math here.

02:32:17: Tivadar Danka
I'm going to switch to my iPad here.

02:32:20: Tivadar Danka
So

02:32:25: Tivadar Danka
Calculus. Alright. So the the

02:32:28: Tivadar Danka
the

02:32:29: Tivadar Danka
basic idea of machine learning is to have

02:32:33: Tivadar Danka
parametric functions

02:32:35: Tivadar Danka
for instance, such as

02:32:37: Tivadar Danka
f of x, equal to a times x, plus

02:32:41: Tivadar Danka
b.

02:32:42: Tivadar Danka
There

02:32:43: Tivadar Danka
a

02:32:47: Tivadar Danka
a and b are two parameters. And the the

02:32:51: Tivadar Danka
the

02:32:52: Tivadar Danka
fit I mean, the the the find a and b

02:32:56: Tivadar Danka
so that the model I set will fit best

02:32:59: Tivadar Danka
to our model.

02:33:01: Tivadar Danka
This is done by by

02:33:02: Tivadar Danka
Kakros. Let's forget about

02:33:08: Tivadar Danka
models right now.

02:33:10: Tivadar Danka
I want to talk to

02:33:13: Tivadar Danka
first

02:33:14: Tivadar Danka
what I want to talk about first is the derivative.

02:33:21: Tivadar Danka
If you have any any

02:33:23: Tivadar Danka
I don't know,

02:33:25: Tivadar Danka
previous previous

02:33:27: Tivadar Danka
if you had any previous encounters with calculus,

02:33:30: Tivadar Danka
know that if if f

02:33:32: Tivadar Danka
is a is a is a

02:33:34: Tivadar Danka
single variable function,

02:33:36: Tivadar Danka
written as f

02:33:38: Tivadar Danka
column r to r. This means that this is like a function that maps a real number to a number

02:33:47: Tivadar Danka
I mean, you can

02:33:49: Tivadar Danka
visualize this

02:33:53: Tivadar Danka
as a graph. This is the form x.

02:33:57: Tivadar Danka
We have the concept of the derivative. I will

02:34:01: Tivadar Danka
I

02:34:02: Tivadar Danka
show you the definition and then explain

02:34:05: Tivadar Danka
it is.

02:34:08: Tivadar Danka
F prime

02:34:09: Tivadar Danka
f prime,

02:34:11: Tivadar Danka
Let me see the exact variables I use. F prime at x.

02:34:17: Tivadar Danka
The limit

02:34:19: Tivadar Danka
of

02:34:21: Tivadar Danka
the so called difference quotient s y goes to egg. F of x minus f of

02:34:29: Tivadar Danka
y over x minus y.

02:34:33: Tivadar Danka
Alright. This is

02:34:34: Tivadar Danka
a limit.

02:34:37: Tivadar Danka
It's

02:34:38: Tivadar Danka
not necessarily

02:34:40: Tivadar Danka
important to understand the the precise mathematical value of or the mathematical definition of the limit.

02:34:47: Tivadar Danka
What it means is that we have have an expression

02:34:50: Tivadar Danka
right here

02:34:52: Tivadar Danka
This f x minus f y over x minus y.

02:34:57: Tivadar Danka
And we substitute values

02:34:59: Tivadar Danka
for for for y,

02:35:03: Tivadar Danka
And, essentially, the the the substitute values that are getting closer and closer to to x.

02:35:09: Tivadar Danka
And if if

02:35:10: Tivadar Danka
if

02:35:11: Tivadar Danka
x sorry. If y is really, really close to x, then the value of this expression will be really, really close to to a to a single number that is called the limit.

02:35:21: Tivadar Danka
The limit

02:35:23: Tivadar Danka
does not always exist.

02:35:25: Tivadar Danka
But when it does,

02:35:27: Tivadar Danka
it's it's called the derivative of

02:35:30: Tivadar Danka
of at at at

02:35:32: Tivadar Danka
Let me

02:35:33: Tivadar Danka
visualize

02:35:36: Tivadar Danka
this here.

02:35:37: Tivadar Danka
This is called the the

02:35:39: Tivadar Danka
difference quotient.

02:35:48: Tivadar Danka
So this is

02:35:50: Tivadar Danka
f of x. And let's say

02:35:54: Tivadar Danka
we have we have an x zero here. I think I will erase the the y axis

02:36:01: Tivadar Danka
just

02:36:04: Tivadar Danka
to reduce visual clutter here.

02:36:09: Tivadar Danka
Let's say

02:36:11: Tivadar Danka
why is is here, but we choose an arbitrary why. It's a real number. And the difference quotient here

02:36:19: Tivadar Danka
is going to be the slope slope of this line.

02:36:24: Tivadar Danka
So

02:36:25: Tivadar Danka
Alright. So

02:36:28: Tivadar Danka
I can

02:36:31: Tivadar Danka
probably

02:36:36: Tivadar Danka
projected to to both axes.

02:36:45: Tivadar Danka
So this here

02:36:47: Tivadar Danka
is

02:36:49: Tivadar Danka
x zero Sorry.

02:36:53: Tivadar Danka
The the the this distance

02:36:55: Tivadar Danka
is

02:36:56: Tivadar Danka
y minus x zero.

02:36:59: Tivadar Danka
And I can also project it to the y axis.

02:37:07: Tivadar Danka
Here,

02:37:09: Tivadar Danka
This distance

02:37:11: Tivadar Danka
is

02:37:17: Tivadar Danka
f of x

02:37:19: Tivadar Danka
Sorry.

02:37:22: Tivadar Danka
Yeah. I mean,

02:37:23: Tivadar Danka
I guess it depends on what coordinate you take first, but it's it's

02:37:28: Tivadar Danka
yeah, it's f

02:37:30: Tivadar Danka
y

02:37:32: Tivadar Danka
minus f of x zero.

02:37:34: Tivadar Danka
So

02:37:36: Tivadar Danka
as you know, the slope of a line

02:37:38: Tivadar Danka
is the ratio of the of the of the

02:37:41: Tivadar Danka
of the

02:37:41: Tivadar Danka
increments of the projections with respect to both axes.

02:37:46: Tivadar Danka
So there, we have

02:37:49: Tivadar Danka
the slope of the extension line sorry. Not tangent. It it's called secant line.

02:37:55: Tivadar Danka
F of

02:37:56: Tivadar Danka
y minus f of x zero over

02:38:01: Tivadar Danka
y minus x zero.

02:38:03: Tivadar Danka
We can simplify this, basically. We can we can multiply both the numerator and the denominator by minus one.

02:38:09: Tivadar Danka
To obtain it to obtain f x zero minus f

02:38:15: Tivadar Danka
y

02:38:16: Tivadar Danka
over x zero minus y.

02:38:20: Tivadar Danka
So this what you see here

02:38:24: Tivadar Danka
is the same as in the definition.

02:38:28: Tivadar Danka
Let me check the the

02:38:30: Tivadar Danka
the

02:38:30: Tivadar Danka
the

02:38:32: Tivadar Danka
questions.

02:38:33: Tivadar Danka
First,

02:38:37: Tivadar Danka
No questions so far. Let me know if if the the explanation

02:38:41: Tivadar Danka
doesn't make sense, or do you have any questions about it?

02:38:45: Tivadar Danka
Alright. So so

02:38:48: Tivadar Danka
what what we are doing here

02:38:52: Tivadar Danka
then we take the limits.

02:38:54: Tivadar Danka
Let me draw

02:38:56: Tivadar Danka
another figure

02:39:01: Tivadar Danka
Hopefully, with the with the same function.

02:39:05: Tivadar Danka
So this is the same app of apps.

02:39:08: Tivadar Danka
What does it mean that y goes towards x

02:39:12: Tivadar Danka
is what I'm about to explain.

02:39:16: Tivadar Danka
So here, we have

02:39:20: Tivadar Danka
the text

02:39:23: Tivadar Danka
and

02:39:24: Tivadar Danka
why

02:39:25: Tivadar Danka
going to x?

02:39:27: Tivadar Danka
Meaning that we we we select

02:39:29: Tivadar Danka
these y value y variables closer and closer to x.

02:39:36: Tivadar Danka
So for instance, if this is

02:39:37: Tivadar Danka
y zero,

02:39:39: Tivadar Danka
I

02:39:41: Tivadar Danka
we have the slope of this line.

02:39:44: Tivadar Danka
Move a bit closer to to to x.

02:39:47: Tivadar Danka
We have this this y one. We take the slope of of this line.

02:39:59: Tivadar Danka
Alright. Then we move

02:40:03: Tivadar Danka
the

02:40:04: Tivadar Danka
bit more

02:40:05: Tivadar Danka
closer to to x.

02:40:10: Tivadar Danka
We take the slope of this second line and so on and so on. We we do do this.

02:40:16: Tivadar Danka
Until

02:40:17: Tivadar Danka
we we

02:40:18: Tivadar Danka
we reach x. And, basically,

02:40:21: Tivadar Danka
at that point,

02:40:23: Tivadar Danka
these secret lines

02:40:25: Tivadar Danka
will will be transformed into the tangent line.

02:40:33: Tivadar Danka
So this is the tangent line. Attacks.

02:40:37: Tivadar Danka
So this

02:40:39: Tivadar Danka
illustration is a bit

02:40:41: Tivadar Danka
cluttered right now. So let me

02:40:44: Tivadar Danka
take the next step on a different,

02:40:48: Tivadar Danka
illustration.

02:40:60: Tivadar Danka
So if we have

02:41:05: Tivadar Danka
x here,

02:41:09: Tivadar Danka
the tangent line,

02:41:11: Tivadar Danka
is a single line that touches the function is the line that touches the function's graph at at

02:41:16: Tivadar Danka
only a single point.

02:41:20: Tivadar Danka
Alright. This is the tangent line.

02:41:29: Tivadar Danka
And the derivative of f

02:41:33: Tivadar Danka
is the slope of the tension line.

02:41:37: Tivadar Danka
At x.

02:41:38: Tivadar Danka
So

02:41:40: Tivadar Danka
f prime x

02:41:43: Tivadar Danka
is the slope,

02:41:46: Tivadar Danka
of the tangent line Alright.

02:41:57: Tivadar Danka
So

02:41:59: Tivadar Danka
If we are talking about

02:42:01: Tivadar Danka
a time

02:42:04: Tivadar Danka
and and

02:42:05: Tivadar Danka
space plot of a moving object.

02:42:08: Tivadar Danka
Then the derivative

02:42:09: Tivadar Danka
is the the

02:42:12: Tivadar Danka
the speed of the of the

02:42:14: Tivadar Danka
object that moves, the instantaneous speed.

02:42:20: Tivadar Danka
I I am explaining this in the in the lecture notes that will be sent out to you, but I also explained this in in the in the

02:42:29: Tivadar Danka
book

02:42:30: Tivadar Danka
itself.

02:42:34: Tivadar Danka
In the book.

02:42:35: Tivadar Danka
I I dedicate a couple of

02:42:37: Tivadar Danka
chapters to to understanding what what the limit is.

02:42:41: Tivadar Danka
Mathematically speaking, it's it's it's quite a complicated

02:42:45: Tivadar Danka
So it's a complicated

02:42:48: Tivadar Danka
concept.

02:42:49: Tivadar Danka
But

02:42:50: Tivadar Danka
intuitively, it's really easy. You just get closer and closer to to x. With the variable

02:42:56: Tivadar Danka
y.

02:42:60: Tivadar Danka
And as I mentioned, there's also there's also also physics implementation.

02:43:06: Tivadar Danka
Alright.

02:43:09: Tivadar Danka
Couple of alternative, definitions or

02:43:14: Tivadar Danka
we we we define it as, again, as the limit y goes to x f of x minus f of

02:43:23: Tivadar Danka
y.

02:43:24: Tivadar Danka
Over x minus y.

02:43:26: Tivadar Danka
But there's another way to to write this limit.

02:43:31: Tivadar Danka
This is limit h to to zero.

02:43:38: Tivadar Danka
F x plus

02:43:39: Tivadar Danka
h minus f of x

02:43:45: Tivadar Danka
over

02:43:46: Tivadar Danka
h. Alright.

02:43:52: Tivadar Danka
I don't see any new questions.

02:43:53: Tivadar Danka
So far.

02:43:58: Tivadar Danka
Alright.

02:44:07: Tivadar Danka
We can we can move on.

02:44:17: Tivadar Danka
I wanted to mention one one more thing about the derivatives.

02:44:21: Tivadar Danka
Yeah. One one important fact.

02:44:29: Tivadar Danka
So

02:44:30: Tivadar Danka
the sign

02:44:32: Tivadar Danka
the derivative

02:44:33: Tivadar Danka
describes

02:44:35: Tivadar Danka
whether or not a function is increasing or decreasing.

02:44:40: Tivadar Danka
So

02:44:41: Tivadar Danka
Let's say we have this function

02:44:49: Tivadar Danka
that's

02:44:49: Tivadar Danka
oscillates

02:44:51: Tivadar Danka
back and forth.

02:44:53: Tivadar Danka
If you plot the tangent line here,

02:44:57: Tivadar Danka
the

02:44:58: Tivadar Danka
can see that the slope is positive.

02:45:01: Tivadar Danka
Actually, let's use the red color to

02:45:05: Tivadar Danka
to

02:45:05: Tivadar Danka
to

02:45:08: Tivadar Danka
to to to demonstrate that it's positive.

02:45:12: Tivadar Danka
On the other hand,

02:45:14: Tivadar Danka
if you take take the slope of the of the tangent line, at a point where the function is decreasing,

02:45:20: Tivadar Danka
for instance, here,

02:45:25: Tivadar Danka
you see that the slope is negative. Here, the slope is positive. So one one important aspect. This this can be translated to the to the language of of derivatives.

02:45:49: Tivadar Danka
Most importantly for us,

02:45:51: Tivadar Danka
if

02:45:53: Tivadar Danka
f prime of x

02:45:54: Tivadar Danka
is larger than than zero,

02:45:57: Tivadar Danka
Function is increasing.

02:46:05: Tivadar Danka
If f prime of access is less than zero,

02:46:07: Tivadar Danka
it means the function is decreasing.

02:46:18: Tivadar Danka
And this fundamental fact is is

02:46:21: Tivadar Danka
I mean, this this,

02:46:23: Tivadar Danka
property of of derivatives is behind the fact

02:46:27: Tivadar Danka
that

02:46:28: Tivadar Danka
gradient descent works. But we will talk more about it

02:46:33: Tivadar Danka
bit later.

02:46:36: Tivadar Danka
Actually, I wanted to just

02:46:38: Tivadar Danka
quickly highlight the the properties of of of

02:46:42: Tivadar Danka
differentiation.

02:46:45: Tivadar Danka
There are a couple of of, computational rules.

02:46:50: Tivadar Danka
So for instance, if you have two functions, f and g,

02:46:53: Tivadar Danka
then f plus g prime is f prime plus

02:46:57: Tivadar Danka
g prime

02:46:58: Tivadar Danka
or

02:46:60: Tivadar Danka
if we have a scalar c,

02:47:04: Tivadar Danka
then c times f prime is c times f prime.

02:47:09: Tivadar Danka
Where c is

02:47:12: Tivadar Danka
a real number.

02:47:13: Tivadar Danka
This too is called the linearity of the derivative. It's important because in practice, we compute derivatives by by

02:47:23: Tivadar Danka
by computing the relative for a couple of building blocks. And then using these these rules to to

02:47:29: Tivadar Danka
break them down into into components for which we can calculate the derivative by hand. And it's it's

02:47:37: Tivadar Danka
extremely important.

02:47:40: Tivadar Danka
I mean, for instance,

02:47:42: Tivadar Danka
back propagation.

02:47:43: Tivadar Danka
The whole entire back propagation algorithm

02:47:46: Tivadar Danka
is just to compute the derivatives

02:47:49: Tivadar Danka
within a huge network.

02:47:51: Tivadar Danka
Alright?

02:47:53: Tivadar Danka
So

02:47:54: Tivadar Danka
current news is very important.

02:47:58: Tivadar Danka
Alright.

02:47:60: Tivadar Danka
One more thing I wanted to mention about

02:48:02: Tivadar Danka
differentiation.

02:48:04: Tivadar Danka
And this is the the the chain rule.

02:48:06: Tivadar Danka
Bread and butter of of back propagation and neural networks.

02:48:12: Tivadar Danka
I mean,

02:48:14: Tivadar Danka
you can do

02:48:15: Tivadar Danka
more than just add them and and and and and, multiply functions together. You can also compose them.

02:48:22: Tivadar Danka
So for instance,

02:48:23: Tivadar Danka
f of g of x.

02:48:26: Tivadar Danka
I mean, just to give you give you a quick example,

02:48:30: Tivadar Danka
e to the power of x squared.

02:48:33: Tivadar Danka
This is a composite function, and these happen all the time in in machine learning.

02:48:39: Tivadar Danka
A neural network is a composition of its layers. So computing the composition computing the derivative of the composition is very important. And this is what the chain rule is about.

02:48:51: Tivadar Danka
So

02:48:52: Tivadar Danka
the derivative of f at g of x

02:48:56: Tivadar Danka
is f prime

02:48:58: Tivadar Danka
at g of x.

02:48:60: Tivadar Danka
Times g prime of x.

02:49:03: Tivadar Danka
This is called the chain rule.

02:49:14: Tivadar Danka
Really, the the only only way to familiarize familiarize yourself with differentiation and calculus is to is to

02:49:23: Tivadar Danka
to

02:49:24: Tivadar Danka
do a bunch of examples by hand. We don't have the time to to do that here. But I have included a couple of practice exercises in the in the in the lecture notes. That are going to be distributed

02:49:35: Tivadar Danka
after the worship is over.

02:49:37: Tivadar Danka
Not

02:49:38: Tivadar Danka
right way,

02:49:39: Tivadar Danka
because I still need a couple of days to to finish the the

02:49:43: Tivadar Danka
writing.

02:49:45: Tivadar Danka
But

02:49:45: Tivadar Danka
I

02:49:46: Tivadar Danka
rest assured, you will receive the lecture notes for the entire workshop. Alright. And now I want to

02:49:58: Tivadar Danka
talk about

02:49:58: Tivadar Danka
gradient descent.

02:50:07: Tivadar Danka
Let me find

02:50:09: Tivadar Danka
gradient descent in in in my notes.

02:50:22: Tivadar Danka
One very important thing I did not mention about

02:50:26: Tivadar Danka
the derivatives. So we have talked about the sign of the derivative. If it's positive, that means the function is increasing If it's negative, then the function is decreasing.

02:50:38: Tivadar Danka
This is this is the function, f of x.

02:50:41: Tivadar Danka
Machine learning always comes down

02:50:46: Tivadar Danka
to minimizing the loss function.

02:50:49: Tivadar Danka
Because loss function quantifies

02:50:51: Tivadar Danka
how well or model fits. The lower the loss is, the better the the the model fits the data.

02:50:58: Tivadar Danka
So, essentially,

02:50:60: Tivadar Danka
our goal is to minimize

02:51:02: Tivadar Danka
the loss function.

02:51:05: Tivadar Danka
It is it is it is done

02:51:07: Tivadar Danka
by a simple simple

02:51:10: Tivadar Danka
property of derivatives.

02:51:12: Tivadar Danka
If, visually, if you check here, this is where the minimum is.

02:51:17: Tivadar Danka
Can see that the tangent line

02:51:19: Tivadar Danka
completely vertical.

02:51:22: Tivadar Danka
This is a vertical tangent line.

02:51:30: Tivadar Danka
If this is

02:51:32: Tivadar Danka
at some x zero,

02:51:34: Tivadar Danka
it means that f prime of x zero is zero.

02:51:41: Tivadar Danka
Of course, this is not always the case. I mean I mean,

02:51:45: Tivadar Danka
if the if f has a minimum here yeah. Sorry. Horizontal. Yeah. Not vertical.

02:51:50: Tivadar Danka
I have a very strange brain, by the way. Like,

02:51:54: Tivadar Danka
I almost always conflict related concepts. Like,

02:51:57: Tivadar Danka
left and right, up and down, horizontal,

02:52:01: Tivadar Danka
vertical, but not just with respect to dimensions.

02:52:04: Tivadar Danka
These things I I always mix up.

02:52:06: Tivadar Danka
And feel free to point me out anytime if I if I mess this up.

02:52:11: Tivadar Danka
Yeah. So so

02:52:13: Tivadar Danka
I

02:52:14: Tivadar Danka
not vertical,

02:52:17: Tivadar Danka
Horizontal.

02:52:25: Tivadar Danka
Actually, it's it's not even a language problem.

02:52:28: Tivadar Danka
Because I I mixed these up in Hungarian as well, which is my my my my

02:52:33: Tivadar Danka
first language. Alright? So this is how my brain is wired. Sometimes,

02:52:38: Tivadar Danka
I wonder how I was able to get a PhD in mathematics.

02:52:43: Tivadar Danka
Never mind.

02:52:45: Tivadar Danka
I got one, and I'm here.

02:52:48: Tivadar Danka
Alright.

02:52:49: Tivadar Danka
So

02:52:50: Tivadar Danka
this is what we want to do.

02:52:53: Tivadar Danka
Alright?

02:52:55: Tivadar Danka
One one idea to find the minimum of any function is to solve this equation.

02:53:02: Tivadar Danka
Which I'm

02:53:03: Tivadar Danka
circling. This is an equation.

02:53:06: Tivadar Danka
However, if f is

02:53:10: Tivadar Danka
not simple,

02:53:11: Tivadar Danka
it's

02:53:12: Tivadar Danka
it's impossible to solve analytically.

02:53:16: Tivadar Danka
Because f can be, like, very complicated.

02:53:19: Tivadar Danka
Expression. And f f prime can be very complicated.

02:53:22: Tivadar Danka
So what we do

02:53:24: Tivadar Danka
as computer scientists is to apply a very simple

02:53:29: Tivadar Danka
iterative algorithm called gradient descent.

02:53:33: Tivadar Danka
I will explain gradient descent to you right now, and then we will implement it. After taking, a short break.

02:53:41: Tivadar Danka
But first,

02:53:42: Tivadar Danka
the gradient descent algorithm itself.

02:53:48: Tivadar Danka
So, again, we have this function here.

02:53:50: Tivadar Danka
And we're going to find this point. What we do is we take a guess. Alright? We take take a random point let's say.

02:53:59: Tivadar Danka
Here.

02:54:01: Tivadar Danka
I mean, let's

02:54:02: Tivadar Danka
let's

02:54:04: Tivadar Danka
let's draw

02:54:06: Tivadar Danka
the minimum point with red. Okay. So this is our first guess. This is x zero.

02:54:12: Tivadar Danka
What we do here

02:54:14: Tivadar Danka
is like,

02:54:15: Tivadar Danka
suppose that we are climbing a mountain, but instead of want to climb up, we want to climb down. We take a look at our left, take a look at our right,

02:54:24: Tivadar Danka
and we see

02:54:25: Tivadar Danka
which way the the

02:54:27: Tivadar Danka
the

02:54:28: Tivadar Danka
mountain itself

02:54:30: Tivadar Danka
goes down.

02:54:32: Tivadar Danka
And we see that if we take a step

02:54:35: Tivadar Danka
to the left,

02:54:38: Tivadar Danka
I was paying attention this time. If you take a step to the left, then

02:54:43: Tivadar Danka
we will we will descend

02:54:46: Tivadar Danka
in the in this in this

02:54:49: Tivadar Danka
valley.

02:54:50: Tivadar Danka
Right? And how do we decide which way is down is by taking a look at the derivative.

02:54:58: Tivadar Danka
So

02:55:01: Tivadar Danka
here, we have the derivative

02:55:03: Tivadar Danka
which is positive.

02:55:05: Tivadar Danka
F prime x zero,

02:55:09: Tivadar Danka
is larger than zero, it means that we have to have to move in in in

02:55:14: Tivadar Danka
to to the to the left.

02:55:15: Tivadar Danka
In order to to decrease or or or current height.

02:55:22: Tivadar Danka
So we take a step

02:55:24: Tivadar Danka
to the left.

02:55:25: Tivadar Danka
So

02:55:26: Tivadar Danka
We arrive here.

02:55:28: Tivadar Danka
This is our x one.

02:55:32: Tivadar Danka
Again, what we do is we look around, look at the

02:55:35: Tivadar Danka
left, look to the right,

02:55:37: Tivadar Danka
see which way is down. Again, we see that we have to have to move

02:55:42: Tivadar Danka
towards left in order to decrease. And if we take another step to the left, there we arrive to x two.

02:55:54: Tivadar Danka
Where we repeat this process all over again.

02:56:02: Tivadar Danka
Arriving at x x three and so on and so on. And if we do this long enough, we will

02:56:10: Tivadar Danka
hopefully, find a point which is at the minimum of of the of the whole

02:56:16: Tivadar Danka
loss landscape.

02:56:18: Tivadar Danka
Two things can go wrong. With with gradient descent.

02:56:23: Tivadar Danka
One one thing is that if we have a function

02:56:30: Tivadar Danka
such as this,

02:56:33: Tivadar Danka
if we start from I mean, this function has several

02:56:37: Tivadar Danka
local minimums.

02:56:38: Tivadar Danka
Right?

02:56:41: Tivadar Danka
This is a local minimum. This is a local minimum.

02:56:45: Tivadar Danka
This is a local minimum.

02:56:48: Tivadar Danka
This is local maximum.

02:56:50: Tivadar Danka
This is local maximum.

02:56:54: Tivadar Danka
But

02:56:55: Tivadar Danka
most importantly, if we do gradient descent,

02:56:58: Tivadar Danka
the video descend down into this valley here and and get stuck here.

02:57:05: Tivadar Danka
At this point here. However,

02:57:09: Tivadar Danka
this point

02:57:12: Tivadar Danka
has a better loss value.

02:57:13: Tivadar Danka
If we are talking about a machine learning model because it's it's it's slower.

02:57:18: Tivadar Danka
If we continue the function,

02:57:21: Tivadar Danka
that might be

02:57:26: Tivadar Danka
this point might indicate an even better fit for us. However, gradient descent makes us stuck

02:57:33: Tivadar Danka
here.

02:57:35: Tivadar Danka
This is one one caveat of

02:57:38: Tivadar Danka
really understand.

02:57:44: Tivadar Danka
These can be solved by, for instance, something called momentum. Rather, other other nice tricks which we won't

02:57:50: Tivadar Danka
talk about at this moment.

02:57:52: Tivadar Danka
But we will we will

02:57:55: Tivadar Danka
probably

02:57:58: Tivadar Danka
talk about it later maybe when we are implementing the the gradient descent algorithm.

02:58:02: Tivadar Danka
Think now it's time to take

02:58:04: Tivadar Danka
ten minutes a break. Let me just check for questions before we go on on the break. We don't have any more questions. Alright. Let's take

02:58:14: Tivadar Danka
ten more minutes, and this will be our last

02:58:18: Tivadar Danka
one hour session.

02:58:19: Tivadar Danka
They will they will implement a gradient descent algorithm.

02:58:23: Tivadar Danka
By the end of this this

02:58:25: Tivadar Danka
one hour session. Alright. So

02:58:28: Tivadar Danka
see you in ten minutes.

03:08:40: Tivadar Danka
Hey. Welcome back. Hope you can

03:08:44: Tivadar Danka
hear in Sydney properly.

03:08:49: Tivadar Danka
Let's talk about it a bit more. Let's talk a bit more about the Grady and the Sand.

03:08:57: Tivadar Danka
Alright. So now

03:08:59: Tivadar Danka
you have seen the

03:09:01: Tivadar Danka
idea, the motivation behind it.

03:09:04: Tivadar Danka
Now let's

03:09:05: Tivadar Danka
put this into mathematical terms.

03:09:10: Tivadar Danka
It's a gradient descent.

03:09:11: Tivadar Danka
Is basically defining the record of sequence

03:09:18: Tivadar Danka
So we have a first guess.

03:09:21: Tivadar Danka
X zero.

03:09:24: Tivadar Danka
Which is I don't know. I guess it's it's somewhere here.

03:09:28: Tivadar Danka
X zero.

03:09:31: Tivadar Danka
They compute the subsequent points

03:09:33: Tivadar Danka
by

03:09:34: Tivadar Danka
taking a look at the grad at at

03:09:37: Tivadar Danka
not necessarily I mean, at this point, we don't have gradients. We have a derivative.

03:09:41: Tivadar Danka
Take a look at the derivative.

03:09:43: Tivadar Danka
We decide which way to go.

03:09:46: Tivadar Danka
Where where where is the the

03:09:48: Tivadar Danka
decent

03:09:50: Tivadar Danka
And this is done.

03:09:52: Tivadar Danka
By actually like, I mean, suppose that

03:09:55: Tivadar Danka
x one and so on, x m are given.

03:10:01: Tivadar Danka
This is initial guess.

03:10:07: Tivadar Danka
Actually,

03:10:09: Tivadar Danka
let me delete these. These are not important.

03:10:13: Tivadar Danka
So

03:10:14: Tivadar Danka
x n plus one.

03:10:16: Tivadar Danka
Is defined by

03:10:22: Tivadar Danka
x m minus

03:10:26: Tivadar Danka
some constant h times f prime

03:10:30: Tivadar Danka
x m.

03:10:31: Tivadar Danka
This h is a learning rate.

03:10:48: Tivadar Danka
Think about this for a minute.

03:10:52: Tivadar Danka
Let's revisit

03:10:55: Tivadar Danka
this drawing here.

03:10:59: Tivadar Danka
Here, in the first instance,

03:11:02: Tivadar Danka
x zero,

03:11:05: Tivadar Danka
f prime

03:11:06: Tivadar Danka
is positive,

03:11:08: Tivadar Danka
which means we want to move to the left.

03:11:12: Tivadar Danka
Right?

03:11:13: Tivadar Danka
So we want want to decrease

03:11:16: Tivadar Danka
the value of x zero because the minimum will be

03:11:21: Tivadar Danka
more likely towards or left.

03:11:24: Tivadar Danka
Hence,

03:11:27: Tivadar Danka
the sign here

03:11:30: Tivadar Danka
Right? So we we subtract derivative.

03:11:32: Tivadar Danka
On the other hand,

03:11:34: Tivadar Danka
if we start from here, from the other side, for instance,

03:11:43: Tivadar Danka
if this is our x zero or

03:11:46: Tivadar Danka
whatever, this is our or

03:11:48: Tivadar Danka
x n.

03:11:49: Tivadar Danka
Then the derivative will be negative. Because the function is decreasing.

03:11:54: Tivadar Danka
This means that we want to to

03:11:58: Tivadar Danka
increase the value of our action.

03:12:01: Tivadar Danka
Because the the the

03:12:02: Tivadar Danka
local minimum will be

03:12:05: Tivadar Danka
to our right.

03:12:07: Tivadar Danka
So

03:12:09: Tivadar Danka
if we subtract a negative number,

03:12:14: Tivadar Danka
it means we add

03:12:15: Tivadar Danka
to to. Right? So this is how you can you can kind of

03:12:20: Tivadar Danka
understand

03:12:22: Tivadar Danka
why this minus sign is here.

03:12:26: Tivadar Danka
By the way, gradient descent was originally used for for

03:12:30: Tivadar Danka
maximizing the functions.

03:12:33: Tivadar Danka
And this is called gradient ascent. This is the default version of the algorithm. Gradient descent is is a modification there.

03:12:41: Tivadar Danka
Instead of going up, they go down.

03:12:44: Tivadar Danka
And I think we can

03:12:46: Tivadar Danka
implement this

03:12:48: Tivadar Danka
for ourselves in in in

03:12:50: Tivadar Danka
Python.

03:12:51: Tivadar Danka
It will be

03:12:52: Tivadar Danka
very easy.

03:12:54: Tivadar Danka
I promise.

03:12:59: Tivadar Danka
For the sake of simplicity, we are working with here with a very simple example function f of x is x squared.

03:13:09: Tivadar Danka
I prepared a plot for you which I'm copying and pasting from manager notes.

03:13:17: Tivadar Danka
You see, this is just the the plot of

03:13:19: Tivadar Danka
f of x.

03:13:22: Tivadar Danka
And, again, what we need to compute I'm putting this formula here just for us to to

03:13:30: Tivadar Danka
to have our site upon. X m plus one equals x m minus h times f prime

03:13:42: Tivadar Danka
x m. If you don't understand what I'm writing here, it's not a problem. It's something called LaTeX, which is the

03:13:50: Tivadar Danka
number one way to render mathematical equations.

03:13:54: Tivadar Danka
Of course, it was created by a computer scientist called Donald Knuth, He he is a legend.

03:14:01: Tivadar Danka
Lotak and tech in general, it's it's okay. Of the one of the most significant contribute significant contributions to the to the field of

03:14:10: Tivadar Danka
basically,

03:14:11: Tivadar Danka
mathematics, physics, and other sciences.

03:14:14: Tivadar Danka
I I love tech and. So enough praise about it.

03:14:19: Tivadar Danka
But you don't need it to to to to keep the pace with this structure.

03:14:26: Tivadar Danka
Here, we define the the

03:14:29: Tivadar Danka
so what what what do we have to do here?

03:14:32: Tivadar Danka
Is to also implement the derivative

03:14:36: Tivadar Danka
of of f.

03:14:38: Tivadar Danka
You have to take my word for it.

03:14:42: Tivadar Danka
If you if you don't know calculus really well,

03:14:46: Tivadar Danka
I included a couple of practice examples in the lecture notes.

03:14:50: Tivadar Danka
But I also have, like, a really, really long chapter on on calculus.

03:14:54: Tivadar Danka
The mathematics of machine learning textbook.

03:14:56: Tivadar Danka
This is two times x.

03:14:58: Tivadar Danka
Okay. So it's

03:14:59: Tivadar Danka
quite simple.

03:15:02: Tivadar Danka
I'm not going to plot this.

03:15:05: Tivadar Danka
Check if we have a question.

03:15:08: Tivadar Danka
Can you type the name of the equation program you are using

03:15:12: Tivadar Danka
Yes.

03:15:15: Tivadar Danka
LaTeX. This is this is how it's written. I mean, tac was the original version, and lotac is the is the more modern version.

03:15:26: Tivadar Danka
I mean,

03:15:26: Tivadar Danka
this is supported by markdown.

03:15:29: Tivadar Danka
It's it's it's it's a great

03:15:31: Tivadar Danka
great language. Alright.

03:15:34: Tivadar Danka
But let's not talk about that.

03:15:36: Tivadar Danka
Anymore.

03:15:37: Tivadar Danka
VIVIL implant

03:15:38: Tivadar Danka
gradient descent. And we'll also also visualize it

03:15:44: Tivadar Danka
So, actually, it requires a couple of of gradient descent function. It requires

03:15:50: Tivadar Danka
a couple of arguments. First, in what we want to to

03:15:54: Tivadar Danka
feed

03:15:54: Tivadar Danka
the derivative of function, which is f prime.

03:15:58: Tivadar Danka
Like, see here,

03:15:59: Tivadar Danka
I'm not sure I can highlight.

03:16:03: Tivadar Danka
Do you see my cursor? Let's just see my cursor.

03:16:06: Tivadar Danka
Alright.

03:16:07: Tivadar Danka
So this formula, I'm hovering around. Basically, it needs two two inputs.

03:16:14: Tivadar Danka
H and f prime.

03:16:17: Tivadar Danka
Third third input is is x zero, which is not in this formula, but it it's there implicitly. So we need x prime

03:16:23: Tivadar Danka
x zero, which is our ratio. Yes. We need the learning rate, which instead of h,

03:16:30: Tivadar Danka
I will I will I will name l r short for learning rate. This is customary in machine learning frameworks.

03:16:36: Tivadar Danka
IBS has a default value for it. It's 0.0101.

03:16:42: Tivadar Danka
And

03:16:42: Tivadar Danka
since

03:16:44: Tivadar Danka
we are moving from mathematics to to to more

03:16:48: Tivadar Danka
practical fields.

03:16:50: Tivadar Danka
We can't just run, like, a a a a requisition sequence indefinitely. We have to specify the number of steps we want to take.

03:16:59: Tivadar Danka
Which I will

03:17:02: Tivadar Danka
I will I will

03:17:04: Tivadar Danka
inform the gradient descent about it in the end steps argument. I will set a default argument, so we don't have to worry about this later.

03:17:15: Tivadar Danka
The implementation is is quite simple.

03:17:19: Tivadar Danka
In the end,

03:17:21: Tivadar Danka
I want to return a list.

03:17:24: Tivadar Danka
A list of faxes.

03:17:26: Tivadar Danka
This is x zero, x one, and so on.

03:17:30: Tivadar Danka
So actions via list. Whenever this function is called,

03:17:34: Tivadar Danka
axis

03:17:35: Tivadar Danka
will contain only environment x zero.

03:17:38: Tivadar Danka
Which will be our initial guess.

03:17:41: Tivadar Danka
And

03:17:42: Tivadar Danka
from them,

03:17:43: Tivadar Danka
they will use a four loop. So four four underscore in range,

03:17:50: Tivadar Danka
and steps, I will do the actual gradient descent

03:17:57: Tivadar Danka
part.

03:17:58: Tivadar Danka
Whenever I don't need the variable in the four loop, I I use an underscore. This is customary in Python.

03:18:05: Tivadar Danka
So what we have here

03:18:07: Tivadar Danka
is we we have to take x m.

03:18:11: Tivadar Danka
Which I'll be your name x current.

03:18:14: Tivadar Danka
And this is always the last

03:18:17: Tivadar Danka
element of the least axis

03:18:21: Tivadar Danka
Alright. We we won't use NumPy or anything here. We just want to keep things simple.

03:18:26: Tivadar Danka
Again, this is Python Python is very flexible.

03:18:29: Tivadar Danka
Which is more reason

03:18:30: Tivadar Danka
why they are slow, but we are not going for speed here. So x current is the last member of x's.

03:18:38: Tivadar Danka
In the first step of the four loop, this will be x zero. In second step of the forward loop, this will be

03:18:43: Tivadar Danka
x one. And now

03:18:47: Tivadar Danka
I have to add x next,

03:18:50: Tivadar Danka
and append the next element of x into the access list. And then it goes on and on and on until

03:18:59: Tivadar Danka
we exhaust all the steps. What is x next? It's very simple. It's the formula

03:19:06: Tivadar Danka
I wrote here. It's x current minus learning rate times f prime.

03:19:14: Tivadar Danka
At

03:19:14: Tivadar Danka
x current.

03:19:17: Tivadar Danka
So

03:19:19: Tivadar Danka
let's check.

03:19:22: Tivadar Danka
First if if this function

03:19:25: Tivadar Danka
if the gradient isn't implementation functions properly.

03:19:30: Tivadar Danka
So we have an f prime

03:19:32: Tivadar Danka
Let's x zero be, I don't know, minus 0.8.

03:19:38: Tivadar Danka
And we don't want to change the number of tests and the learning rate.

03:19:47: Tivadar Danka
An issue came up. Yeah. I mean, I I messed up the implementation of f prime because it returns none. I forgot to actually return the value. Now we have access. As you can see,

03:20:02: Tivadar Danka
these final elements are really close to zero. They still move around a little bit. They don't settle at zero. Because

03:20:13: Tivadar Danka
they keep the the the steps the gradient descent steps going. But it's it's a good enough result.

03:20:23: Tivadar Danka
Let's visualize this. Again, I I prepared the visualization code in advance because we don't want to take a look at how this tracks. By the way, I made this with with a larger.

03:20:35: Tivadar Danka
Because I don't

03:20:36: Tivadar Danka
like

03:20:39: Tivadar Danka
visualization code.

03:20:41: Tivadar Danka
I I think I think it's it's it's for me, it's exhausting to to write code, so I always trust a large lang when I have to do it.

03:20:49: Tivadar Danka
Alright. So this is this is x one here.

03:20:51: Tivadar Danka
Sorry. X zero x one, x two, x three. And so and you can see that the steps are getting

03:20:58: Tivadar Danka
smaller and and and and smaller.

03:21:06: Tivadar Danka
Actually, let's let's increase the the line width of of the of the path it takes.

03:21:14: Tivadar Danka
Because it is badly visible.

03:21:22: Tivadar Danka
Sorry. Sorry for just a second. I know that it's it's

03:21:24: Tivadar Danka
probably annoying, but I want want to demonstrate something to you. I mean,

03:21:31: Tivadar Danka
let me turn my night lights off.

03:21:34: Tivadar Danka
We have these these paths that the gradient is empty. It's here. What happens if we if we set the learning rate to be really, really low?

03:21:47: Tivadar Danka
And, basically, it gets stuck.

03:21:50: Tivadar Danka
A single point.

03:21:54: Tivadar Danka
Which means that the convergence can be really slow.

03:21:56: Tivadar Danka
Alright? It it's still it goes towards the minimum but it's really slow.

03:22:02: Tivadar Danka
On the other hand, what happens if it is too large?

03:22:07: Tivadar Danka
I mean, it's

03:22:08: Tivadar Danka
yeah, I mean, let's

03:22:10: Tivadar Danka
use 0.9.

03:22:13: Tivadar Danka
It bounces around. And the bounce bouncing around in this case, it works out.

03:22:18: Tivadar Danka
But it it can can be very bad for us.

03:22:24: Tivadar Danka
Because here,

03:22:26: Tivadar Danka
this this

03:22:27: Tivadar Danka
figure doesn't tell us, but the gradient descent

03:22:30: Tivadar Danka
diverges. So if the learning rate is is not appropriately scientific. So let's

03:22:36: Tivadar Danka
look at the first at the final couple of of axis. Like, you see these are

03:22:42: Tivadar Danka
really, really large.

03:22:43: Tivadar Danka
And they jump around

03:22:45: Tivadar Danka
quite a bit.

03:22:47: Tivadar Danka
So gradient descent

03:22:49: Tivadar Danka
requires some

03:22:50: Tivadar Danka
so called hyperparameter tuning. Learning rate is something which you have to have to figure out yourself.

03:22:58: Tivadar Danka
Getting a good initial guess is also

03:23:01: Tivadar Danka
has its own own own

03:23:03: Tivadar Danka
I don't know, art or or science behind it.

03:23:06: Tivadar Danka
In in machine learning machine learning terms,

03:23:09: Tivadar Danka
x zero or the initial guess

03:23:12: Tivadar Danka
Sorry. Not initial guess. Initial guess is is how you initialize the weights of your model. And there are

03:23:20: Tivadar Danka
dozens of articles written about this question. I mean, they what kind of probability distribution they should come from? Whatever. Because

03:23:29: Tivadar Danka
even though this

03:23:30: Tivadar Danka
might seem like a small issue,

03:23:33: Tivadar Danka
this can, in fact, impact the performance of of your of your

03:23:38: Tivadar Danka
training code. So

03:23:40: Tivadar Danka
I mean, it's it's quite an interesting subject.

03:23:43: Tivadar Danka
But we don't have time to to go

03:23:47: Tivadar Danka
towards it.

03:23:47: Tivadar Danka
Questions in q q and a, questions in the chat. No more questions.

03:23:52: Tivadar Danka
Alright. So we have roughly half an hour left

03:23:56: Tivadar Danka
on this workshop.

03:23:59: Tivadar Danka
I think

03:23:60: Tivadar Danka
let's let's make let's make an attempt at

03:24:03: Tivadar Danka
at basically generalizing this whole whole

03:24:07: Tivadar Danka
stuff in in higher dimensions. So let's move on.

03:24:10: Tivadar Danka
Towards multivariable calculus. Let me

03:24:14: Tivadar Danka
switch to my iPad, drink, sip of water, and then

03:24:19: Tivadar Danka
lights or rockets.

03:24:26: Tivadar Danka
Alright. What's wrong with what we have seen so far? The main issue is that machine learning models have more than one parameter.

03:24:36: Tivadar Danka
Alright? So

03:24:38: Tivadar Danka
even in the very simple case,

03:24:40: Tivadar Danka
of linear regression,

03:24:46: Tivadar Danka
this has two parameters.

03:24:48: Tivadar Danka
A and b.

03:24:49: Tivadar Danka
Which means that we have to optimize in two dimensions So what we see so far

03:24:55: Tivadar Danka
doesn't cut it.

03:24:57: Tivadar Danka
If we use a simple mean squared error,

03:25:01: Tivadar Danka
with linear regression, then we have the function l a b In the very, very basic case,

03:25:08: Tivadar Danka
one over n

03:25:10: Tivadar Danka
times sum as y goes from one to n. Where x, y,

03:25:15: Tivadar Danka
x I will be will be our dataset.

03:25:18: Tivadar Danka
A x I plus b minus

03:25:22: Tivadar Danka
y I to the squared

03:25:25: Tivadar Danka
X I is is the data.

03:25:29: Tivadar Danka
Or

03:25:30: Tivadar Danka
should I say feature?

03:25:38: Tivadar Danka
And the y is the target.

03:25:42: Tivadar Danka
Alright.

03:25:43: Tivadar Danka
Even in this case,

03:25:45: Tivadar Danka
the whole thing doesn't work.

03:25:50: Tivadar Danka
So we have to have to

03:25:53: Tivadar Danka
define

03:25:54: Tivadar Danka
different kind of ingredients and and

03:25:58: Tivadar Danka
whatever.

03:26:02: Tivadar Danka
Usually, for multivariable functions, we use the f of x notation where x is a vector.

03:26:12: Tivadar Danka
So

03:26:13: Tivadar Danka
This means that we are talking about the function of n variables. Right? Like, f x one and so on.

03:26:20: Tivadar Danka
X m.

03:26:24: Tivadar Danka
How to define

03:26:26: Tivadar Danka
the derivative of this function

03:26:30: Tivadar Danka
Can I just write f prime

03:26:32: Tivadar Danka
at x to be the limit

03:26:35: Tivadar Danka
as the y vector goes to x?

03:26:38: Tivadar Danka
F of x minus f of

03:26:42: Tivadar Danka
y.

03:26:42: Tivadar Danka
Over x minus

03:26:45: Tivadar Danka
y.

03:26:47: Tivadar Danka
I cannot

03:26:49: Tivadar Danka
because notice that

03:26:50: Tivadar Danka
we did not define

03:26:53: Tivadar Danka
division by director. Alright? It

03:26:56: Tivadar Danka
it

03:26:57: Tivadar Danka
doesn't make sense.

03:26:58: Tivadar Danka
Mathematically.

03:27:01: Tivadar Danka
So what what we can do here

03:27:04: Tivadar Danka
for instance, in a in a two variable function,

03:27:10: Tivadar Danka
For instance, f, x, and y. Is to fix one variable

03:27:21: Tivadar Danka
Sorry.

03:27:22: Tivadar Danka
Need to take another sip of water.

03:27:32: Tivadar Danka
I usually talk a lot, but talking for four hours is is still too much for me. And I

03:27:39: Tivadar Danka
We have f of two variables.

03:27:42: Tivadar Danka
And what we can do here is to fix one variable

03:27:46: Tivadar Danka
and take the derivative in the second variable.

03:27:49: Tivadar Danka
For instance.

03:27:51: Tivadar Danka
Just for for example, f

03:27:56: Tivadar Danka
f of x

03:27:57: Tivadar Danka
and

03:27:58: Tivadar Danka
I don't know, 42.

03:28:01: Tivadar Danka
This here

03:28:02: Tivadar Danka
is a univariate function, a function of a single variable.

03:28:06: Tivadar Danka
We can compute its derivative simply with respect to to x.

03:28:12: Tivadar Danka
So now

03:28:14: Tivadar Danka
I

03:28:14: Tivadar Danka
we can just think about y as as as a constant number.

03:28:19: Tivadar Danka
Is fixed and take the derivative with respect to to x. This is called the partial derivative.

03:28:26: Tivadar Danka
It is denoted by

03:28:28: Tivadar Danka
bunch of bunch of other bunch of ways.

03:28:31: Tivadar Danka
What I use is the is the

03:28:34: Tivadar Danka
partial symbol underscore one.

03:28:38: Tivadar Danka
This is what you see here.

03:28:40: Tivadar Danka
It's something called a partial symbol.

03:28:46: Tivadar Danka
Partial one means

03:28:47: Tivadar Danka
partial derivative with respect to the first variable.

03:28:50: Tivadar Danka
And here, the definition is

03:28:53: Tivadar Danka
take the limit

03:28:54: Tivadar Danka
as

03:28:55: Tivadar Danka
h goes to zero f of egg plus

03:29:00: Tivadar Danka
h

03:29:02: Tivadar Danka
and y minus f of egg and y over age.

03:29:08: Tivadar Danka
Similarly,

03:29:10: Tivadar Danka
the partial derivative with respect to the second variable, which is why

03:29:16: Tivadar Danka
denoted by by

03:29:18: Tivadar Danka
partial to

03:29:19: Tivadar Danka
f of x and y.

03:29:24: Tivadar Danka
Similarly, limit as age goes to zero.

03:29:28: Tivadar Danka
F of x and y plus

03:29:32: Tivadar Danka
h.

03:29:34: Tivadar Danka
Not plus, but minus f of x and y,

03:29:38: Tivadar Danka
overage.

03:29:41: Tivadar Danka
This is the partial derivative.

03:29:48: Tivadar Danka
The general notation, if we have I don't know, vector I mean, if you use the vector notation,

03:29:54: Tivadar Danka
then

03:29:54: Tivadar Danka
partial I of f of x, where x is the vector, is

03:30:01: Tivadar Danka
maybe

03:30:02: Tivadar Danka
grab your chairs or desks for a minute because I'm going to write some complicated notation here.

03:30:08: Tivadar Danka
This is the limit

03:30:13: Tivadar Danka
as age goes to zero.

03:30:14: Tivadar Danka
F of egg plus

03:30:18: Tivadar Danka
h times e y, ideally explain what this e I is, but

03:30:23: Tivadar Danka
you can tell your guess already. Minus

03:30:26: Tivadar Danka
f at x

03:30:29: Tivadar Danka
over

03:30:30: Tivadar Danka
h.

03:30:32: Tivadar Danka
But

03:30:33: Tivadar Danka
e sub

03:30:34: Tivadar Danka
I is a vector

03:30:37: Tivadar Danka
whose coordinates are zero,

03:30:41: Tivadar Danka
except for the highest coordinates, So this is the

03:30:46: Tivadar Danka
ice coordinates.

03:30:51: Tivadar Danka
This is the main dimensional vector. If anyone asks you

03:30:56: Tivadar Danka
this is the definition of the partial derivative.

03:30:58: Tivadar Danka
Alright.

03:31:07: Tivadar Danka
Here, the gradient effects is

03:31:10: Tivadar Danka
defined

03:31:13: Tivadar Danka
or maybe denoted by this this

03:31:16: Tivadar Danka
symbol, this upside down triangle called.

03:31:19: Tivadar Danka
Which is fun fact, it's it's a made up Greek letter.

03:31:24: Tivadar Danka
So it was I think it was it was it was coined by the physicist Paul Dirac. And it

03:31:30: Tivadar Danka
is, like, not a Greek letter, but it is supposed to look like one and sound like one. It is called.

03:31:38: Tivadar Danka
This is the gradient, and this is basically the

03:31:41: Tivadar Danka
partial derivative with respect to the first variable

03:31:44: Tivadar Danka
the vector of partial derivatives, basically.

03:31:55: Tivadar Danka
This is the gradient.

03:32:03: Tivadar Danka
Let's revisit

03:32:06: Tivadar Danka
or

03:32:06: Tivadar Danka
single variable example.

03:32:11: Tivadar Danka
This is f.

03:32:15: Tivadar Danka
This is our our point, and this is the tension plane.

03:32:18: Tivadar Danka
And

03:32:18: Tivadar Danka
basically,

03:32:20: Tivadar Danka
in multivariable

03:32:22: Tivadar Danka
for multivariable functions, I will show a couple of visualizations right

03:32:26: Tivadar Danka
after after I explain this.

03:32:28: Tivadar Danka
The gradient

03:32:30: Tivadar Danka
all these points

03:32:31: Tivadar Danka
towards the steepest increase

03:32:34: Tivadar Danka
Alright?

03:32:37: Tivadar Danka
Coincidentally, minus gradient

03:32:39: Tivadar Danka
points toward the steepest decrease.

03:32:42: Tivadar Danka
So

03:32:43: Tivadar Danka
now we are climbing a mountain that is in in

03:32:48: Tivadar Danka
two or more dimensions. It's not in one dimension.

03:32:50: Tivadar Danka
And we are at some point, and we take a look around. But but

03:32:54: Tivadar Danka
by taking a look around,

03:32:56: Tivadar Danka
instead of just looking to our left and looking to our right,

03:32:59: Tivadar Danka
have to take a look at

03:33:01: Tivadar Danka
every possible point around us.

03:33:03: Tivadar Danka
The the gradient will tell us

03:33:06: Tivadar Danka
which direction we should go to if we want to to to

03:33:10: Tivadar Danka
to

03:33:10: Tivadar Danka
take basically the steepest

03:33:12: Tivadar Danka
decrease.

03:33:14: Tivadar Danka
Coincidentally, the other other

03:33:17: Tivadar Danka
name for gradient descent

03:33:19: Tivadar Danka
is the method of steepest descent.

03:33:25: Tivadar Danka
Because the gradient points there.

03:33:29: Tivadar Danka
Alright. So we have a

03:33:30: Tivadar Danka
could you could you scroll up a bit?

03:33:34: Tivadar Danka
Sorry. I did not see this

03:33:37: Tivadar Danka
question or or request in the chat. I don't know

03:33:41: Tivadar Danka
where you want me to

03:33:42: Tivadar Danka
scroll to.

03:33:45: Tivadar Danka
I also alright. So you want this early.

03:33:51: Tivadar Danka
And we also have someone who can't hear

03:33:54: Tivadar Danka
Please.

03:33:54: Tivadar Danka
Confirm if this is still an issue for you.

03:34:02: Tivadar Danka
My microphone

03:34:05: Tivadar Danka
seems to be turned on, and others can hear. So I don't know what's the issue.

03:34:10: Tivadar Danka
Audio is working for you. Alright. So

03:34:14: Tivadar Danka
I think I can I can move on with with couple of visualizations? So

03:34:20: Tivadar Danka
for single variable functions, we had graphs. Which are which are basically just

03:34:25: Tivadar Danka
curves

03:34:26: Tivadar Danka
drawn

03:34:27: Tivadar Danka
in the plane.

03:34:29: Tivadar Danka
Here,

03:34:30: Tivadar Danka
in multiple dimensions, we will have

03:34:33: Tivadar Danka
not graphs, but

03:34:34: Tivadar Danka
surfaces. And I prepared a couple of

03:34:39: Tivadar Danka
illustrations for you.

03:34:41: Tivadar Danka
Using the the plot library.

03:34:43: Tivadar Danka
These are

03:34:44: Tivadar Danka
going to be interactive visualizations. So again, we will take a very simple function f

03:34:54: Tivadar Danka
if you want

03:34:57: Tivadar Danka
the mathematical definition,

03:34:59: Tivadar Danka
it is x

03:34:60: Tivadar Danka
square plus y square. This is what we will see.

03:35:06: Tivadar Danka
And I'll just just import the

03:35:09: Tivadar Danka
the

03:35:10: Tivadar Danka
visualization code

03:35:22: Tivadar Danka
So this is the surface given by this function.

03:35:26: Tivadar Danka
This is a

03:35:28: Tivadar Danka
beautiful, simple

03:35:30: Tivadar Danka
complex function.

03:35:33: Tivadar Danka
Whenever you will receive the the

03:35:35: Tivadar Danka
version notes, you will be able to

03:35:38: Tivadar Danka
interactively

03:35:39: Tivadar Danka
turn this around.

03:35:44: Tivadar Danka
One major difference between single and multivariable calculus is that

03:35:49: Tivadar Danka
in single variables,

03:35:50: Tivadar Danka
they had a tangent line.

03:35:53: Tivadar Danka
Now

03:35:54: Tivadar Danka
we don't have a tensioned line.

03:35:56: Tivadar Danka
Have a tangent plane.

03:36:02: Tivadar Danka
Again, I don't really want to to

03:36:06: Tivadar Danka
explain the mathematical details at this point. For instance, like,

03:36:10: Tivadar Danka
how this tension pain is defined is is

03:36:13: Tivadar Danka
although it's it's interesting and instructive

03:36:16: Tivadar Danka
let's not talk about this. Also, let's not talk about how you grade is defined. I just want to show this to

03:36:24: Tivadar Danka
because we don't have the the time to go into that,

03:36:28: Tivadar Danka
fine details.

03:36:30: Tivadar Danka
Because I still want to implement the

03:36:33: Tivadar Danka
very general version of of

03:36:36: Tivadar Danka
gradient descent. Like you see here,

03:36:38: Tivadar Danka
this is the function graph.

03:36:41: Tivadar Danka
And there we have the the gradient sorry. The the the, yeah, the gradient plane at this point. The point itself is defined by

03:36:50: Tivadar Danka
the coordinates zero point five and zero point two.

03:36:53: Tivadar Danka
When you receive the code, you will play play around with this interactively. Alright. So this is one one major difference. And

03:37:02: Tivadar Danka
basically, gradient shows us the point of our steepest descent. So what I want to

03:37:10: Tivadar Danka
show

03:37:12: Tivadar Danka
to you

03:37:14: Tivadar Danka
is how to train, like, a multivariable

03:37:18: Tivadar Danka
machine learning model with with with the general version of gray and the sentence format. We will we will use

03:37:25: Tivadar Danka
the the

03:37:26: Tivadar Danka
linear regression

03:37:28: Tivadar Danka
example.

03:37:33: Tivadar Danka
Let's go to my

03:37:35: Tivadar Danka
iPad a bit.

03:37:42: Tivadar Danka
Let's see if we have any questions so far.

03:37:52: Tivadar Danka
What was the vector e again?

03:37:54: Tivadar Danka
Sorry.

03:37:56: Tivadar Danka
I I answered this question five minutes later, but I didn't see the question.

03:37:59: Tivadar Danka
This is this is directory.

03:38:04: Tivadar Danka
E y is the vector whose ice coordinate is one and the other coordinate is zero. In in in two yeah. Sorry. In three dimensions,

03:38:13: Tivadar Danka
e one is one zero zero,

03:38:18: Tivadar Danka
e two is zero one zero,

03:38:22: Tivadar Danka
and e three is zero zero one.

03:38:26: Tivadar Danka
Alright. I hope it makes sense.

03:38:29: Tivadar Danka
Is this complex variables? We are not talking about complex variables.

03:38:34: Tivadar Danka
God. No. These are these are all real numbers. I mean, if if we bring complex numbers into the picture,

03:38:41: Tivadar Danka
then

03:38:42: Tivadar Danka
we are

03:38:42: Tivadar Danka
making our job much more difficult. These are just

03:38:46: Tivadar Danka
plain

03:38:46: Tivadar Danka
good old real numbers. Alright.

03:38:50: Tivadar Danka
The numbers are

03:38:50: Tivadar Danka
simple.

03:38:52: Tivadar Danka
We like your numbers. Alright.

03:38:55: Tivadar Danka
Back to back to the to the explanation.

03:38:59: Tivadar Danka
So

03:39:01: Tivadar Danka
they have the the single variable gradient descent

03:39:05: Tivadar Danka
in our minds. Let me

03:39:06: Tivadar Danka
just

03:39:08: Tivadar Danka
let me just

03:39:10: Tivadar Danka
scribble it down. X m plus one is defined by x n minus

03:39:16: Tivadar Danka
learning rate times f prime

03:39:18: Tivadar Danka
of x m.

03:39:22: Tivadar Danka
Multivariable gradient descent

03:39:23: Tivadar Danka
works exactly the same.

03:39:26: Tivadar Danka
But instead of a derivative,

03:39:27: Tivadar Danka
like, instead of the derivative function, we have the gradient has the name gradient descent.

03:39:34: Tivadar Danka
Generally, x n plus one equals x m

03:39:39: Tivadar Danka
minus

03:39:41: Tivadar Danka
h times

03:39:42: Tivadar Danka
the gradient of

03:39:44: Tivadar Danka
f at

03:39:47: Tivadar Danka
x m.

03:39:48: Tivadar Danka
And you see here

03:39:51: Tivadar Danka
this is already vectorized. Right? This is what the entire first part is all about. The whole vectorization stuff.

03:40:02: Tivadar Danka
The same

03:40:03: Tivadar Danka
formula works. The only thing we change is the function.

03:40:08: Tivadar Danka
This is this is good. We like this. It is simple, easy.

03:40:13: Tivadar Danka
Makes our job

03:40:14: Tivadar Danka
much easier.

03:40:18: Tivadar Danka
Alright.

03:40:19: Tivadar Danka
So

03:40:21: Tivadar Danka
So here, we want to optimize

03:40:24: Tivadar Danka
The loss function l a b

03:40:31: Tivadar Danka
Again, we seen this before. It's one over m

03:40:34: Tivadar Danka
times

03:40:37: Tivadar Danka
y going from one to n.

03:40:39: Tivadar Danka
A x

03:40:41: Tivadar Danka
I plus b minus y I in the square. This is called me squared error. It's

03:40:49: Tivadar Danka
one of the most

03:40:50: Tivadar Danka
common

03:40:52: Tivadar Danka
most famous, loss functions.

03:40:55: Tivadar Danka
First, we need to calculate the gradient of this function.

03:40:60: Tivadar Danka
Partial one l

03:41:02: Tivadar Danka
at a b.

03:41:06: Tivadar Danka
Is basically

03:41:07: Tivadar Danka
if you if you follow the lecture notes and the book closer, you will be able to compute the the the gradient by hand.

03:41:15: Tivadar Danka
Let me just write it down for you.

03:41:21: Tivadar Danka
Because I want to show you the wonders of of

03:41:23: Tivadar Danka
gradient descent. I want want us to be able to train

03:41:28: Tivadar Danka
like, an actual machine learning model before we finish up this workshop.

03:41:36: Tivadar Danka
So here, we take the derivative with respect to a,

03:41:40: Tivadar Danka
Of course, this involves the chain rule, so this is

03:41:43: Tivadar Danka
two times x

03:41:45: Tivadar Danka
I times

03:41:46: Tivadar Danka
a x I plus

03:41:50: Tivadar Danka
b minus

03:41:50: Tivadar Danka
y I.

03:41:52: Tivadar Danka
And we lose the square because of how

03:41:54: Tivadar Danka
differentiation works.

03:41:57: Tivadar Danka
Remember that the first variable of l is is a

03:41:60: Tivadar Danka
less compute the derivative with respect to b.

03:42:06: Tivadar Danka
That is the second variable.

03:42:09: Tivadar Danka
Even simpler because if you take a look at this expression,

03:42:14: Tivadar Danka
consider it

03:42:16: Tivadar Danka
as

03:42:18: Tivadar Danka
a x

03:42:19: Tivadar Danka
I and y I being fixed, then the derivative is simple.

03:42:23: Tivadar Danka
It's just

03:42:26: Tivadar Danka
two times a x I plus

03:42:29: Tivadar Danka
b minus

03:42:30: Tivadar Danka
y I. Alright. So

03:42:33: Tivadar Danka
gradient is is quite easy.

03:42:36: Tivadar Danka
To

03:42:38: Tivadar Danka
do.

03:42:38: Tivadar Danka
To compute

03:42:40: Tivadar Danka
Let me check for questions.

03:42:44: Tivadar Danka
Okay. Real and then that's real components.

03:42:51: Tivadar Danka
Yeah. The the the e vectors are also described in in the book.

03:42:56: Tivadar Danka
If you want to be really fancy about it, they are called standard orthonormal basis.

03:43:03: Tivadar Danka
But we we don't want to use such

03:43:06: Tivadar Danka
big words for now.

03:43:10: Tivadar Danka
Let me switch to my

03:43:13: Tivadar Danka
Versus code instance.

03:43:16: Tivadar Danka
So first, we we

03:43:18: Tivadar Danka
generate some some training data

03:43:22: Tivadar Danka
x will be the

03:43:25: Tivadar Danka
training data itself, randomly distributed

03:43:30: Tivadar Danka
mean, we can we can take a look. It's

03:43:33: Tivadar Danka
it's

03:43:34: Tivadar Danka
it's it's it's just a list of of real, like, random real numbers. For for us, I mean,

03:43:41: Tivadar Danka
this toy dataset will be enough because

03:43:44: Tivadar Danka
again, we are talking about a simple

03:43:48: Tivadar Danka
linear regression model. And and

03:43:50: Tivadar Danka
why

03:43:51: Tivadar Danka
is obtained I mean,

03:43:54: Tivadar Danka
by by

03:43:55: Tivadar Danka
by

03:43:56: Tivadar Danka
multiplying points from ads by 1.2,

03:44:00: Tivadar Danka
and adding 0.1,

03:44:02: Tivadar Danka
plus some random noise.

03:44:07: Tivadar Danka
I think we can we can plot this

03:44:10: Tivadar Danka
simply.

03:44:14: Tivadar Danka
I have some visualization codes from the from the first part of the lecture.

03:44:22: Tivadar Danka
This is doctor Amy. Alright. So

03:44:25: Tivadar Danka
linear regression model will fit perfectly.

03:44:28: Tivadar Danka
Because it is generated in the way that that linear regression would work.

03:44:34: Tivadar Danka
So the model they want to obtain is 1.2 times x plus 0.1. A should be close to 1.2. B should be close to to 0.1.

03:44:45: Tivadar Danka
Alright.

03:44:51: Tivadar Danka
Now we can even even plot the Okay. Let's let's not forget to to

03:44:57: Tivadar Danka
plot the

03:44:59: Tivadar Danka
loss function itself and the model.

03:45:06: Tivadar Danka
So the model is just simply a times x plus b. Alright. We have seen this.

03:45:14: Tivadar Danka
The loss function depends on a, d, and x, and y. Which is our our dataset itself.

03:45:22: Tivadar Danka
This is basically the sum of something

03:45:26: Tivadar Danka
over

03:45:28: Tivadar Danka
the length of that x.

03:45:31: Tivadar Danka
Actually, if if we use NumPy, they can just they can just take

03:45:37: Tivadar Danka
MP dot mean

03:45:45: Tivadar Danka
Without

03:45:46: Tivadar Danka
x a b, miles, y, square

03:45:52: Tivadar Danka
Let us check if it works.

03:46:00: Tivadar Danka
It works properly. So this is the loss function. In NumPy notation,

03:46:06: Tivadar Danka
Again, this is this is

03:46:07: Tivadar Danka
already vectorized. Right?

03:46:11: Tivadar Danka
We are talking about univariate.

03:46:13: Tivadar Danka
Model.

03:46:15: Tivadar Danka
Of course. So

03:46:17: Tivadar Danka
there is not much vectorization going around here, but

03:46:20: Tivadar Danka
the implementation of the of the of the

03:46:22: Tivadar Danka
loss function is is is nice and simple.

03:46:26: Tivadar Danka
Alright. Let's let's visualize this loss landscape.

03:46:30: Tivadar Danka
And this is where things are

03:46:31: Tivadar Danka
getting interesting.

03:46:35: Tivadar Danka
Mostly because my visualization code doesn't work.

03:46:58: Tivadar Danka
K. So that there is something going on with with the

03:47:02: Tivadar Danka
non py implementation. So so let's let's do with the plain plain old way. This contains

03:47:10: Tivadar Danka
So we have a sum in the formula of the of the

03:47:14: Tivadar Danka
of the of the

03:47:15: Tivadar Danka
loss function.

03:47:17: Tivadar Danka
So

03:47:19: Tivadar Danka
Let me just

03:47:21: Tivadar Danka
copy the the Lattek code for it so that our implementation is is

03:47:26: Tivadar Danka
easier. Alright. So this is this is what we have here.

03:47:29: Tivadar Danka
The second line.

03:47:31: Tivadar Danka
So that you will see this simultaneously.

03:47:35: Tivadar Danka
So this is this this end here is the length of the of the datasets. And what's the sum? Is something

03:47:46: Tivadar Danka
for x and y in

03:47:49: Tivadar Danka
the ZIP.

03:47:51: Tivadar Danka
Of capital x, capital y. Here, basically,

03:47:58: Tivadar Danka
what we have is model

03:47:59: Tivadar Danka
the model of of

03:48:01: Tivadar Danka
of,

03:48:02: Tivadar Danka
I mean, the prediction given by the model, which is

03:48:06: Tivadar Danka
in this code. This is written by model x a and b. And we we subtract the ground root and and raise it to the square.

03:48:15: Tivadar Danka
Now it works. Now the visualization code should also work. This is the loss landscape.

03:48:24: Tivadar Danka
Which is

03:48:26: Tivadar Danka
I I think it's not not that surprising because it's a it's a simple model. So you see that that

03:48:30: Tivadar Danka
right here, there there's a valley.

03:48:33: Tivadar Danka
And and

03:48:34: Tivadar Danka
if you pick a parameter from from here like, each each point in this surface corresponds to to a model, like, a parameter configuration of a model.

03:48:45: Tivadar Danka
And here here, we see that the the loss is the smallest around here.

03:48:50: Tivadar Danka
And they will show using gradient descent that

03:48:54: Tivadar Danka
the the

03:48:55: Tivadar Danka
loss will indeed converge there.

03:48:59: Tivadar Danka
Do we have any any new questions

03:49:05: Tivadar Danka
Not yet.

03:49:06: Tivadar Danka
Let me know anytime. Now it's time to implement the the gradient of of, the loss function.

03:49:15: Tivadar Danka
Which I will call l grad.

03:49:21: Tivadar Danka
Again, it's a function of a b x and y.

03:49:25: Tivadar Danka
Which which are or or or data.

03:49:28: Tivadar Danka
They will confuse the the

03:49:30: Tivadar Danka
gradient, fill this back to a.

03:49:34: Tivadar Danka
The gradient with respect to b,

03:49:35: Tivadar Danka
the

03:49:37: Tivadar Danka
and return a top row.

03:49:40: Tivadar Danka
We are not going to use NumPy here. We are just going

03:49:43: Tivadar Danka
good old Python

03:49:47: Tivadar Danka
So let me copy the the gradient here so that we can

03:49:51: Tivadar Danka
we can see it while we implement this. Again, I will use the the

03:49:59: Tivadar Danka
wonderful LaTeX

03:50:02: Tivadar Danka
package to render

03:50:04: Tivadar Danka
equations.

03:50:11: Tivadar Danka
This is the gradient itself.

03:50:15: Tivadar Danka
You see that these are, again, basically sun's

03:50:21: Tivadar Danka
Like, the the the

03:50:22: Tivadar Danka
loss function itself.

03:50:26: Tivadar Danka
The only difference is what's inside this. Some

03:50:28: Tivadar Danka
both run through x and y for x and y in the zip of of

03:50:38: Tivadar Danka
x and y.

03:50:45: Tivadar Danka
The only difference is that it's the text y.

03:50:47: Tivadar Danka
Factor here. So here, we have

03:50:50: Tivadar Danka
two times

03:50:52: Tivadar Danka
x times

03:50:56: Tivadar Danka
a times x plus b minus y. And below,

03:51:05: Tivadar Danka
we have two times a

03:51:10: Tivadar Danka
times x

03:51:10: Tivadar Danka
plus b minus y. Alright. This is the the gradient itself. And now we are ready to do the gradient on this end.

03:51:23: Tivadar Danka
In

03:51:23: Tivadar Danka
multiple dimensions.

03:51:25: Tivadar Danka
So, again,

03:51:27: Tivadar Danka
it depends on

03:51:28: Tivadar Danka
the whole entire gradient. This function, we have to supply the the

03:51:32: Tivadar Danka
gradient function itself. The initial guesses in both parameters the data, the number of steps we want to take, and the learning rate, which is 0.1 by default.

03:51:47: Tivadar Danka
Again,

03:51:48: Tivadar Danka
we will return a list of w's short for weights. The initial guess is

03:51:55: Tivadar Danka
a zero, b zero.

03:51:57: Tivadar Danka
And

03:51:59: Tivadar Danka
there will be a for loop here

03:52:07: Tivadar Danka
This should be implemented later.

03:52:09: Tivadar Danka
Point point is we return the list of of parameter configurations. And

03:52:17: Tivadar Danka
the code is almost exactly the same as before. So we extract the current values of the of the parameters, which is basically a n and b n.

03:52:34: Tivadar Danka
These these are are coming from the the

03:52:36: Tivadar Danka
last element of w's.

03:52:39: Tivadar Danka
And they compute the gradient

03:52:42: Tivadar Danka
which I I didn't also with d a and d b.

03:52:46: Tivadar Danka
The gradient depends on on the

03:52:51: Tivadar Danka
parameter values

03:52:55: Tivadar Danka
and the data itself based

03:52:56: Tivadar Danka
Don't forget that if your data changed, then your your your loss landscape also changes and simply compute the

03:53:07: Tivadar Danka
next

03:53:09: Tivadar Danka
iteration by doing exactly the same thing as we did before. So take the current value,

03:53:15: Tivadar Danka
and we subtract the learning rate times the gradient.

03:53:19: Tivadar Danka
Which this is not vectorized at the moment. So, basically, what's what's here is

03:53:29: Tivadar Danka
the

03:53:30: Tivadar Danka
univariate gradient descent, but in in in two dimensions.

03:53:34: Tivadar Danka
And we we also append this new

03:53:38: Tivadar Danka
new weight configuration

03:53:44: Tivadar Danka
Alright.

03:53:46: Tivadar Danka
Let's see see if it works. So let's be

03:53:50: Tivadar Danka
the result of of a gradient descent.

03:53:55: Tivadar Danka
I don't know. I guess we can start from milestone

03:53:58: Tivadar Danka
and milestone. If you check check or or

03:54:01: Tivadar Danka
plot here, I mean, the ticks are

03:54:05: Tivadar Danka
incorrect in this plot, but

03:54:07: Tivadar Danka
the range

03:54:08: Tivadar Danka
of this plot

03:54:10: Tivadar Danka
is is minus ten and ten in both

03:54:12: Tivadar Danka
both axes, which are a and b this time.

03:54:19: Tivadar Danka
It gives data. And

03:54:23: Tivadar Danka
last role.

03:54:31: Tivadar Danka
There's a visual glitch going around in my in my in my visual code node Visual Studio.

03:54:37: Tivadar Danka
Instance. So let's see the the last three instances.

03:54:42: Tivadar Danka
I mean, we see that it's it's

03:54:46: Tivadar Danka
decent.

03:54:48: Tivadar Danka
I mean, we can we can basically

03:54:53: Tivadar Danka
add a couple of more steps reduce our learning rates,

03:54:57: Tivadar Danka
to avoid

03:54:59: Tivadar Danka
jumping around

03:54:59: Tivadar Danka
And now now we are talking

03:55:10: Tivadar Danka
Alright.

03:55:12: Tivadar Danka
Thousand steps, or maybe can we do one of these tests with learning rate one?

03:55:18: Tivadar Danka
That's really this this is the result. You can see that this is

03:55:25: Tivadar Danka
close to to what we we we had had in mind.

03:55:30: Tivadar Danka
Let's visualize this. Again, I will I will copy the visualization code directly because it is not that important. And you see that the gradient and the sun beautiful converges

03:55:44: Tivadar Danka
somewhere.

03:55:46: Tivadar Danka
Around here. We can even zoom in Zooming is pretty slow in this

03:55:54: Tivadar Danka
tool for some reason.

03:55:57: Tivadar Danka
But to see that we we got skin in here.

03:55:59: Tivadar Danka
But the multidimensional gradient works.

03:56:04: Tivadar Danka
I I I think this is quite quite an excellent time to to close the version because if you think about it, we we built something that

03:56:11: Tivadar Danka
that is is is working properly. We can see the result.

03:56:15: Tivadar Danka
Even though this is a simple linear regression,

03:56:18: Tivadar Danka
I think

03:56:19: Tivadar Danka
watching

03:56:21: Tivadar Danka
the the entire gradient descent process is is very

03:56:25: Tivadar Danka
instructive, and I find it beautiful. Like, these are the moments I I enjoy the most whenever I'm doing computer science or building something from scratch. The first time.

03:56:35: Tivadar Danka
I see that it works.

03:56:36: Tivadar Danka
In the past four hours, we studied a bunch of mathematical theory We also

03:56:43: Tivadar Danka
reviewed a bunch of

03:56:45: Tivadar Danka
Python stuff, object oriented Python.

03:56:47: Tivadar Danka
It was all all towards this goal, to make

03:56:51: Tivadar Danka
this work.

03:56:52: Tivadar Danka
It's

03:56:54: Tivadar Danka
not state of the art.

03:56:56: Tivadar Danka
But we did it.

03:56:58: Tivadar Danka
By ourselves.

03:56:59: Tivadar Danka
And this is this is this is the moment I I enjoy the most.

03:57:05: Tivadar Danka
And I think it's it's the the

03:57:07: Tivadar Danka
very good point to to conclude The workshop

03:57:13: Tivadar Danka
I finished it just in time. In the lecture notes, which will be distributed to, there will be

03:57:19: Tivadar Danka
a bit more to it.

03:57:22: Tivadar Danka
I will also vectorize the code, which we just seen by now.

03:57:27: Tivadar Danka
Did did you have any any questions or concerns I I I would appreciate

03:57:32: Tivadar Danka
I see

03:57:33: Tivadar Danka
some some in the chat.

03:57:37: Tivadar Danka
Is there a solution to the books and of chatbot problems?

03:57:40: Tivadar Danka
Unfortunately, not. We have a bunch of problems, and I I didn't have time to to compile, like, a

03:57:47: Tivadar Danka
a full

03:57:48: Tivadar Danka
document on the solutions. But as someone pointed out,

03:57:52: Tivadar Danka
you can you can talk about them in the Discord group for the book because I'm I'm a participant in the Discord chat. I frequently

03:58:00: Tivadar Danka
check Discord for messages. I think I I use it more than any other messaging messaging app.

03:58:07: Tivadar Danka
So whenever you have a question to to me,

03:58:09: Tivadar Danka
you can reach me there. Hopefully, I will answer within a couple of days. If not,

03:58:15: Tivadar Danka
there's yeah.

03:58:17: Tivadar Danka
Is there a way to get in touch with

03:58:19: Tivadar Danka
with me?

03:58:20: Tivadar Danka
Of course. A Discord group.

03:58:23: Tivadar Danka
For the for the book itself.

03:58:26: Tivadar Danka
Could you share the visualization code here? I'm very close to finish the code and would like to see the same visuals

03:58:34: Tivadar Danka
as as you did.

03:58:39: Tivadar Danka
I'm I'm not sure if if copying and pasting this in the chat would be

03:58:43: Tivadar Danka
would be good. But

03:58:45: Tivadar Danka
can you

03:58:47: Tivadar Danka
I think I think I think they sent you a message here.

03:58:52: Tivadar Danka
Yeah. I sent I sent the message to you here, and I will I will send the visual position code.

03:58:56: Tivadar Danka
On on on this this channel.

03:58:60: Tivadar Danka
Alright? So so we don't for done.

03:59:03: Tivadar Danka
Put, like, blocks of code into into the the the

03:59:09: Tivadar Danka
let me check the q and a channel

03:59:19: Tivadar Danka
Link to the to the Discord group, please.

03:59:21: Tivadar Danka
I think the the the host for for the can add add the the links here. I don't know if I can send you an invite to this group group because I might not have the permissions for that.

03:59:32: Tivadar Danka
Can you explain the convergence again, what limits what limits are?

03:59:37: Tivadar Danka
What preferences from diverging?

03:59:41: Tivadar Danka
Everything can diverge.

03:59:43: Tivadar Danka
First of all. Even even the the

03:59:47: Tivadar Danka
the terms in the derivative

03:59:48: Tivadar Danka
can diverge. I mean,

03:59:51: Tivadar Danka
yeah. It's I think it's I mean, it it will be instructive to see see an example. Just bear with me for a minute. I I think we have a half an hour for this q and a session.

04:00:02: Tivadar Danka
I will I will answer the questions

04:00:05: Tivadar Danka
whenever up up until we're out of time.

04:00:09: Tivadar Danka
To answer this question,

04:00:11: Tivadar Danka
please let me show you an example of a function that is non differentiable I know this wasn't exactly your question, but let me also get to that a bit later.

04:00:31: Tivadar Danka
So we have the the

04:00:32: Tivadar Danka
the

04:00:34: Tivadar Danka
famous absolute value function. F of x equals x. And

04:00:41: Tivadar Danka
the issue with dysfunction

04:00:43: Tivadar Danka
it's not not differentiable at zero.

04:00:48: Tivadar Danka
Because

04:00:50: Tivadar Danka
there is no no attention planned here.

04:00:53: Tivadar Danka
So

04:00:55: Tivadar Danka
if you check the difference quotient, f x

04:00:59: Tivadar Danka
minus f zero or maybe instead of

04:01:07: Tivadar Danka
of x,

04:01:08: Tivadar Danka
I should write

04:01:10: Tivadar Danka
h

04:01:18: Tivadar Danka
h is basically

04:01:22: Tivadar Danka
a

04:01:23: Tivadar Danka
one if

04:01:26: Tivadar Danka
h is larger than zero and minus one if

04:01:30: Tivadar Danka
h is less than zero. Right? So the limit

04:01:38: Tivadar Danka
h zero h over h

04:01:41: Tivadar Danka
doesn't exist.

04:01:47: Tivadar Danka
So it's it's it's non definition well. But that that was that wasn't really your question.

04:01:59: Tivadar Danka
Your question was about

04:02:00: Tivadar Danka
limits.

04:02:01: Tivadar Danka
So

04:02:02: Tivadar Danka
let's forget functions.

04:02:05: Tivadar Danka
For a while, and let's talk about sequences.

04:02:11: Tivadar Danka
Sequences

04:02:12: Tivadar Danka
form the foundations of calculus.

04:02:15: Tivadar Danka
Deep down, it's all about sequencing. So a sequence basic is basically like a

04:02:19: Tivadar Danka
as the the names has a sequence of numbers.

04:02:23: Tivadar Danka
A zero,

04:02:25: Tivadar Danka
a

04:02:27: Tivadar Danka
a one, a two, and so on.

04:02:31: Tivadar Danka
One example of a sequence

04:02:33: Tivadar Danka
a n is one over m.

04:02:37: Tivadar Danka
Or b n equals

04:02:40: Tivadar Danka
m squared or c m equals the cosine of m.

04:02:47: Tivadar Danka
Right?

04:02:47: Tivadar Danka
This is sequence.

04:02:51: Tivadar Danka
I I will plot

04:02:54: Tivadar Danka
a.

04:02:56: Tivadar Danka
Sometimes my iPad

04:02:58: Tivadar Danka
lags a bit behind, which confuses me a lot. So this is this is n.

04:03:03: Tivadar Danka
And this is

04:03:04: Tivadar Danka
a of n.

04:03:06: Tivadar Danka
Here we have one,

04:03:07: Tivadar Danka
two,

04:03:09: Tivadar Danka
three, four, five, and so on.

04:03:14: Tivadar Danka
This is this is AM here.

04:03:23: Tivadar Danka
You see that the sequence a n

04:03:27: Tivadar Danka
is never going to take the value zero.

04:03:31: Tivadar Danka
Because there is no number who's who's

04:03:34: Tivadar Danka
whose reciprocal is zero.

04:03:36: Tivadar Danka
However,

04:03:38: Tivadar Danka
if you go long enough

04:03:40: Tivadar Danka
it will get closer and closer and closer to one.

04:03:45: Tivadar Danka
As close as you want.

04:03:47: Tivadar Danka
And

04:03:48: Tivadar Danka
because of this sorry. Not one to zero. It will get as close as you want to zero.

04:03:55: Tivadar Danka
As I mentioned, my mind is wired differently, and some I confuse

04:03:59: Tivadar Danka
and conflate

04:04:01: Tivadar Danka
concepts.

04:04:03: Tivadar Danka
And I I'm I'm the I'm the math teacher who says a,

04:04:08: Tivadar Danka
thinks b, writes c, but d is the correct answer.

04:04:12: Tivadar Danka
Which is which is a there's a quote from from Hungarian mathematician, Georg Poia. Alright. So this is this is why we can

04:04:20: Tivadar Danka
write that the limit

04:04:22: Tivadar Danka
of

04:04:23: Tivadar Danka
a of n as angles to

04:04:27: Tivadar Danka
to infinity is zero.

04:04:30: Tivadar Danka
So

04:04:32: Tivadar Danka
What happens in this case?

04:04:34: Tivadar Danka
When when the sequence is n squared?

04:04:51: Tivadar Danka
B m is

04:04:55: Tivadar Danka
n squared or or m, whatever.

04:05:01: Tivadar Danka
It will

04:05:03: Tivadar Danka
I mean, if cannot really

04:05:05: Tivadar Danka
connect these dots, but for visualization purposes, let's connect them.

04:05:09: Tivadar Danka
It will it will go higher and higher. So there is no single value

04:05:15: Tivadar Danka
to which the sequence gets closer and closer to

04:05:19: Tivadar Danka
to

04:05:22: Tivadar Danka
So in this case,

04:05:23: Tivadar Danka
we say that limb

04:05:26: Tivadar Danka
the limits of

04:05:27: Tivadar Danka
b n's n goes to infinity

04:05:30: Tivadar Danka
is infinity. A third case is the is the

04:05:34: Tivadar Danka
more interesting one.

04:05:36: Tivadar Danka
Instead instead of of

04:05:39: Tivadar Danka
using the cosine of m, let me define c n by minus one to the end.

04:05:47: Tivadar Danka
Which is which is a sequence. Minus one one minus one one and so on.

04:05:55: Tivadar Danka
It's easy to to to visualize.

04:06:03: Tivadar Danka
One,

04:06:04: Tivadar Danka
two,

04:06:05: Tivadar Danka
minus one one minus one one and so on.

04:06:13: Tivadar Danka
So here,

04:06:15: Tivadar Danka
there is no single value

04:06:17: Tivadar Danka
to which the sequence gets closer and closer to

04:06:21: Tivadar Danka
In this case, we say that that c m is or the sequence is divergent.

04:06:27: Tivadar Danka
This is when when we say that the limit doesn't doesn't exist.

04:06:37: Tivadar Danka
So it's it's it's not a number. It's not infinity. It does not exist. And when we talk about functions, it's

04:06:52: Tivadar Danka
it's very, very similar.

04:06:58: Tivadar Danka
If we move

04:07:03: Tivadar Danka
to the domain of functions, let's say f x is one over x.

04:07:09: Tivadar Danka
Then the

04:07:11: Tivadar Danka
we plot the function itself

04:07:16: Tivadar Danka
Let's see. That's

04:07:18: Tivadar Danka
if x grows

04:07:20: Tivadar Danka
and it goes to infinity,

04:07:22: Tivadar Danka
the value of f of x

04:07:24: Tivadar Danka
will be around

04:07:27: Tivadar Danka
zero.

04:07:27: Tivadar Danka
It it goes to as close to zero as you want.

04:07:33: Tivadar Danka
Again, I don't want to to

04:07:35: Tivadar Danka
talk about the precise mathematical definition here.

04:07:39: Tivadar Danka
So the limit of

04:07:43: Tivadar Danka
f of x s s goes to infinity is zero. However,

04:08:00: Tivadar Danka
What happens if you

04:08:03: Tivadar Danka
take the sign function?

04:08:04: Tivadar Danka
F of x is the sign of x. Means

04:08:11: Tivadar Danka
it's a very ugly sign function, but

04:08:13: Tivadar Danka
let's just go with the flow.

04:08:22: Tivadar Danka
It's limit as x goes to infinity.

04:08:28: Tivadar Danka
Sine of x doesn't exist. Because there is no single value.

04:08:33: Tivadar Danka
To which the function gets closer and closer to

04:08:37: Tivadar Danka
and it doesn't leave

04:08:39: Tivadar Danka
this this small neighborhood of of that value.

04:08:49: Tivadar Danka
Alright. So

04:08:51: Tivadar Danka
I

04:08:53: Tivadar Danka
a nutshell, this is

04:08:55: Tivadar Danka
convergence divergence. And, again, to to address the final part of your question, what prevents

04:09:02: Tivadar Danka
this from diverging

04:09:03: Tivadar Danka
Nothing.

04:09:05: Tivadar Danka
Grady and distant can diverge.

04:09:07: Tivadar Danka
The the difference quotients can diverge.

04:09:12: Tivadar Danka
I mean, functions are not always nice and and pretty.

04:09:15: Tivadar Danka
As what we we used to in machine learning.

04:09:19: Tivadar Danka
In machine learning, we try to work with with with functions that are that are differentiable.

04:09:25: Tivadar Danka
At least

04:09:26: Tivadar Danka
almost everywhere.

04:09:28: Tivadar Danka
The famous activation function is not differentiable. Differentiable.

04:09:39: Tivadar Danka
See, this is

04:09:43: Tivadar Danka
this is the rel function.

04:09:44: Tivadar Danka
So

04:09:46: Tivadar Danka
accidentally turned the screen of my iPad off.

04:09:55: Tivadar Danka
This is the ReLU.

04:09:58: Tivadar Danka
Of x.

04:10:01: Tivadar Danka
And as you can see,

04:10:04: Tivadar Danka
there is no no single tangent plane.

04:10:07: Tivadar Danka
Alright?

04:10:08: Tivadar Danka
Do you

04:10:11: Tivadar Danka
you guys have

04:10:12: Tivadar Danka
any other questions? I think some in the chat.

04:10:19: Tivadar Danka
Yeah. One one of your favorites is f x equals sign one over x.

04:10:24: Tivadar Danka
Yeah. I mean,

04:10:26: Tivadar Danka
if you want to practice

04:10:29: Tivadar Danka
feel free to to to to plot, sign, one over x. In in Not Not Lib. This it is, I mean, did an interesting example

04:10:38: Tivadar Danka
Yeah. It I mean, as you go towards zero,

04:10:41: Tivadar Danka
the oscillation gets really, really, really fast as fast as you want.

04:10:45: Tivadar Danka
So this is also one of my my favorite examples, the the sign sign of one over x.

04:10:54: Tivadar Danka
I think I think I mentioned this in in the

04:10:55: Tivadar Danka
in in the lecture notes.

04:11:00: Tivadar Danka
Somewhere.

04:11:03: Tivadar Danka
Never mind.

04:11:12: Tivadar Danka
Alright. We we have some in the in the q and a.

04:11:18: Tivadar Danka
Can you explain how integration is used in machine learning in simple terms?

04:11:23: Tivadar Danka
Sure.

04:11:27: Tivadar Danka
Let me

04:11:29: Tivadar Danka
think on it a bit.

04:11:32: Tivadar Danka
Have you heard about the concept of entropy, for instance, or

04:11:36: Tivadar Danka
Kuhlback Libular Divergence?

04:11:39: Tivadar Danka
My pronunciation is not not perfect. I I launched a YouTube channel

04:11:44: Tivadar Danka
couple of weeks ago, and and

04:11:46: Tivadar Danka
basically, all the time, I I have I I get comments about my accent. It I have a Hungarian accent. My pronunciation is not perfect.

04:11:55: Tivadar Danka
Enthropy. Yeah. Sure.

04:11:57: Tivadar Danka
Entropy is defined

04:11:59: Tivadar Danka
in terms of an integral, for instance.

04:12:03: Tivadar Danka
For for for for continuous random variables.

04:12:07: Tivadar Danka
Generally speaking, the concept of expected value

04:12:11: Tivadar Danka
is also also an integral

04:12:14: Tivadar Danka
itself for continuous So the expected values are all around machine learning.

04:12:20: Tivadar Danka
Mean,

04:12:21: Tivadar Danka
mean squared error is an expected value.

04:12:24: Tivadar Danka
Cost is an expected value.

04:12:27: Tivadar Danka
Right? So these these are are everywhere.

04:12:31: Tivadar Danka
Although, I have to admit,

04:12:33: Tivadar Danka
that that let me

04:12:36: Tivadar Danka
switch to my

04:12:38: Tivadar Danka
face cam mode, I guess.

04:12:39: Tivadar Danka
So I have to admit that

04:12:42: Tivadar Danka
in my opinion, differentiation is more more prevalent in machine learning than integration.

04:12:50: Tivadar Danka
Although, it's it's it's quite important.

04:12:54: Tivadar Danka
Probability theory uses integration

04:12:57: Tivadar Danka
all the time.

04:13:03: Tivadar Danka
Yeah. We have a question or comment in the chat. I have to go. I I I

04:13:09: Tivadar Danka
you for for being here. I hope you you

04:13:11: Tivadar Danka
like this workshop.

04:13:26: Tivadar Danka
Alright.

04:13:31: Tivadar Danka
Shoot me a

04:13:31: Tivadar Danka
a couple of questions.

04:13:35: Tivadar Danka
It doesn't have to be machine learning. You can you can ask me anything. Usually usually, I like like to keep these these these workshops

04:13:43: Tivadar Danka
unprofessional in a good way. Right? So so

04:13:47: Tivadar Danka
basically,

04:13:49: Tivadar Danka
feel free to ask me whatever you want.

04:13:52: Tivadar Danka
So it's more for building where differentiation is the mean, differentiation is what you use to to to to train models.

04:13:59: Tivadar Danka
And

04:14:00: Tivadar Danka
integration

04:14:02: Tivadar Danka
is is more for for understanding the theory behind it. But that's just my take.

04:14:08: Tivadar Danka
I mean, let me come up with an example where integration is important besides besides these

04:14:19: Tivadar Danka
I think, mechanistically,

04:14:22: Tivadar Danka
you can get by without integration. If you just want to train train models, and this But

04:14:28: Tivadar Danka
once

04:14:30: Tivadar Danka
once you want to to

04:14:31: Tivadar Danka
start, you know, understanding how models work on the inside,

04:14:36: Tivadar Danka
If you want to want to build

04:14:38: Tivadar Danka
mathematical frameworks for for those models, integration is is very important.

04:14:48: Tivadar Danka
Can you talk about taking delivery with partial deliver things on real data impact so you don't know the function equation. I mean, in in machine learning, you always know the the

04:14:59: Tivadar Danka
loss function itself.

04:15:02: Tivadar Danka
But I think you you are talking about

04:15:04: Tivadar Danka
numerical differentiation

04:15:07: Tivadar Danka
if if I'm if I'm correct.

04:15:10: Tivadar Danka
So

04:15:13: Tivadar Danka
mathematically,

04:15:15: Tivadar Danka
or maybe not mathematically, but computationally,

04:15:21: Tivadar Danka
you you don't always have a function. You have

04:15:24: Tivadar Danka
like, a sequence.

04:15:27: Tivadar Danka
Whose values, you know,

04:15:28: Tivadar Danka
Or maybe maybe you you you don't you don't have a formula

04:15:31: Tivadar Danka
for the function itself. Let me switch to my

04:15:35: Tivadar Danka
iPad here.

04:15:38: Tivadar Danka
Yeah.

04:15:39: Tivadar Danka
So the issue is that you have sorry.

04:15:42: Tivadar Danka
I

04:15:43: Tivadar Danka
You have f of x

04:15:47: Tivadar Danka
this is something which

04:15:48: Tivadar Danka
you don't have a formula for.

04:15:51: Tivadar Danka
But you have have have a function implemented somewhere and you can take the the

04:15:56: Tivadar Danka
substitution. You can substitute any value

04:15:59: Tivadar Danka
to find out f of x.

04:16:02: Tivadar Danka
But you don't know f itself. You don't

04:16:05: Tivadar Danka
are unable to use the rules of differentiation to compute the derivative. I think this is what you mean. Then

04:16:13: Tivadar Danka
I

04:16:15: Tivadar Danka
what you what you do here

04:16:18: Tivadar Danka
is instead of

04:16:20: Tivadar Danka
taking f prime sorry.

04:16:26: Tivadar Danka
My

04:16:27: Tivadar Danka
iPad is lagging a bit. So I think I'm I'm going to start a new new page because I think the the previous page was filled with with with the drawings, and it was

04:16:39: Tivadar Danka
a bit much for my iPad's memory.

04:16:47: Tivadar Danka
Or my iPad is dying.

04:16:49: Tivadar Danka
I don't know which one is worse.

04:16:52: Tivadar Danka
Essentially, you have f of x.

04:16:53: Tivadar Danka
At

04:16:54: Tivadar Danka
f prime of x

04:16:57: Tivadar Danka
can be approximated

04:16:59: Tivadar Danka
by

04:17:01: Tivadar Danka
f x plus

04:17:03: Tivadar Danka
h minus f x over h

04:17:08: Tivadar Danka
if

04:17:10: Tivadar Danka
h is small enough

04:17:20: Tivadar Danka
This is this is

04:17:21: Tivadar Danka
called, roughly speaking, numeric differentiation.

04:17:25: Tivadar Danka
And the principle is exactly the same

04:17:28: Tivadar Danka
for for for multivariable functions. Right? So

04:17:32: Tivadar Danka
if you have, I don't know, g of x and you want to

04:17:37: Tivadar Danka
compute the ice partial derivative.

04:17:41: Tivadar Danka
G of x,

04:17:43: Tivadar Danka
approximate it with g

04:17:45: Tivadar Danka
x plus

04:17:47: Tivadar Danka
h times

04:17:49: Tivadar Danka
this ominous e y vector that we talked about minus f of x.

04:17:56: Tivadar Danka
Divided by h.

04:17:58: Tivadar Danka
If

04:17:59: Tivadar Danka
h is small,

04:18:01: Tivadar Danka
in principle,

04:18:03: Tivadar Danka
this expression will be close to the

04:18:07: Tivadar Danka
derivative itself. However, there are

04:18:09: Tivadar Danka
number of issues. If the function oscillates,

04:18:13: Tivadar Danka
really, really fast,

04:18:15: Tivadar Danka
then

04:18:15: Tivadar Danka
this numerical optimization can be really unstable.

04:18:19: Tivadar Danka
So this

04:18:20: Tivadar Danka
this is something which we do not use in machine learning.

04:18:24: Tivadar Danka
Because in machine learning or, for instance, in neurons, we have

04:18:28: Tivadar Danka
layers and layers and layers of of functions.

04:18:31: Tivadar Danka
And each time they take an approximation for derivative,

04:18:35: Tivadar Danka
the the approximation error

04:18:37: Tivadar Danka
is amplified

04:18:39: Tivadar Danka
So by the time beyond the by by the time they reach the end of of a large function chain,

04:18:46: Tivadar Danka
our our results are basically nowhere near

04:18:50: Tivadar Danka
the the true derivative.

04:18:54: Tivadar Danka
Of all the areas of math applied to machine learning, what is your favorite leader, algebra? Hands down. Are they however, it's

04:19:02: Tivadar Danka
it's hard to choose. I mean,

04:19:04: Tivadar Danka
I'm a mathematician, so I I love all all parts of mathematics.

04:19:10: Tivadar Danka
I was, in in my studies, especially during my PhD, I was mostly focused on on on mathematical analysis. Which is basically a fancy word for calculus.

04:19:20: Tivadar Danka
That was my favorite

04:19:23: Tivadar Danka
Now I think I like linear algebra the most because

04:19:26: Tivadar Danka
it's

04:19:28: Tivadar Danka
such an elegant framework that connects so many dots in in machine learning and computer science.

04:19:35: Tivadar Danka
Can we, on the lecture notes, have an example of what we have done today and how we could apply it in a machine learning environment?

04:19:41: Tivadar Danka
Super interesting information, and I am excited to go through your book. I would just like to be able connect the dots

04:19:49: Tivadar Danka
I'm going to be honest with you. We are in the in the theory.

04:19:52: Tivadar Danka
Part. And

04:19:54: Tivadar Danka
we are still

04:19:56: Tivadar Danka
very far from from actual applications.

04:19:59: Tivadar Danka
So

04:19:60: Tivadar Danka
one thing you have to know about the mathematics of machine learning book, I started to write it I mean, I'm

04:20:06: Tivadar Danka
exactly five years ago.

04:20:07: Tivadar Danka
In in 2021. And

04:20:11: Tivadar Danka
I wrote about

04:20:13: Tivadar Danka
1,500 pages

04:20:16: Tivadar Danka
So

04:20:17: Tivadar Danka
I realized that

04:20:19: Tivadar Danka
I'm a session with writing two books. One is is about the mathematics of machine learning and the second is about basically implementing machine learning algorithms from scratch using the the mathematical theory

04:20:32: Tivadar Danka
we just built. And the book that was released

04:20:36: Tivadar Danka
is the first part. Right? So

04:20:39: Tivadar Danka
that won't connect the dots.

04:20:41: Tivadar Danka
For you just yet. However,

04:20:44: Tivadar Danka
the second part, which is

04:20:46: Tivadar Danka
I mean,

04:20:47: Tivadar Danka
more or less ready,

04:20:48: Tivadar Danka
and I'm I'm planning on finishing you this year. V I nine nine dot for you.

04:20:53: Tivadar Danka
Then we will have

04:20:55: Tivadar Danka
implementations for all machine learning algorithms. I mean, decision trees, neural network frameworks,

04:21:01: Tivadar Danka
I have have a neural network framework

04:21:04: Tivadar Danka
from scratch. Let me just link it in the chat.

04:21:17: Tivadar Danka
Let my find the GitHub repository for it.

04:21:24: Tivadar Danka
I think of course, I linked it in the q and a session because I'm not a very advanced user of this Hermit platform, but

04:21:36: Tivadar Danka
I think I can I can do it, do it, do it, do it, do it, do it, reply for you as well?

04:21:40: Tivadar Danka
Feel free to to to take a look around. I think this is the closest I I I

04:21:45: Tivadar Danka
am right now at at connecting the dots.

04:21:51: Tivadar Danka
First book is met back now, Second book will be real world applications. I mean, not real world application. But machine learning algorithms implemented from scratch.

04:21:60: Tivadar Danka
So we'll implement

04:22:02: Tivadar Danka
a full machine learning framework and neural network framework with back propagation, computation, all that stuff.

04:22:09: Tivadar Danka
I mean, it's it's it's beautiful. I'm I'm still working on it. It is going to be

04:22:15: Tivadar Danka
hopefully, like, a a very secret to the mathematics of machine learning book. I don't think if I'm even supposed to talk about it I I I hope it's it's it's fine if I if I hint that there will be a

04:22:29: Tivadar Danka
sequel coming to the book.

04:22:34: Tivadar Danka
Let's hop back into chat.

04:22:36: Tivadar Danka
Cool. For sure happy happy for for you to be here. If you if you have to go

04:22:41: Tivadar Danka
feel free. I'm just I'm just answering questions.

04:22:45: Tivadar Danka
You can question questions, like, not necessarily mathematics.

04:22:49: Tivadar Danka
Assuming it will be parked, yes. It will be. It will be parked. Yeah.

04:22:58: Tivadar Danka
Excellent. Thank you for answering. And excited for the second book. Thank you, I hope,

04:23:02: Tivadar Danka
my answer to your question was adequate.

04:23:06: Tivadar Danka
Yeah. I'm also excited for the second book.

04:23:24: Abhishek Kaushik
Hi, everyone. Again, before we wrap up, I want to send out a huge thank you to each one you you each one of you for being such an amazing audience. The chat was active, interactive. The questions were thoughtful. And

04:23:35: Abhishek Kaushik
your participation truly brought energy and workshop to this to this event. Hosting you all has been an absolute pleasure. And your energy made this session really special. A big thank you as well to Thiwadar for sharing his experience, insights, and practical strategies.

04:23:52: Abhishek Kaushik
What you learned today is a strong first step towards applying mathematical concepts in your own journey. Before you head out, we would really appreciate your feedback. It means a lot to us and helps us improve. We have put together a quick survey that takes less than ten seconds to fill up. Please take a moment to rate your experience and share your thoughts on that.

04:24:11: Abhishek Kaushik
As far as the session recordings are concerned, those will be available on the platform shortly after we close this workshop. You can access them using the same link you joined with, and they'll remain available on your dashboard for the next one twenty days. Certificates, transcripts, and all code files for everyone who attended today's workshop will be sent on their registered email addresses by the end of the week. And now is the time

04:24:35: Abhishek Kaushik
for a special shout out to our leader leader board winners. Congratulations, to all of you. We have the winners on the leader board.

04:24:42: Abhishek Kaushik
Eric Arroyo has won a six month pack subscription who has secured the first position. Heshavari Mantha is taking away home a three month pack subscription and has been on the second spot. And on the third spot, it's John Franovich who has win one ebook of his choice. We'll be reaching out to you all via email very soon with details on how to claim your prices. And one last thing before you sign off, the event wrap up is now live on LinkedIn, and we would love for you to keep the conversation going live there.

04:25:15: Abhishek Kaushik
The link is already in the chat, so feel free to jump in, share your key takeaways, and connect with the fellow participants.

04:25:21: Abhishek Kaushik
One last thing, remember, this is just the beginning.

04:25:24: Abhishek Kaushik
We'll continue bringing you more search of high value sessions on machine learning and the data world. Be sure to subscribe to the Data Pro newsletter for exclusive perks and special discounts. And conclusively, on behalf of pact here, I thank you all once again for your time enthusiasm, and support.

04:25:43: Abhishek Kaushik
I hope to see you at our very next workshop very soon, Until then, keep learning, keep experimenting, practicing, stay healthy, be active, and use the mathematics used in machine learning.

04:25:54: Abhishek Kaushik
Thank you very much.

04:25:56: Abhishek Kaushik
K.

04:25:59: Abhishek Kaushik
Bye.